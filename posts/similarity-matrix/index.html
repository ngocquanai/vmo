<!doctype html>














<!-- `site.alt_lang` can specify a language different from the UI -->
<html lang="en" >
  <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta
    name="viewport"
    content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover"
  ><!-- Setup Open Graph image -->

  

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Similar Properties of Similarity Matrix" />
<meta property="og:locale" content="en" />
<meta name="description" content="Abstract: Firstly, I will introduce specific definitions relevant to matrix similarity topic. Subsequently, I will explore various common properties of similar matrices, such as trace, rank, determinant, among others. Finally, I will present criteria for determining whether any two given matrices are similar. DEFINITIONS 1.1: SIMILARITY MATRIX Two square \(n \times n\) matrices A and B are called similar if there exists an invertible \(n*n\) matrix \(P\) such that [B = P^{-1}AP] 1.2: REPRESENTATION OF A VECTOR \(V\) is a vector space with the basis \(B=\left&lt;\overrightarrow{\beta_1}, \overrightarrow{\beta_2}, \ldots, \overrightarrow{\beta_n}\right&gt; , \vec{v} \in V, \alpha_1, \ldots, \alpha_n \in \mathbb{R}\) such that \(\vec{v}=\alpha_1 \overrightarrow{\beta_1}+\alpha_2 \overrightarrow{\beta_2}+\ldots+\alpha_n \overrightarrow{\beta_n}.\) Then we have [{Rep}_B(\vec{v})=\left(\begin{array}{c} \alpha_1 \ \alpha_2 \ \vdots \ \alpha_n \end{array}\right)] 1.3: MATRIX REPRESENTATION \(V, W\) are vector spaces with dimension \(n, m\) and basis \(B, D\). \(f \colon V \to W\) is a linear mapping. If we have \(\operatorname{Rep}_D(h(\overrightarrow{\beta_1}))=\left(\begin{array}{c}h_{1,1} \\ h_{2,1} \\ \vdots \\ h_{n, 1}\end{array}\right), \ldots, \operatorname{Rep}_D(h(\overrightarrow{\beta_n}))=\left(\begin{array}{c}h_{1, n} \\ h_{2, n} \\ \vdots \\ h_{n, n}\end{array}\right)\). Then [\left(\begin{array}{cccc}h_{1,1} &amp; h_{1,2} &amp; \cdots &amp; h_{1, n} \ h_{2,1} &amp; h_{2,2} &amp; \cdots &amp; h_{2, n} \ \vdots &amp; \vdots &amp; &amp; \vdots \ h_{n, 1} &amp; h_{n, 2} &amp; \cdots &amp; h_{n, n}\end{array}\right)] is called matrix representation of mapping f with respect to the bases \(B, D\), notated by \({Rep}_{B,D}(f)\) 1.4: DETERMINANT OF MATRIX \(n \times n\) determinant is a function \(f \colon M_{n \times n} \rightarrow \mathbb{R}\) such that (1) \(\operatorname{det}\left(\overrightarrow{p_1}, \ldots, k \overrightarrow{p_i}+\overrightarrow{p_j}, \ldots, \overrightarrow{p_n}\right)=\operatorname{det}\left(\overrightarrow{p_1}, \ldots, \overrightarrow{p_j}, \ldots, \overrightarrow{p_n}\right)\), for all \(k \in \mathbb{R}\) (2) \(\operatorname{det}\left(\overrightarrow{p_1}, \ldots, \overrightarrow{p_j}, \ldots, \overrightarrow{p_i}, \ldots, \overrightarrow{p_n}\right)=-\operatorname{det}\left(\overrightarrow{p_1}, \ldots, \overrightarrow{p_i}, \ldots, \overrightarrow{p_j}, \ldots, \overrightarrow{p_n}\right)\) (3) \(\operatorname{det}\left(\overrightarrow{p_1}, \ldots, k \overrightarrow{p_i}, \ldots, \overrightarrow{p_n}\right) = k \operatorname{det}\left(\overrightarrow{p_1}, \ldots, \overrightarrow{p_i}, \ldots, \overrightarrow{p_n}\right)\), for all \(k \in \mathbb{R}\) (4) \(\operatorname{det}(I) = 1\), \(I\) is the identity matrix. Normally, we use the notation \(|T|\) instead of \(\operatorname{det}(T)\). 1.5: TRACE OF MATRIX The trace of a square matrix \(A\) is the sum of elements on the main diagonal of matrix \(A\), denoted by \(tr(A)\) 1.6: POLYNOMIAL OF MATRIX \(T\) is a square matrix, \(f(x)\) is a polynomial with degree n \(f(x) = c_n x^n + c_{n-1} x^{n-1} + \cdots + c_1 x + c_0\) \((c_i \in \mathbb{R})\). Then we have \(f(T) = c_n T^n + c_{n-1} T^{n-1} + \cdots + c_1 T + c_0 I\) (\(I\) is identity matrix) is a polynomial of matrix \(T\) and also a square matrix of the same size as \(T\). 1.7: MINIMAL POLYNOMIAL For a square matrix \(T\), its minimal polynomial is the monic polynomial \(f\) of the lowest degree such that \(f(T) = 0\). 1.8: INDEX OF NIPOTENT A square matrix $A$ is said to have a nilpotent index $k$ if \(k\) is the smallest natural number(if exists) such that $A^k = 0$. 1.9: SUBDIAGONAL The subdiagonal of a square matrix is the line immediately below the main diagonal of the matrix. 1.10: JORDAN BLOCK A Jordan block is a square matrix where the off-diagonal entries are zero, the entries on the main diagonal are all $\lambda$ (where $\lambda \in \mathbb{C}$), and every entry on the subdiagonal is 1. 1.11: JORDAN CANONICAL FORM The square matrix $A$ can be represented in the form of a Jordan normal form if [A = \left( \begin{array}{cccc} J_{\lambda_1} &amp; 0 &amp; \cdots &amp; 0 0 &amp; J_{\lambda_2} &amp; \ddots &amp; \vdots \vdots &amp; \ddots &amp; \ddots &amp; 0 0 &amp; \cdots &amp; 0 &amp; J_{\lambda_k} \end{array} \right)] with \(J_{\lambda_i}\) are Jordan blocks. 1.12: CHARACTERISTIC POLYNOMIAL A is an \(n \times n\) matrix, the characteristic polynomial of A, denoted by \(p_{A}(x)\) is defined by [p_{A}(x) = \operatorname{det}(A - xI)] where \(I\) denotes the \(n \times n\) identity matrix. 1.13: GEOMETRIC AND ALGEBRAIC MULTIPLICITY A is a square matrix with characteristic polynomial \(f(x) = (x- \lambda_1)^{m_1} (x - \lambda_2)^{m_2} \ldots (x- \lambda_r)^{m_r}\). Then \(m_i\) is called algebraic multiplicity corresponding to eigenvalue \(\lambda_i\), the dimension of eigenspace corresponding to eigenvalue \(\lambda_i\) is called geometric multiplicity. 1.14: COFACTOR OF MATRIX Consider the \(n \times n\) square matrix \(T\), we define the \(i,j\) minor \(M_{i,j}\) of matrix \(T\) is the \((n-1) \times (n-1)\) matrix obtained by eliminating the i-th row and j-th column in the matrix \(T\). Then we have \(i,j\) cofactor \(T_{i,j} = (-1)^{i+j} \operatorname{det}(M_{i,j})\) 1.15: MATRIX ADJOINT Matrix adjoint of square matrix T is denoted by [\text{adj}(T) = \left( \begin{array}{cccc} T_{1,1} &amp; T_{2,1} &amp; \cdots &amp; T_{n,1} T_{1,2} &amp; T_{2,2} &amp; \cdots &amp; T_{n,2} \vdots &amp; \vdots &amp; \ddots &amp; \vdots T_{1,n} &amp; T_{2,n} &amp; \cdots &amp; T_{n,n} \end{array} \right)] where \(T_{i,j}\) is the \(i,j\) cofactor of matrix T. 1.16: EQUIVALENCE RELATION A binary relation \(\sim\) on a set \(\mathbb{X}\) is said to be an equivalence relation if and only if it is relexive, symmetric and transitive. That is, for all \(a, b, c \in \mathbb{X}\) : Reflexive: \(a \sim a\) Symmetric: \(a \sim b\) if and only of \(b \sim a\) Transitive: If \(a \sim b\) and \(b \sim c\), then \(a \sim c\) 2. PROPERTIES 2.1: EQUIVALENCE RELATION PROPERTY 2.1: Matrix similarity is equivalence relation. We aim to prove that the similarity matrix relation satisfies the properties of reflexivity, symmetry, and transitivity. Reflexive: For any square matrix \(A\), it is obvious that \(I^{-1}AI = A\), where \(I\) is the identity matrix. Therefore, \(A\) is similar to itself. Symmetric: Suppose that B is similar to A,which means there exists an invertible matrix \(P\) such that \(B = P^{-1}AP\) \(\implies B P^{-1} = P^{-1}AI = P^{-1}A\) \(\iff PBP^{-1} = IA = A\) This implies A is also similar to B, so we have the similarity relation is symmetric. Transitive: Consider three matrices \(A\), \(B\), and \(C\) such that \(A\) is similar to \(B\) and \(B\) is similar to \(C\). Hence, there exist invertible matrices \(P\), \(Q\) such that \(B = P^{-1}AP\) and \(C = Q^{-1}BQ\). \(\implies C = Q^{-1}P^{-1}APQ = (PQ)^{-1}APQ\) Therefore, \(C\) is similar to \(A\), and matrix similarity relation is transitive. 2.2: SIMILAR LINEAR MAPPING PROPERTY 2.2: Similarity matrices are matrix representations of the same linear mapping but with respect to different bases. Lemma 1: Consider three vector spaces \(U,V,W\) and two linear mappings \(f \colon U \to V\) and \(g \colon V \to W\); \(B,D,E\) are the bases of 3 vector spaces, respectively. Then we have $$ \operatorname{Rep}{B,E}(f \circ g) = \operatorname{Rep}{B,D}(f) \cdot \operatorname{Rep}_{D,E}(g) $$ The notation id represent for identity matrix Consider two similar matrices \(H,G\) such that \(H= \operatorname{Rep}_{B,B}(h)\) and \(G= P^{-1}HP\), where \(B\) is the basis of vector space \(V\) including \(\overrightarrow{\beta_1}, \overrightarrow{\beta_2}, \ldots, \overrightarrow{\beta_n}\). \(h\) is a linear mapping. Consider the basis \(D\) such that \(\operatorname{Rep}_{D,B}(id) = P\) From the figure above, we have \(h = id \circ h \circ id\). Using Lemma 1 then we have [\operatorname{Rep}{D,D}(h) = \operatorname{Rep}{D,B}(id) \cdot \operatorname{Rep}{B,B}(h) \cdot \operatorname{Rep}{B,D}(id) = P^{-1}HP \iff G = \operatorname{Rep}_{D,D}(h) : : : (Q.E.D)] 2.3: SIMILAR RANK PROPERTY 2.3: Similarity matrices have the same rank. Lemma 2: \(f \colon U \to V\) is a linear mapping with a matrix representation \(A\). Then we have rank \(A\) equals to the dimension of the image space of linear mapping \(f\) So from Lemma 2 and Properties 2.1, we infer that similar matrices have the same rank. 2.4: SIMILAR DETERMINANT PROPERTY 2.4: Similarity matrices have the same determinant. Consider two similar matrices \(A, B\). Since \(A, B\) are two similar matrices, there exists an invertible matric P such that \(B = PAP^{-1}\) We have [\begin{align} |B| &amp;= |PAP^{-1}| &amp;= |P| |A| |P^{-1}| &amp;= |A| |PP^{-1}| &amp;= |A| : : \text{Determinant of identity matrix equals 1} \end{align}] So we have \(|A| = |B|\) (Q.E.D) 2.5: SIMILAR TRACE PROPERTY 2.5: Similarity matrices have the same trace. Lemma 3: \(M,N\) are two arbitrary square matrices, then we have \(\operatorname{tr}(MN) = \operatorname{tr}(NM)\). \ Since \(A, B\) are two similar matrices, there exists an invertible matric P such that \(B = PAP^{-1}\) \(\implies \operatorname{tr}(B) = \operatorname{tr}(P^{-1}AP)\) Using Lemma 3 we have : [\begin{align} \operatorname{tr}(B) &amp;= \operatorname{tr}(PAP^{-1}) &amp;= \operatorname{tr}(APP^{-1}) &amp;= \operatorname{tr}(AI) &amp;= \operatorname{tr}(A) \end{align}] So we have \(\operatorname{trace}(A) = \operatorname{trace}(B)\) Remark: If two square matrices of the same order \(A\) and \(B\) satisfy \(\operatorname{tr}(A) = \operatorname{tr}(B)\) and \(|A| = |B|\), we cannot conclude that \(A\) and \(B\) are similar matrices. Indeed, we will demonstrate a counterexample. Consider the two matrices \(A = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix}, B = \begin{pmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{pmatrix}.\) It is easy to see that \(|A| = \left| B \right| = 1, \quad \operatorname{tr}(A) = \operatorname{tr}(B) = 2.\) However, there was not exist an invertible matrix \(P\) such that \(B = PAP^{-1}\) because if it were so, then \(B = PAP^{-1} = PIP^{-1} = PP^{-1} = I\) which is absurd. Therefore, we conclude that \(A\) and \(B\) are not similar matrices. 2.6: SIMILARITY MATRIX ADJOINT PROPERTY 2.6: Matrix adjoint of two similar matrices are also similar. Lemma 4(Laplace Expansion of Determinants): For an \(n \times n\) square matrix \(T\) with the \(i,j\) cofactor \(T_{i,j}\), the entry at row \(i\), column \(j\) of matrix \(T\) is denoted by \(t_{i,j}\). Then we have: [\begin{align} |T| &amp;= t_{i,1}T_{i,1} + t_{i,2}T_{i,2} + \ldots + t_{i,n}T_{i,n} &amp;= t_{1,j}T_{1,j} + t_{2,j}T_{2,j} + \ldots + t_{n,j}T_{n,j} \end{align}] Lemma 5: Consider \(M_n\) is the set contains all square matrices of degree \(n\). Then the function \(f \colon M_n \to M_n\) that satisfy \(f(H) = adj(H), \forall H \in M_n\) is a continuous function. Consider the matrix \(T = \begin{pmatrix} t_{1,1} &amp; t_{1,2} &amp; \ldots &amp; t_{1,n} \\ t_{2,1} &amp; t_{2,2} &amp; \ldots &amp; t_{2,n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ t_{n,1} &amp; t_{n,2} &amp; \ldots &amp; t_{n,n} \end{pmatrix}\) with the adjugate \(\text{adj}(T) = \begin{pmatrix} T_{1,1} &amp; T_{2,1} &amp; \ldots &amp; T_{n,1} \\ T_{1,2} &amp; T_{2,2} &amp; \ldots &amp; T_{n,2} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ T_{1,n} &amp; T_{2,n} &amp; \ldots &amp; T_{n,n} \end{pmatrix}\) First, we prove that \(\sum_{j=1}^{n}t_{i,j} T_{k,j} = \sum_{j=1}^{n} t_{j,i}T_{j,k} = 0 \text{ for all } i \neq k\) Consider square matrix \(T\), and the matrix \(A\) defined by preserving all values of the matrix \(T\) except for the values in row \(k\); the values in row \(k\) of matrix \(A\) equal to row \(i\) in matrix \(T\).(which means row \(i\) and \(k\) of matrix A have the same values.) Similarly, matrix A have entries \(a_{i,j}\) and \(i,j\) cofactor \(A_{i,j}\). Since the row \(i\) and the row \(k\) of matrix \(A\) is the same, it implies that \(|A| = 0\). Using Lemma 4 we have [\begin{align} \sum_{j=1}^{n}t_{i,j} T_{k,j} &amp;= \sum_{j=1}^{n} a_{k,j}T_{k,j} &amp; \text{(Row i of T and row k of A are the same)} \ &amp;= \sum_{j=1}^{n} a_{k,j}A_{k,j} &amp; \text{(when eliminating row k, the remaining of A and T are the same)} &amp;= |A| &amp; \text{Lemma 4} \ &amp;= 0 &amp; \text{(A have 2 rows with the same values.)} \end{align}] Similarly, we prove that \(\sum_{j=1}^{n} t_{j,i}T_{j,k} = 0, \forall i \neq k\) Using the Lemma 4 we imply that [T \operatorname{adj}(T) = \operatorname{adj}(T) \cdot T = T \cdot I : : : : (1)] Consider two similar matrices A and B. There exist an invertible matrix \(P\) such that \(B = P^{-1}AP\) We prove the Property 2.5 in two cases: \(|A| = 0\) and \(|A| \neq 0\) Case 1: \(|A| \neq 0\) Using Property 2.4 we imply \(|B| \neq 0\), then \(A\) and \(B\) are invertible matrices. Using (1) we have [\begin{align} \operatorname{adj}(A) &amp; =\operatorname{det}(A) \cdot A^{-1} \ \operatorname{adj}(B) &amp; =\operatorname{det}(B) \cdot B^{-1} \ \operatorname{adj}(AB) &amp;= |AB| \cdot (AB)^{-1} &amp;= |A| \cdot |B| \cdot B^{-1} A^{-1} &amp;= |B| \cdot B^{-1} \cdot |A| \cdot A^{-1} &amp;= \operatorname{adj}(B) \operatorname{adj}(A) \end{align}] From the above, we imply the following: [\begin{align} \operatorname{adj}(B) &amp;= \operatorname{adj}(P^{-1}AP) &amp;= \operatorname{adj}(P (AP^{-1})) &amp;= \operatorname{adj}(AP^{-1}) \operatorname{adj}(P) &amp;= \operatorname{adj}(P^{-1}) \operatorname{adj}(A) \operatorname{adj}(P) \end{align}] Otherwise, from (1) we have \(T^{-1} \operatorname{adj}(T^{-1}) = |T^{-1}| \cdot I\) with an invertible matrix \(T\) \(\implies \operatorname{adj}(T^{-1}) = |T^{-1}| \cdot T\) But we have \(\operatorname{adj}(T) = |T| \cdot T^{-1} \implies \operatorname{adj}(T) \operatorname{adj}(T^{-1}) = I\) So we imply that \(\operatorname{adj}(B) = \operatorname{adj}(P)^{-1} \operatorname{adj}(A) \operatorname{adj}(P)\), which means \(\operatorname{adj}(A)\) and \(\operatorname{adj}(B)\) are two similar matrices . Case 2: \(|A| = 0\) From the Property 2.3 we imply \(|B| = 0\) For an arbitrary square matrix \(M\), we have \(M\) is an invertible matrix \(\iff |M| \neq 0\) Using Definition 1.12 we imply that matrix \(A + \lambda I\) is invertible for all \(\lambda\) such that \(\lambda\) is not an eigenvalue of matrix \(-A\). Since matrix \(-A\) has finite eigenvalues; therefore, for all \(\lambda\) small enough we have matrix \(A + \lambda I\) is invertible, then : [\operatorname{adj}((A + \lambda I)B) = \operatorname{adj}(B) \operatorname{adj}(A + \lambda I)] Using Lemma 5 we imply that : [\begin{aligned} \operatorname{adj}(A B) &amp; =\operatorname{adj}\left(\lim _{\lambda \rightarrow 0}(A+\lambda I) B\right) &amp; =\lim _{\lambda \rightarrow 0} \operatorname{adj}((A+\lambda I) B) &amp; =\lim _{\lambda \rightarrow 0} \operatorname{adj}(B) \cdot \operatorname{adj}(A+\lambda I) &amp; =\operatorname{adj}(B) \cdot \lim _{\lambda \rightarrow 0} \operatorname{adj}(A+\lambda I) &amp; =\operatorname{adj}(B) . \operatorname{adj}(A) \end{aligned}] Hence, by a similar proof as in Case 1, we conclude that A and B are two similar matrices. 2.7: SIMILAR EIGENVALUES PROPERTY 2.7: Similarity matrices have the same eigenvalues. Consider two similar matrices \(A\) and \(B\). Assume that B has an eigenvalue \(\lambda\) corresponding to eigenvector \(\vec{v}\). We have : [\begin{aligned} B \vec{v}=\lambda \vec{v} \ \Leftrightarrow &amp; P^{-1} A P \vec{v}=\lambda \vec{v} \ \Leftrightarrow &amp; A P \vec{v}=P \lambda \vec{v}=\lambda P \vec{v} \end{aligned}] \(\implies\) Every eigenvalue \(\lambda\) of matrix \(B\) is also an eigenvalue of matrix \(A\) Using similarly proof, we have every eigenvalue of matrix \(A\) is also an eigenvalue of matrix \(B\); therefore, \(A\) and \(B\) have same eigenvalues. (Q.E.D) 2.8: SIMILAR CHARACTERISTIC AND MINIMAL POLYNOMIAL PROPERTY 2.8: \(A\) and \(B\) are two similar matrices, then \(A\) and \(B\) have the same characteristic polynomial and minimal polynomial. The characteristic polynomial of matrix \(A\) and \(B\) is \(\operatorname{det}(A - xI)\) and \(\operatorname{det}(B - xI)\), respectively, with \(x\) is a variable. We have: [\begin{aligned} \operatorname{det}(B-x I) &amp; =\operatorname{det}\left(P^{-1} A P-x I\right) \ &amp; =\operatorname{det}\left(P^{-1} A P-P^{-1} x I P\right) \ &amp; =\operatorname{det}\left(P^{-1}(A-x I) P\right) \ &amp; =\operatorname{det}\left(P^{-1}\right) \operatorname{det}(A-x I) \operatorname{det} P \ &amp; =\operatorname{det}(A-x I) \operatorname{det} P \cdot \operatorname{det}\left(P^{-1}\right) \ &amp; =\operatorname{det}(A-x I) \end{aligned}] So we conclude that \(A\) and \(B\) have same characteristic polynomial. \ Consider polynomial \(f(x)=x^n+a_{n-1} x^{n-1}+\ldots+a_1 x+a_0\) is a minimal polynomial of matrix \(A\). \(\implies f(A)=0\). Then we have : [\begin{aligned} f\left( B \right) &amp;= f\left(P^{-1} A P\right) &amp;= \left(P^{-1} A P\right)^n+a_{n-1}\left(P^{-1} A P\right)^{n-1}+\ldots+a_n\left(P^{-1} A P\right) +a_0 I \ \left(P^{-1} A P\right)^n &amp;= P^{-1} A P \cdot P^{-1} A P \ldots P^{-1} A P &amp;=P^{-1} A\left(P P^{-1}\right) \cdot A\left(P P^{-1}\right) \ldots\left(P P^{-1}\right) A P &amp;=P^{-1} A^n P \ \implies f(B) &amp;= P^{-1} A^n P +a_{n-1} P^{-1} A^{n-1} P +\ldots +a_1 P^{-1} A P+a_0 P^{-1} I P &amp;=P^{-1}\left(A^n+a_{n-1} A^{n-1}+\ldots+a_1 A+a_0 I\right) P &amp;=P^{-1} \cdot f(A) \cdot P &amp;=0 \end{aligned}] Consider \(g(x)\) is the minimal polynomial of matrix \(B\), since \(f(B) = 0\) then we imply \(\operatorname{deg}(f) \geq \operatorname{deg}(g)\). Using similarly proof, we have \(\operatorname{deg}(g) \geq \operatorname{deg}(f)\), then we imply that \(\operatorname{deg}(f) = \operatorname{deg}(g)\). Assume that \(f(x) \neq g(x)\), then consider \(h(x) = f(x) - g(x)\). We have \(h(x)\) has smaller degree than \(f , g\), and \(h(x)\) is not a zero polynomial. \(\implies h(B) = f(B) - g(B) = 0\) is a characteristic polynomial of matrix \(B\) with smaller degree than its minimal polynomial \(g(x)\) (CONTRADICTION). Therefore, we have \(f(x) = g(x)\), which means two matrices \(A\) and \(B\) have same minimal polynomial. 2.9: SIMILAR GEOMETRIC MULTIPLICITY, ALGEBRAIC MULTIPLICITY PROPERTY 2.9: \(A\) and \(B\) are two similar matrices. By Property 2.7 they also have same eigenvalues. Then for each eigenvalue, matrix \(A\) and \(B\) have same geometric multiplicity and algebraic multiplicity corresponding to this eigenvalue. From Property 2.7 we have \(A\) anh \(B\) have same chracteristic polynomial. By Definition 1.13 we imply that these two matrices have same algebraic multiplicity for each eigenvalue. Since \(A\) and \(B\) are similar, so there exist invertible matrix \(P\) such that \(B=P^{-1}AP\). Consider \(\lambda\) is an eigenvalue of matrix \(A\) and \(\left( \vec{v_1}, \vec{v_2}, \ldots, \vec{v_k} \right)\) is a basis of eigenspace corresponding to eigenvalue \(\lambda\) Using proof in Property 2.7 we imply that \(P \vec{v_1}, P \vec{v_2}, \ldots, P \vec{v_k}\) are eigenvectors corresponding to the eigenvalue \(\lambda\) of the matrix \(B\). We prove that \(\left( P^{-1} \vec{v_1}, P^{-1} \vec{v_2}, \ldots, P^{-1} \vec{v_k} \right)\) is the basis of eigenspace corresponding to eigenvalue \(\lambda\) of matrix \(B\). Assume there exists \(\alpha_1, \alpha_2, \ldots, \alpha_n\) such that [\begin{aligned} &amp; \alpha_1 P^{-1} \vec{v_1}+\alpha_2 P^{-1} \vec{v_2}+\ldots+\alpha_k P^{-1} \vec{v_k}=\vec{0} \ \Rightarrow &amp; \alpha_1 \vec{v_1}+\alpha_2 \vec{v_2}+\ldots+\alpha_k \vec{v_k}= P \cdot \vec{0}= \vec{0} \end{aligned}] Since \(\left( \vec{v_1}, \vec{v_2}, \ldots, \vec{v_k} \right)\) is a basis of vector space with dimension \(k\), then we have \(\vec{\alpha_1} = \vec{\alpha_2} = \ldots = \vec{\alpha_k} = 0\) Hence, we have \(\left(P^{-1} \vec{v_1}, P^{-1} \vec{v_2}, \ldots, P^{-1} \vec{v_k} \right)\) is linear independent. Assume there exists vector \(\vec{v}\) is a eigenvector of matrix \(B\) that correspond to eigenvalue \(\lambda\) such that \(\vec{v} \notin \operatorname{Span}({P^{-1} \vec{v_1}, P^{-1} \vec{v_2}, \ldots, P^{-1} \vec{v_k}})\) Hence, there exists eigenvector \(\vec{u}\) corresponding to eigenvalue \(\lambda\) of matrix \(A\) such that \(\vec{v} = P^{-1} \vec{u}\). Then we imply that \(\vec{u} \notin \operatorname{Span}({\vec{v_1}, \vec{v_2}, \ldots, \vec{v_k} })\) (CONTRADICTION) Therefore, we can imply that \(\left( P^{-1} \vec{v_1}, P^{-1} \vec{v_2}, \ldots, P^{-1} \vec{v_k} \right)\) is the basis of eigenspace corresponding to eigenvalue \(\lambda\) of matrix \(B\). Hence, we can conclude that eigenspace corresponding to eigenvalue \(\lambda\) of \(A\) and \(B\) have same dimension. Therefore, by definition, similar matrices have same geometric multiplicity. (Q.E.D)" />
<meta property="og:description" content="Abstract: Firstly, I will introduce specific definitions relevant to matrix similarity topic. Subsequently, I will explore various common properties of similar matrices, such as trace, rank, determinant, among others. Finally, I will present criteria for determining whether any two given matrices are similar. DEFINITIONS 1.1: SIMILARITY MATRIX Two square \(n \times n\) matrices A and B are called similar if there exists an invertible \(n*n\) matrix \(P\) such that [B = P^{-1}AP] 1.2: REPRESENTATION OF A VECTOR \(V\) is a vector space with the basis \(B=\left&lt;\overrightarrow{\beta_1}, \overrightarrow{\beta_2}, \ldots, \overrightarrow{\beta_n}\right&gt; , \vec{v} \in V, \alpha_1, \ldots, \alpha_n \in \mathbb{R}\) such that \(\vec{v}=\alpha_1 \overrightarrow{\beta_1}+\alpha_2 \overrightarrow{\beta_2}+\ldots+\alpha_n \overrightarrow{\beta_n}.\) Then we have [{Rep}_B(\vec{v})=\left(\begin{array}{c} \alpha_1 \ \alpha_2 \ \vdots \ \alpha_n \end{array}\right)] 1.3: MATRIX REPRESENTATION \(V, W\) are vector spaces with dimension \(n, m\) and basis \(B, D\). \(f \colon V \to W\) is a linear mapping. If we have \(\operatorname{Rep}_D(h(\overrightarrow{\beta_1}))=\left(\begin{array}{c}h_{1,1} \\ h_{2,1} \\ \vdots \\ h_{n, 1}\end{array}\right), \ldots, \operatorname{Rep}_D(h(\overrightarrow{\beta_n}))=\left(\begin{array}{c}h_{1, n} \\ h_{2, n} \\ \vdots \\ h_{n, n}\end{array}\right)\). Then [\left(\begin{array}{cccc}h_{1,1} &amp; h_{1,2} &amp; \cdots &amp; h_{1, n} \ h_{2,1} &amp; h_{2,2} &amp; \cdots &amp; h_{2, n} \ \vdots &amp; \vdots &amp; &amp; \vdots \ h_{n, 1} &amp; h_{n, 2} &amp; \cdots &amp; h_{n, n}\end{array}\right)] is called matrix representation of mapping f with respect to the bases \(B, D\), notated by \({Rep}_{B,D}(f)\) 1.4: DETERMINANT OF MATRIX \(n \times n\) determinant is a function \(f \colon M_{n \times n} \rightarrow \mathbb{R}\) such that (1) \(\operatorname{det}\left(\overrightarrow{p_1}, \ldots, k \overrightarrow{p_i}+\overrightarrow{p_j}, \ldots, \overrightarrow{p_n}\right)=\operatorname{det}\left(\overrightarrow{p_1}, \ldots, \overrightarrow{p_j}, \ldots, \overrightarrow{p_n}\right)\), for all \(k \in \mathbb{R}\) (2) \(\operatorname{det}\left(\overrightarrow{p_1}, \ldots, \overrightarrow{p_j}, \ldots, \overrightarrow{p_i}, \ldots, \overrightarrow{p_n}\right)=-\operatorname{det}\left(\overrightarrow{p_1}, \ldots, \overrightarrow{p_i}, \ldots, \overrightarrow{p_j}, \ldots, \overrightarrow{p_n}\right)\) (3) \(\operatorname{det}\left(\overrightarrow{p_1}, \ldots, k \overrightarrow{p_i}, \ldots, \overrightarrow{p_n}\right) = k \operatorname{det}\left(\overrightarrow{p_1}, \ldots, \overrightarrow{p_i}, \ldots, \overrightarrow{p_n}\right)\), for all \(k \in \mathbb{R}\) (4) \(\operatorname{det}(I) = 1\), \(I\) is the identity matrix. Normally, we use the notation \(|T|\) instead of \(\operatorname{det}(T)\). 1.5: TRACE OF MATRIX The trace of a square matrix \(A\) is the sum of elements on the main diagonal of matrix \(A\), denoted by \(tr(A)\) 1.6: POLYNOMIAL OF MATRIX \(T\) is a square matrix, \(f(x)\) is a polynomial with degree n \(f(x) = c_n x^n + c_{n-1} x^{n-1} + \cdots + c_1 x + c_0\) \((c_i \in \mathbb{R})\). Then we have \(f(T) = c_n T^n + c_{n-1} T^{n-1} + \cdots + c_1 T + c_0 I\) (\(I\) is identity matrix) is a polynomial of matrix \(T\) and also a square matrix of the same size as \(T\). 1.7: MINIMAL POLYNOMIAL For a square matrix \(T\), its minimal polynomial is the monic polynomial \(f\) of the lowest degree such that \(f(T) = 0\). 1.8: INDEX OF NIPOTENT A square matrix $A$ is said to have a nilpotent index $k$ if \(k\) is the smallest natural number(if exists) such that $A^k = 0$. 1.9: SUBDIAGONAL The subdiagonal of a square matrix is the line immediately below the main diagonal of the matrix. 1.10: JORDAN BLOCK A Jordan block is a square matrix where the off-diagonal entries are zero, the entries on the main diagonal are all $\lambda$ (where $\lambda \in \mathbb{C}$), and every entry on the subdiagonal is 1. 1.11: JORDAN CANONICAL FORM The square matrix $A$ can be represented in the form of a Jordan normal form if [A = \left( \begin{array}{cccc} J_{\lambda_1} &amp; 0 &amp; \cdots &amp; 0 0 &amp; J_{\lambda_2} &amp; \ddots &amp; \vdots \vdots &amp; \ddots &amp; \ddots &amp; 0 0 &amp; \cdots &amp; 0 &amp; J_{\lambda_k} \end{array} \right)] with \(J_{\lambda_i}\) are Jordan blocks. 1.12: CHARACTERISTIC POLYNOMIAL A is an \(n \times n\) matrix, the characteristic polynomial of A, denoted by \(p_{A}(x)\) is defined by [p_{A}(x) = \operatorname{det}(A - xI)] where \(I\) denotes the \(n \times n\) identity matrix. 1.13: GEOMETRIC AND ALGEBRAIC MULTIPLICITY A is a square matrix with characteristic polynomial \(f(x) = (x- \lambda_1)^{m_1} (x - \lambda_2)^{m_2} \ldots (x- \lambda_r)^{m_r}\). Then \(m_i\) is called algebraic multiplicity corresponding to eigenvalue \(\lambda_i\), the dimension of eigenspace corresponding to eigenvalue \(\lambda_i\) is called geometric multiplicity. 1.14: COFACTOR OF MATRIX Consider the \(n \times n\) square matrix \(T\), we define the \(i,j\) minor \(M_{i,j}\) of matrix \(T\) is the \((n-1) \times (n-1)\) matrix obtained by eliminating the i-th row and j-th column in the matrix \(T\). Then we have \(i,j\) cofactor \(T_{i,j} = (-1)^{i+j} \operatorname{det}(M_{i,j})\) 1.15: MATRIX ADJOINT Matrix adjoint of square matrix T is denoted by [\text{adj}(T) = \left( \begin{array}{cccc} T_{1,1} &amp; T_{2,1} &amp; \cdots &amp; T_{n,1} T_{1,2} &amp; T_{2,2} &amp; \cdots &amp; T_{n,2} \vdots &amp; \vdots &amp; \ddots &amp; \vdots T_{1,n} &amp; T_{2,n} &amp; \cdots &amp; T_{n,n} \end{array} \right)] where \(T_{i,j}\) is the \(i,j\) cofactor of matrix T. 1.16: EQUIVALENCE RELATION A binary relation \(\sim\) on a set \(\mathbb{X}\) is said to be an equivalence relation if and only if it is relexive, symmetric and transitive. That is, for all \(a, b, c \in \mathbb{X}\) : Reflexive: \(a \sim a\) Symmetric: \(a \sim b\) if and only of \(b \sim a\) Transitive: If \(a \sim b\) and \(b \sim c\), then \(a \sim c\) 2. PROPERTIES 2.1: EQUIVALENCE RELATION PROPERTY 2.1: Matrix similarity is equivalence relation. We aim to prove that the similarity matrix relation satisfies the properties of reflexivity, symmetry, and transitivity. Reflexive: For any square matrix \(A\), it is obvious that \(I^{-1}AI = A\), where \(I\) is the identity matrix. Therefore, \(A\) is similar to itself. Symmetric: Suppose that B is similar to A,which means there exists an invertible matrix \(P\) such that \(B = P^{-1}AP\) \(\implies B P^{-1} = P^{-1}AI = P^{-1}A\) \(\iff PBP^{-1} = IA = A\) This implies A is also similar to B, so we have the similarity relation is symmetric. Transitive: Consider three matrices \(A\), \(B\), and \(C\) such that \(A\) is similar to \(B\) and \(B\) is similar to \(C\). Hence, there exist invertible matrices \(P\), \(Q\) such that \(B = P^{-1}AP\) and \(C = Q^{-1}BQ\). \(\implies C = Q^{-1}P^{-1}APQ = (PQ)^{-1}APQ\) Therefore, \(C\) is similar to \(A\), and matrix similarity relation is transitive. 2.2: SIMILAR LINEAR MAPPING PROPERTY 2.2: Similarity matrices are matrix representations of the same linear mapping but with respect to different bases. Lemma 1: Consider three vector spaces \(U,V,W\) and two linear mappings \(f \colon U \to V\) and \(g \colon V \to W\); \(B,D,E\) are the bases of 3 vector spaces, respectively. Then we have $$ \operatorname{Rep}{B,E}(f \circ g) = \operatorname{Rep}{B,D}(f) \cdot \operatorname{Rep}_{D,E}(g) $$ The notation id represent for identity matrix Consider two similar matrices \(H,G\) such that \(H= \operatorname{Rep}_{B,B}(h)\) and \(G= P^{-1}HP\), where \(B\) is the basis of vector space \(V\) including \(\overrightarrow{\beta_1}, \overrightarrow{\beta_2}, \ldots, \overrightarrow{\beta_n}\). \(h\) is a linear mapping. Consider the basis \(D\) such that \(\operatorname{Rep}_{D,B}(id) = P\) From the figure above, we have \(h = id \circ h \circ id\). Using Lemma 1 then we have [\operatorname{Rep}{D,D}(h) = \operatorname{Rep}{D,B}(id) \cdot \operatorname{Rep}{B,B}(h) \cdot \operatorname{Rep}{B,D}(id) = P^{-1}HP \iff G = \operatorname{Rep}_{D,D}(h) : : : (Q.E.D)] 2.3: SIMILAR RANK PROPERTY 2.3: Similarity matrices have the same rank. Lemma 2: \(f \colon U \to V\) is a linear mapping with a matrix representation \(A\). Then we have rank \(A\) equals to the dimension of the image space of linear mapping \(f\) So from Lemma 2 and Properties 2.1, we infer that similar matrices have the same rank. 2.4: SIMILAR DETERMINANT PROPERTY 2.4: Similarity matrices have the same determinant. Consider two similar matrices \(A, B\). Since \(A, B\) are two similar matrices, there exists an invertible matric P such that \(B = PAP^{-1}\) We have [\begin{align} |B| &amp;= |PAP^{-1}| &amp;= |P| |A| |P^{-1}| &amp;= |A| |PP^{-1}| &amp;= |A| : : \text{Determinant of identity matrix equals 1} \end{align}] So we have \(|A| = |B|\) (Q.E.D) 2.5: SIMILAR TRACE PROPERTY 2.5: Similarity matrices have the same trace. Lemma 3: \(M,N\) are two arbitrary square matrices, then we have \(\operatorname{tr}(MN) = \operatorname{tr}(NM)\). \ Since \(A, B\) are two similar matrices, there exists an invertible matric P such that \(B = PAP^{-1}\) \(\implies \operatorname{tr}(B) = \operatorname{tr}(P^{-1}AP)\) Using Lemma 3 we have : [\begin{align} \operatorname{tr}(B) &amp;= \operatorname{tr}(PAP^{-1}) &amp;= \operatorname{tr}(APP^{-1}) &amp;= \operatorname{tr}(AI) &amp;= \operatorname{tr}(A) \end{align}] So we have \(\operatorname{trace}(A) = \operatorname{trace}(B)\) Remark: If two square matrices of the same order \(A\) and \(B\) satisfy \(\operatorname{tr}(A) = \operatorname{tr}(B)\) and \(|A| = |B|\), we cannot conclude that \(A\) and \(B\) are similar matrices. Indeed, we will demonstrate a counterexample. Consider the two matrices \(A = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix}, B = \begin{pmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{pmatrix}.\) It is easy to see that \(|A| = \left| B \right| = 1, \quad \operatorname{tr}(A) = \operatorname{tr}(B) = 2.\) However, there was not exist an invertible matrix \(P\) such that \(B = PAP^{-1}\) because if it were so, then \(B = PAP^{-1} = PIP^{-1} = PP^{-1} = I\) which is absurd. Therefore, we conclude that \(A\) and \(B\) are not similar matrices. 2.6: SIMILARITY MATRIX ADJOINT PROPERTY 2.6: Matrix adjoint of two similar matrices are also similar. Lemma 4(Laplace Expansion of Determinants): For an \(n \times n\) square matrix \(T\) with the \(i,j\) cofactor \(T_{i,j}\), the entry at row \(i\), column \(j\) of matrix \(T\) is denoted by \(t_{i,j}\). Then we have: [\begin{align} |T| &amp;= t_{i,1}T_{i,1} + t_{i,2}T_{i,2} + \ldots + t_{i,n}T_{i,n} &amp;= t_{1,j}T_{1,j} + t_{2,j}T_{2,j} + \ldots + t_{n,j}T_{n,j} \end{align}] Lemma 5: Consider \(M_n\) is the set contains all square matrices of degree \(n\). Then the function \(f \colon M_n \to M_n\) that satisfy \(f(H) = adj(H), \forall H \in M_n\) is a continuous function. Consider the matrix \(T = \begin{pmatrix} t_{1,1} &amp; t_{1,2} &amp; \ldots &amp; t_{1,n} \\ t_{2,1} &amp; t_{2,2} &amp; \ldots &amp; t_{2,n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ t_{n,1} &amp; t_{n,2} &amp; \ldots &amp; t_{n,n} \end{pmatrix}\) with the adjugate \(\text{adj}(T) = \begin{pmatrix} T_{1,1} &amp; T_{2,1} &amp; \ldots &amp; T_{n,1} \\ T_{1,2} &amp; T_{2,2} &amp; \ldots &amp; T_{n,2} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ T_{1,n} &amp; T_{2,n} &amp; \ldots &amp; T_{n,n} \end{pmatrix}\) First, we prove that \(\sum_{j=1}^{n}t_{i,j} T_{k,j} = \sum_{j=1}^{n} t_{j,i}T_{j,k} = 0 \text{ for all } i \neq k\) Consider square matrix \(T\), and the matrix \(A\) defined by preserving all values of the matrix \(T\) except for the values in row \(k\); the values in row \(k\) of matrix \(A\) equal to row \(i\) in matrix \(T\).(which means row \(i\) and \(k\) of matrix A have the same values.) Similarly, matrix A have entries \(a_{i,j}\) and \(i,j\) cofactor \(A_{i,j}\). Since the row \(i\) and the row \(k\) of matrix \(A\) is the same, it implies that \(|A| = 0\). Using Lemma 4 we have [\begin{align} \sum_{j=1}^{n}t_{i,j} T_{k,j} &amp;= \sum_{j=1}^{n} a_{k,j}T_{k,j} &amp; \text{(Row i of T and row k of A are the same)} \ &amp;= \sum_{j=1}^{n} a_{k,j}A_{k,j} &amp; \text{(when eliminating row k, the remaining of A and T are the same)} &amp;= |A| &amp; \text{Lemma 4} \ &amp;= 0 &amp; \text{(A have 2 rows with the same values.)} \end{align}] Similarly, we prove that \(\sum_{j=1}^{n} t_{j,i}T_{j,k} = 0, \forall i \neq k\) Using the Lemma 4 we imply that [T \operatorname{adj}(T) = \operatorname{adj}(T) \cdot T = T \cdot I : : : : (1)] Consider two similar matrices A and B. There exist an invertible matrix \(P\) such that \(B = P^{-1}AP\) We prove the Property 2.5 in two cases: \(|A| = 0\) and \(|A| \neq 0\) Case 1: \(|A| \neq 0\) Using Property 2.4 we imply \(|B| \neq 0\), then \(A\) and \(B\) are invertible matrices. Using (1) we have [\begin{align} \operatorname{adj}(A) &amp; =\operatorname{det}(A) \cdot A^{-1} \ \operatorname{adj}(B) &amp; =\operatorname{det}(B) \cdot B^{-1} \ \operatorname{adj}(AB) &amp;= |AB| \cdot (AB)^{-1} &amp;= |A| \cdot |B| \cdot B^{-1} A^{-1} &amp;= |B| \cdot B^{-1} \cdot |A| \cdot A^{-1} &amp;= \operatorname{adj}(B) \operatorname{adj}(A) \end{align}] From the above, we imply the following: [\begin{align} \operatorname{adj}(B) &amp;= \operatorname{adj}(P^{-1}AP) &amp;= \operatorname{adj}(P (AP^{-1})) &amp;= \operatorname{adj}(AP^{-1}) \operatorname{adj}(P) &amp;= \operatorname{adj}(P^{-1}) \operatorname{adj}(A) \operatorname{adj}(P) \end{align}] Otherwise, from (1) we have \(T^{-1} \operatorname{adj}(T^{-1}) = |T^{-1}| \cdot I\) with an invertible matrix \(T\) \(\implies \operatorname{adj}(T^{-1}) = |T^{-1}| \cdot T\) But we have \(\operatorname{adj}(T) = |T| \cdot T^{-1} \implies \operatorname{adj}(T) \operatorname{adj}(T^{-1}) = I\) So we imply that \(\operatorname{adj}(B) = \operatorname{adj}(P)^{-1} \operatorname{adj}(A) \operatorname{adj}(P)\), which means \(\operatorname{adj}(A)\) and \(\operatorname{adj}(B)\) are two similar matrices . Case 2: \(|A| = 0\) From the Property 2.3 we imply \(|B| = 0\) For an arbitrary square matrix \(M\), we have \(M\) is an invertible matrix \(\iff |M| \neq 0\) Using Definition 1.12 we imply that matrix \(A + \lambda I\) is invertible for all \(\lambda\) such that \(\lambda\) is not an eigenvalue of matrix \(-A\). Since matrix \(-A\) has finite eigenvalues; therefore, for all \(\lambda\) small enough we have matrix \(A + \lambda I\) is invertible, then : [\operatorname{adj}((A + \lambda I)B) = \operatorname{adj}(B) \operatorname{adj}(A + \lambda I)] Using Lemma 5 we imply that : [\begin{aligned} \operatorname{adj}(A B) &amp; =\operatorname{adj}\left(\lim _{\lambda \rightarrow 0}(A+\lambda I) B\right) &amp; =\lim _{\lambda \rightarrow 0} \operatorname{adj}((A+\lambda I) B) &amp; =\lim _{\lambda \rightarrow 0} \operatorname{adj}(B) \cdot \operatorname{adj}(A+\lambda I) &amp; =\operatorname{adj}(B) \cdot \lim _{\lambda \rightarrow 0} \operatorname{adj}(A+\lambda I) &amp; =\operatorname{adj}(B) . \operatorname{adj}(A) \end{aligned}] Hence, by a similar proof as in Case 1, we conclude that A and B are two similar matrices. 2.7: SIMILAR EIGENVALUES PROPERTY 2.7: Similarity matrices have the same eigenvalues. Consider two similar matrices \(A\) and \(B\). Assume that B has an eigenvalue \(\lambda\) corresponding to eigenvector \(\vec{v}\). We have : [\begin{aligned} B \vec{v}=\lambda \vec{v} \ \Leftrightarrow &amp; P^{-1} A P \vec{v}=\lambda \vec{v} \ \Leftrightarrow &amp; A P \vec{v}=P \lambda \vec{v}=\lambda P \vec{v} \end{aligned}] \(\implies\) Every eigenvalue \(\lambda\) of matrix \(B\) is also an eigenvalue of matrix \(A\) Using similarly proof, we have every eigenvalue of matrix \(A\) is also an eigenvalue of matrix \(B\); therefore, \(A\) and \(B\) have same eigenvalues. (Q.E.D) 2.8: SIMILAR CHARACTERISTIC AND MINIMAL POLYNOMIAL PROPERTY 2.8: \(A\) and \(B\) are two similar matrices, then \(A\) and \(B\) have the same characteristic polynomial and minimal polynomial. The characteristic polynomial of matrix \(A\) and \(B\) is \(\operatorname{det}(A - xI)\) and \(\operatorname{det}(B - xI)\), respectively, with \(x\) is a variable. We have: [\begin{aligned} \operatorname{det}(B-x I) &amp; =\operatorname{det}\left(P^{-1} A P-x I\right) \ &amp; =\operatorname{det}\left(P^{-1} A P-P^{-1} x I P\right) \ &amp; =\operatorname{det}\left(P^{-1}(A-x I) P\right) \ &amp; =\operatorname{det}\left(P^{-1}\right) \operatorname{det}(A-x I) \operatorname{det} P \ &amp; =\operatorname{det}(A-x I) \operatorname{det} P \cdot \operatorname{det}\left(P^{-1}\right) \ &amp; =\operatorname{det}(A-x I) \end{aligned}] So we conclude that \(A\) and \(B\) have same characteristic polynomial. \ Consider polynomial \(f(x)=x^n+a_{n-1} x^{n-1}+\ldots+a_1 x+a_0\) is a minimal polynomial of matrix \(A\). \(\implies f(A)=0\). Then we have : [\begin{aligned} f\left( B \right) &amp;= f\left(P^{-1} A P\right) &amp;= \left(P^{-1} A P\right)^n+a_{n-1}\left(P^{-1} A P\right)^{n-1}+\ldots+a_n\left(P^{-1} A P\right) +a_0 I \ \left(P^{-1} A P\right)^n &amp;= P^{-1} A P \cdot P^{-1} A P \ldots P^{-1} A P &amp;=P^{-1} A\left(P P^{-1}\right) \cdot A\left(P P^{-1}\right) \ldots\left(P P^{-1}\right) A P &amp;=P^{-1} A^n P \ \implies f(B) &amp;= P^{-1} A^n P +a_{n-1} P^{-1} A^{n-1} P +\ldots +a_1 P^{-1} A P+a_0 P^{-1} I P &amp;=P^{-1}\left(A^n+a_{n-1} A^{n-1}+\ldots+a_1 A+a_0 I\right) P &amp;=P^{-1} \cdot f(A) \cdot P &amp;=0 \end{aligned}] Consider \(g(x)\) is the minimal polynomial of matrix \(B\), since \(f(B) = 0\) then we imply \(\operatorname{deg}(f) \geq \operatorname{deg}(g)\). Using similarly proof, we have \(\operatorname{deg}(g) \geq \operatorname{deg}(f)\), then we imply that \(\operatorname{deg}(f) = \operatorname{deg}(g)\). Assume that \(f(x) \neq g(x)\), then consider \(h(x) = f(x) - g(x)\). We have \(h(x)\) has smaller degree than \(f , g\), and \(h(x)\) is not a zero polynomial. \(\implies h(B) = f(B) - g(B) = 0\) is a characteristic polynomial of matrix \(B\) with smaller degree than its minimal polynomial \(g(x)\) (CONTRADICTION). Therefore, we have \(f(x) = g(x)\), which means two matrices \(A\) and \(B\) have same minimal polynomial. 2.9: SIMILAR GEOMETRIC MULTIPLICITY, ALGEBRAIC MULTIPLICITY PROPERTY 2.9: \(A\) and \(B\) are two similar matrices. By Property 2.7 they also have same eigenvalues. Then for each eigenvalue, matrix \(A\) and \(B\) have same geometric multiplicity and algebraic multiplicity corresponding to this eigenvalue. From Property 2.7 we have \(A\) anh \(B\) have same chracteristic polynomial. By Definition 1.13 we imply that these two matrices have same algebraic multiplicity for each eigenvalue. Since \(A\) and \(B\) are similar, so there exist invertible matrix \(P\) such that \(B=P^{-1}AP\). Consider \(\lambda\) is an eigenvalue of matrix \(A\) and \(\left( \vec{v_1}, \vec{v_2}, \ldots, \vec{v_k} \right)\) is a basis of eigenspace corresponding to eigenvalue \(\lambda\) Using proof in Property 2.7 we imply that \(P \vec{v_1}, P \vec{v_2}, \ldots, P \vec{v_k}\) are eigenvectors corresponding to the eigenvalue \(\lambda\) of the matrix \(B\). We prove that \(\left( P^{-1} \vec{v_1}, P^{-1} \vec{v_2}, \ldots, P^{-1} \vec{v_k} \right)\) is the basis of eigenspace corresponding to eigenvalue \(\lambda\) of matrix \(B\). Assume there exists \(\alpha_1, \alpha_2, \ldots, \alpha_n\) such that [\begin{aligned} &amp; \alpha_1 P^{-1} \vec{v_1}+\alpha_2 P^{-1} \vec{v_2}+\ldots+\alpha_k P^{-1} \vec{v_k}=\vec{0} \ \Rightarrow &amp; \alpha_1 \vec{v_1}+\alpha_2 \vec{v_2}+\ldots+\alpha_k \vec{v_k}= P \cdot \vec{0}= \vec{0} \end{aligned}] Since \(\left( \vec{v_1}, \vec{v_2}, \ldots, \vec{v_k} \right)\) is a basis of vector space with dimension \(k\), then we have \(\vec{\alpha_1} = \vec{\alpha_2} = \ldots = \vec{\alpha_k} = 0\) Hence, we have \(\left(P^{-1} \vec{v_1}, P^{-1} \vec{v_2}, \ldots, P^{-1} \vec{v_k} \right)\) is linear independent. Assume there exists vector \(\vec{v}\) is a eigenvector of matrix \(B\) that correspond to eigenvalue \(\lambda\) such that \(\vec{v} \notin \operatorname{Span}({P^{-1} \vec{v_1}, P^{-1} \vec{v_2}, \ldots, P^{-1} \vec{v_k}})\) Hence, there exists eigenvector \(\vec{u}\) corresponding to eigenvalue \(\lambda\) of matrix \(A\) such that \(\vec{v} = P^{-1} \vec{u}\). Then we imply that \(\vec{u} \notin \operatorname{Span}({\vec{v_1}, \vec{v_2}, \ldots, \vec{v_k} })\) (CONTRADICTION) Therefore, we can imply that \(\left( P^{-1} \vec{v_1}, P^{-1} \vec{v_2}, \ldots, P^{-1} \vec{v_k} \right)\) is the basis of eigenspace corresponding to eigenvalue \(\lambda\) of matrix \(B\). Hence, we can conclude that eigenspace corresponding to eigenvalue \(\lambda\) of \(A\) and \(B\) have same dimension. Therefore, by definition, similar matrices have same geometric multiplicity. (Q.E.D)" />
<link rel="canonical" href="http://localhost:4000/posts/similarity-matrix/" />
<meta property="og:url" content="http://localhost:4000/posts/similarity-matrix/" />
<meta property="og:site_name" content="NgocQuan’s Research" />
<meta property="og:image" content="/assets/img/similarity_matrix/similarity_matrix_resize.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-02-10T00:00:00+07:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="/assets/img/similarity_matrix/similarity_matrix_resize.png" />
<meta property="twitter:title" content="Similar Properties of Similarity Matrix" />
<meta name="twitter:site" content="@twitter_username" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-02-10T00:00:00+07:00","datePublished":"2023-02-10T00:00:00+07:00","description":"Abstract: Firstly, I will introduce specific definitions relevant to matrix similarity topic. Subsequently, I will explore various common properties of similar matrices, such as trace, rank, determinant, among others. Finally, I will present criteria for determining whether any two given matrices are similar. DEFINITIONS 1.1: SIMILARITY MATRIX Two square \\(n \\times n\\) matrices A and B are called similar if there exists an invertible \\(n*n\\) matrix \\(P\\) such that [B = P^{-1}AP] 1.2: REPRESENTATION OF A VECTOR \\(V\\) is a vector space with the basis \\(B=\\left&lt;\\overrightarrow{\\beta_1}, \\overrightarrow{\\beta_2}, \\ldots, \\overrightarrow{\\beta_n}\\right&gt; , \\vec{v} \\in V, \\alpha_1, \\ldots, \\alpha_n \\in \\mathbb{R}\\) such that \\(\\vec{v}=\\alpha_1 \\overrightarrow{\\beta_1}+\\alpha_2 \\overrightarrow{\\beta_2}+\\ldots+\\alpha_n \\overrightarrow{\\beta_n}.\\) Then we have [{Rep}_B(\\vec{v})=\\left(\\begin{array}{c} \\alpha_1 \\ \\alpha_2 \\ \\vdots \\ \\alpha_n \\end{array}\\right)] 1.3: MATRIX REPRESENTATION \\(V, W\\) are vector spaces with dimension \\(n, m\\) and basis \\(B, D\\). \\(f \\colon V \\to W\\) is a linear mapping. If we have \\(\\operatorname{Rep}_D(h(\\overrightarrow{\\beta_1}))=\\left(\\begin{array}{c}h_{1,1} \\\\ h_{2,1} \\\\ \\vdots \\\\ h_{n, 1}\\end{array}\\right), \\ldots, \\operatorname{Rep}_D(h(\\overrightarrow{\\beta_n}))=\\left(\\begin{array}{c}h_{1, n} \\\\ h_{2, n} \\\\ \\vdots \\\\ h_{n, n}\\end{array}\\right)\\). Then [\\left(\\begin{array}{cccc}h_{1,1} &amp; h_{1,2} &amp; \\cdots &amp; h_{1, n} \\ h_{2,1} &amp; h_{2,2} &amp; \\cdots &amp; h_{2, n} \\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\ h_{n, 1} &amp; h_{n, 2} &amp; \\cdots &amp; h_{n, n}\\end{array}\\right)] is called matrix representation of mapping f with respect to the bases \\(B, D\\), notated by \\({Rep}_{B,D}(f)\\) 1.4: DETERMINANT OF MATRIX \\(n \\times n\\) determinant is a function \\(f \\colon M_{n \\times n} \\rightarrow \\mathbb{R}\\) such that (1) \\(\\operatorname{det}\\left(\\overrightarrow{p_1}, \\ldots, k \\overrightarrow{p_i}+\\overrightarrow{p_j}, \\ldots, \\overrightarrow{p_n}\\right)=\\operatorname{det}\\left(\\overrightarrow{p_1}, \\ldots, \\overrightarrow{p_j}, \\ldots, \\overrightarrow{p_n}\\right)\\), for all \\(k \\in \\mathbb{R}\\) (2) \\(\\operatorname{det}\\left(\\overrightarrow{p_1}, \\ldots, \\overrightarrow{p_j}, \\ldots, \\overrightarrow{p_i}, \\ldots, \\overrightarrow{p_n}\\right)=-\\operatorname{det}\\left(\\overrightarrow{p_1}, \\ldots, \\overrightarrow{p_i}, \\ldots, \\overrightarrow{p_j}, \\ldots, \\overrightarrow{p_n}\\right)\\) (3) \\(\\operatorname{det}\\left(\\overrightarrow{p_1}, \\ldots, k \\overrightarrow{p_i}, \\ldots, \\overrightarrow{p_n}\\right) = k \\operatorname{det}\\left(\\overrightarrow{p_1}, \\ldots, \\overrightarrow{p_i}, \\ldots, \\overrightarrow{p_n}\\right)\\), for all \\(k \\in \\mathbb{R}\\) (4) \\(\\operatorname{det}(I) = 1\\), \\(I\\) is the identity matrix. Normally, we use the notation \\(|T|\\) instead of \\(\\operatorname{det}(T)\\). 1.5: TRACE OF MATRIX The trace of a square matrix \\(A\\) is the sum of elements on the main diagonal of matrix \\(A\\), denoted by \\(tr(A)\\) 1.6: POLYNOMIAL OF MATRIX \\(T\\) is a square matrix, \\(f(x)\\) is a polynomial with degree n \\(f(x) = c_n x^n + c_{n-1} x^{n-1} + \\cdots + c_1 x + c_0\\) \\((c_i \\in \\mathbb{R})\\). Then we have \\(f(T) = c_n T^n + c_{n-1} T^{n-1} + \\cdots + c_1 T + c_0 I\\) (\\(I\\) is identity matrix) is a polynomial of matrix \\(T\\) and also a square matrix of the same size as \\(T\\). 1.7: MINIMAL POLYNOMIAL For a square matrix \\(T\\), its minimal polynomial is the monic polynomial \\(f\\) of the lowest degree such that \\(f(T) = 0\\). 1.8: INDEX OF NIPOTENT A square matrix $A$ is said to have a nilpotent index $k$ if \\(k\\) is the smallest natural number(if exists) such that $A^k = 0$. 1.9: SUBDIAGONAL The subdiagonal of a square matrix is the line immediately below the main diagonal of the matrix. 1.10: JORDAN BLOCK A Jordan block is a square matrix where the off-diagonal entries are zero, the entries on the main diagonal are all $\\lambda$ (where $\\lambda \\in \\mathbb{C}$), and every entry on the subdiagonal is 1. 1.11: JORDAN CANONICAL FORM The square matrix $A$ can be represented in the form of a Jordan normal form if [A = \\left( \\begin{array}{cccc} J_{\\lambda_1} &amp; 0 &amp; \\cdots &amp; 0 0 &amp; J_{\\lambda_2} &amp; \\ddots &amp; \\vdots \\vdots &amp; \\ddots &amp; \\ddots &amp; 0 0 &amp; \\cdots &amp; 0 &amp; J_{\\lambda_k} \\end{array} \\right)] with \\(J_{\\lambda_i}\\) are Jordan blocks. 1.12: CHARACTERISTIC POLYNOMIAL A is an \\(n \\times n\\) matrix, the characteristic polynomial of A, denoted by \\(p_{A}(x)\\) is defined by [p_{A}(x) = \\operatorname{det}(A - xI)] where \\(I\\) denotes the \\(n \\times n\\) identity matrix. 1.13: GEOMETRIC AND ALGEBRAIC MULTIPLICITY A is a square matrix with characteristic polynomial \\(f(x) = (x- \\lambda_1)^{m_1} (x - \\lambda_2)^{m_2} \\ldots (x- \\lambda_r)^{m_r}\\). Then \\(m_i\\) is called algebraic multiplicity corresponding to eigenvalue \\(\\lambda_i\\), the dimension of eigenspace corresponding to eigenvalue \\(\\lambda_i\\) is called geometric multiplicity. 1.14: COFACTOR OF MATRIX Consider the \\(n \\times n\\) square matrix \\(T\\), we define the \\(i,j\\) minor \\(M_{i,j}\\) of matrix \\(T\\) is the \\((n-1) \\times (n-1)\\) matrix obtained by eliminating the i-th row and j-th column in the matrix \\(T\\). Then we have \\(i,j\\) cofactor \\(T_{i,j} = (-1)^{i+j} \\operatorname{det}(M_{i,j})\\) 1.15: MATRIX ADJOINT Matrix adjoint of square matrix T is denoted by [\\text{adj}(T) = \\left( \\begin{array}{cccc} T_{1,1} &amp; T_{2,1} &amp; \\cdots &amp; T_{n,1} T_{1,2} &amp; T_{2,2} &amp; \\cdots &amp; T_{n,2} \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots T_{1,n} &amp; T_{2,n} &amp; \\cdots &amp; T_{n,n} \\end{array} \\right)] where \\(T_{i,j}\\) is the \\(i,j\\) cofactor of matrix T. 1.16: EQUIVALENCE RELATION A binary relation \\(\\sim\\) on a set \\(\\mathbb{X}\\) is said to be an equivalence relation if and only if it is relexive, symmetric and transitive. That is, for all \\(a, b, c \\in \\mathbb{X}\\) : Reflexive: \\(a \\sim a\\) Symmetric: \\(a \\sim b\\) if and only of \\(b \\sim a\\) Transitive: If \\(a \\sim b\\) and \\(b \\sim c\\), then \\(a \\sim c\\) 2. PROPERTIES 2.1: EQUIVALENCE RELATION PROPERTY 2.1: Matrix similarity is equivalence relation. We aim to prove that the similarity matrix relation satisfies the properties of reflexivity, symmetry, and transitivity. Reflexive: For any square matrix \\(A\\), it is obvious that \\(I^{-1}AI = A\\), where \\(I\\) is the identity matrix. Therefore, \\(A\\) is similar to itself. Symmetric: Suppose that B is similar to A,which means there exists an invertible matrix \\(P\\) such that \\(B = P^{-1}AP\\) \\(\\implies B P^{-1} = P^{-1}AI = P^{-1}A\\) \\(\\iff PBP^{-1} = IA = A\\) This implies A is also similar to B, so we have the similarity relation is symmetric. Transitive: Consider three matrices \\(A\\), \\(B\\), and \\(C\\) such that \\(A\\) is similar to \\(B\\) and \\(B\\) is similar to \\(C\\). Hence, there exist invertible matrices \\(P\\), \\(Q\\) such that \\(B = P^{-1}AP\\) and \\(C = Q^{-1}BQ\\). \\(\\implies C = Q^{-1}P^{-1}APQ = (PQ)^{-1}APQ\\) Therefore, \\(C\\) is similar to \\(A\\), and matrix similarity relation is transitive. 2.2: SIMILAR LINEAR MAPPING PROPERTY 2.2: Similarity matrices are matrix representations of the same linear mapping but with respect to different bases. Lemma 1: Consider three vector spaces \\(U,V,W\\) and two linear mappings \\(f \\colon U \\to V\\) and \\(g \\colon V \\to W\\); \\(B,D,E\\) are the bases of 3 vector spaces, respectively. Then we have $$ \\operatorname{Rep}{B,E}(f \\circ g) = \\operatorname{Rep}{B,D}(f) \\cdot \\operatorname{Rep}_{D,E}(g) $$ The notation id represent for identity matrix Consider two similar matrices \\(H,G\\) such that \\(H= \\operatorname{Rep}_{B,B}(h)\\) and \\(G= P^{-1}HP\\), where \\(B\\) is the basis of vector space \\(V\\) including \\(\\overrightarrow{\\beta_1}, \\overrightarrow{\\beta_2}, \\ldots, \\overrightarrow{\\beta_n}\\). \\(h\\) is a linear mapping. Consider the basis \\(D\\) such that \\(\\operatorname{Rep}_{D,B}(id) = P\\) From the figure above, we have \\(h = id \\circ h \\circ id\\). Using Lemma 1 then we have [\\operatorname{Rep}{D,D}(h) = \\operatorname{Rep}{D,B}(id) \\cdot \\operatorname{Rep}{B,B}(h) \\cdot \\operatorname{Rep}{B,D}(id) = P^{-1}HP \\iff G = \\operatorname{Rep}_{D,D}(h) : : : (Q.E.D)] 2.3: SIMILAR RANK PROPERTY 2.3: Similarity matrices have the same rank. Lemma 2: \\(f \\colon U \\to V\\) is a linear mapping with a matrix representation \\(A\\). Then we have rank \\(A\\) equals to the dimension of the image space of linear mapping \\(f\\) So from Lemma 2 and Properties 2.1, we infer that similar matrices have the same rank. 2.4: SIMILAR DETERMINANT PROPERTY 2.4: Similarity matrices have the same determinant. Consider two similar matrices \\(A, B\\). Since \\(A, B\\) are two similar matrices, there exists an invertible matric P such that \\(B = PAP^{-1}\\) We have [\\begin{align} |B| &amp;= |PAP^{-1}| &amp;= |P| |A| |P^{-1}| &amp;= |A| |PP^{-1}| &amp;= |A| : : \\text{Determinant of identity matrix equals 1} \\end{align}] So we have \\(|A| = |B|\\) (Q.E.D) 2.5: SIMILAR TRACE PROPERTY 2.5: Similarity matrices have the same trace. Lemma 3: \\(M,N\\) are two arbitrary square matrices, then we have \\(\\operatorname{tr}(MN) = \\operatorname{tr}(NM)\\). \\ Since \\(A, B\\) are two similar matrices, there exists an invertible matric P such that \\(B = PAP^{-1}\\) \\(\\implies \\operatorname{tr}(B) = \\operatorname{tr}(P^{-1}AP)\\) Using Lemma 3 we have : [\\begin{align} \\operatorname{tr}(B) &amp;= \\operatorname{tr}(PAP^{-1}) &amp;= \\operatorname{tr}(APP^{-1}) &amp;= \\operatorname{tr}(AI) &amp;= \\operatorname{tr}(A) \\end{align}] So we have \\(\\operatorname{trace}(A) = \\operatorname{trace}(B)\\) Remark: If two square matrices of the same order \\(A\\) and \\(B\\) satisfy \\(\\operatorname{tr}(A) = \\operatorname{tr}(B)\\) and \\(|A| = |B|\\), we cannot conclude that \\(A\\) and \\(B\\) are similar matrices. Indeed, we will demonstrate a counterexample. Consider the two matrices \\(A = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix}, B = \\begin{pmatrix} 1 &amp; 1 \\\\ 0 &amp; 1 \\end{pmatrix}.\\) It is easy to see that \\(|A| = \\left| B \\right| = 1, \\quad \\operatorname{tr}(A) = \\operatorname{tr}(B) = 2.\\) However, there was not exist an invertible matrix \\(P\\) such that \\(B = PAP^{-1}\\) because if it were so, then \\(B = PAP^{-1} = PIP^{-1} = PP^{-1} = I\\) which is absurd. Therefore, we conclude that \\(A\\) and \\(B\\) are not similar matrices. 2.6: SIMILARITY MATRIX ADJOINT PROPERTY 2.6: Matrix adjoint of two similar matrices are also similar. Lemma 4(Laplace Expansion of Determinants): For an \\(n \\times n\\) square matrix \\(T\\) with the \\(i,j\\) cofactor \\(T_{i,j}\\), the entry at row \\(i\\), column \\(j\\) of matrix \\(T\\) is denoted by \\(t_{i,j}\\). Then we have: [\\begin{align} |T| &amp;= t_{i,1}T_{i,1} + t_{i,2}T_{i,2} + \\ldots + t_{i,n}T_{i,n} &amp;= t_{1,j}T_{1,j} + t_{2,j}T_{2,j} + \\ldots + t_{n,j}T_{n,j} \\end{align}] Lemma 5: Consider \\(M_n\\) is the set contains all square matrices of degree \\(n\\). Then the function \\(f \\colon M_n \\to M_n\\) that satisfy \\(f(H) = adj(H), \\forall H \\in M_n\\) is a continuous function. Consider the matrix \\(T = \\begin{pmatrix} t_{1,1} &amp; t_{1,2} &amp; \\ldots &amp; t_{1,n} \\\\ t_{2,1} &amp; t_{2,2} &amp; \\ldots &amp; t_{2,n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ t_{n,1} &amp; t_{n,2} &amp; \\ldots &amp; t_{n,n} \\end{pmatrix}\\) with the adjugate \\(\\text{adj}(T) = \\begin{pmatrix} T_{1,1} &amp; T_{2,1} &amp; \\ldots &amp; T_{n,1} \\\\ T_{1,2} &amp; T_{2,2} &amp; \\ldots &amp; T_{n,2} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ T_{1,n} &amp; T_{2,n} &amp; \\ldots &amp; T_{n,n} \\end{pmatrix}\\) First, we prove that \\(\\sum_{j=1}^{n}t_{i,j} T_{k,j} = \\sum_{j=1}^{n} t_{j,i}T_{j,k} = 0 \\text{ for all } i \\neq k\\) Consider square matrix \\(T\\), and the matrix \\(A\\) defined by preserving all values of the matrix \\(T\\) except for the values in row \\(k\\); the values in row \\(k\\) of matrix \\(A\\) equal to row \\(i\\) in matrix \\(T\\).(which means row \\(i\\) and \\(k\\) of matrix A have the same values.) Similarly, matrix A have entries \\(a_{i,j}\\) and \\(i,j\\) cofactor \\(A_{i,j}\\). Since the row \\(i\\) and the row \\(k\\) of matrix \\(A\\) is the same, it implies that \\(|A| = 0\\). Using Lemma 4 we have [\\begin{align} \\sum_{j=1}^{n}t_{i,j} T_{k,j} &amp;= \\sum_{j=1}^{n} a_{k,j}T_{k,j} &amp; \\text{(Row i of T and row k of A are the same)} \\ &amp;= \\sum_{j=1}^{n} a_{k,j}A_{k,j} &amp; \\text{(when eliminating row k, the remaining of A and T are the same)} &amp;= |A| &amp; \\text{Lemma 4} \\ &amp;= 0 &amp; \\text{(A have 2 rows with the same values.)} \\end{align}] Similarly, we prove that \\(\\sum_{j=1}^{n} t_{j,i}T_{j,k} = 0, \\forall i \\neq k\\) Using the Lemma 4 we imply that [T \\operatorname{adj}(T) = \\operatorname{adj}(T) \\cdot T = T \\cdot I : : : : (1)] Consider two similar matrices A and B. There exist an invertible matrix \\(P\\) such that \\(B = P^{-1}AP\\) We prove the Property 2.5 in two cases: \\(|A| = 0\\) and \\(|A| \\neq 0\\) Case 1: \\(|A| \\neq 0\\) Using Property 2.4 we imply \\(|B| \\neq 0\\), then \\(A\\) and \\(B\\) are invertible matrices. Using (1) we have [\\begin{align} \\operatorname{adj}(A) &amp; =\\operatorname{det}(A) \\cdot A^{-1} \\ \\operatorname{adj}(B) &amp; =\\operatorname{det}(B) \\cdot B^{-1} \\ \\operatorname{adj}(AB) &amp;= |AB| \\cdot (AB)^{-1} &amp;= |A| \\cdot |B| \\cdot B^{-1} A^{-1} &amp;= |B| \\cdot B^{-1} \\cdot |A| \\cdot A^{-1} &amp;= \\operatorname{adj}(B) \\operatorname{adj}(A) \\end{align}] From the above, we imply the following: [\\begin{align} \\operatorname{adj}(B) &amp;= \\operatorname{adj}(P^{-1}AP) &amp;= \\operatorname{adj}(P (AP^{-1})) &amp;= \\operatorname{adj}(AP^{-1}) \\operatorname{adj}(P) &amp;= \\operatorname{adj}(P^{-1}) \\operatorname{adj}(A) \\operatorname{adj}(P) \\end{align}] Otherwise, from (1) we have \\(T^{-1} \\operatorname{adj}(T^{-1}) = |T^{-1}| \\cdot I\\) with an invertible matrix \\(T\\) \\(\\implies \\operatorname{adj}(T^{-1}) = |T^{-1}| \\cdot T\\) But we have \\(\\operatorname{adj}(T) = |T| \\cdot T^{-1} \\implies \\operatorname{adj}(T) \\operatorname{adj}(T^{-1}) = I\\) So we imply that \\(\\operatorname{adj}(B) = \\operatorname{adj}(P)^{-1} \\operatorname{adj}(A) \\operatorname{adj}(P)\\), which means \\(\\operatorname{adj}(A)\\) and \\(\\operatorname{adj}(B)\\) are two similar matrices . Case 2: \\(|A| = 0\\) From the Property 2.3 we imply \\(|B| = 0\\) For an arbitrary square matrix \\(M\\), we have \\(M\\) is an invertible matrix \\(\\iff |M| \\neq 0\\) Using Definition 1.12 we imply that matrix \\(A + \\lambda I\\) is invertible for all \\(\\lambda\\) such that \\(\\lambda\\) is not an eigenvalue of matrix \\(-A\\). Since matrix \\(-A\\) has finite eigenvalues; therefore, for all \\(\\lambda\\) small enough we have matrix \\(A + \\lambda I\\) is invertible, then : [\\operatorname{adj}((A + \\lambda I)B) = \\operatorname{adj}(B) \\operatorname{adj}(A + \\lambda I)] Using Lemma 5 we imply that : [\\begin{aligned} \\operatorname{adj}(A B) &amp; =\\operatorname{adj}\\left(\\lim _{\\lambda \\rightarrow 0}(A+\\lambda I) B\\right) &amp; =\\lim _{\\lambda \\rightarrow 0} \\operatorname{adj}((A+\\lambda I) B) &amp; =\\lim _{\\lambda \\rightarrow 0} \\operatorname{adj}(B) \\cdot \\operatorname{adj}(A+\\lambda I) &amp; =\\operatorname{adj}(B) \\cdot \\lim _{\\lambda \\rightarrow 0} \\operatorname{adj}(A+\\lambda I) &amp; =\\operatorname{adj}(B) . \\operatorname{adj}(A) \\end{aligned}] Hence, by a similar proof as in Case 1, we conclude that A and B are two similar matrices. 2.7: SIMILAR EIGENVALUES PROPERTY 2.7: Similarity matrices have the same eigenvalues. Consider two similar matrices \\(A\\) and \\(B\\). Assume that B has an eigenvalue \\(\\lambda\\) corresponding to eigenvector \\(\\vec{v}\\). We have : [\\begin{aligned} B \\vec{v}=\\lambda \\vec{v} \\ \\Leftrightarrow &amp; P^{-1} A P \\vec{v}=\\lambda \\vec{v} \\ \\Leftrightarrow &amp; A P \\vec{v}=P \\lambda \\vec{v}=\\lambda P \\vec{v} \\end{aligned}] \\(\\implies\\) Every eigenvalue \\(\\lambda\\) of matrix \\(B\\) is also an eigenvalue of matrix \\(A\\) Using similarly proof, we have every eigenvalue of matrix \\(A\\) is also an eigenvalue of matrix \\(B\\); therefore, \\(A\\) and \\(B\\) have same eigenvalues. (Q.E.D) 2.8: SIMILAR CHARACTERISTIC AND MINIMAL POLYNOMIAL PROPERTY 2.8: \\(A\\) and \\(B\\) are two similar matrices, then \\(A\\) and \\(B\\) have the same characteristic polynomial and minimal polynomial. The characteristic polynomial of matrix \\(A\\) and \\(B\\) is \\(\\operatorname{det}(A - xI)\\) and \\(\\operatorname{det}(B - xI)\\), respectively, with \\(x\\) is a variable. We have: [\\begin{aligned} \\operatorname{det}(B-x I) &amp; =\\operatorname{det}\\left(P^{-1} A P-x I\\right) \\ &amp; =\\operatorname{det}\\left(P^{-1} A P-P^{-1} x I P\\right) \\ &amp; =\\operatorname{det}\\left(P^{-1}(A-x I) P\\right) \\ &amp; =\\operatorname{det}\\left(P^{-1}\\right) \\operatorname{det}(A-x I) \\operatorname{det} P \\ &amp; =\\operatorname{det}(A-x I) \\operatorname{det} P \\cdot \\operatorname{det}\\left(P^{-1}\\right) \\ &amp; =\\operatorname{det}(A-x I) \\end{aligned}] So we conclude that \\(A\\) and \\(B\\) have same characteristic polynomial. \\ Consider polynomial \\(f(x)=x^n+a_{n-1} x^{n-1}+\\ldots+a_1 x+a_0\\) is a minimal polynomial of matrix \\(A\\). \\(\\implies f(A)=0\\). Then we have : [\\begin{aligned} f\\left( B \\right) &amp;= f\\left(P^{-1} A P\\right) &amp;= \\left(P^{-1} A P\\right)^n+a_{n-1}\\left(P^{-1} A P\\right)^{n-1}+\\ldots+a_n\\left(P^{-1} A P\\right) +a_0 I \\ \\left(P^{-1} A P\\right)^n &amp;= P^{-1} A P \\cdot P^{-1} A P \\ldots P^{-1} A P &amp;=P^{-1} A\\left(P P^{-1}\\right) \\cdot A\\left(P P^{-1}\\right) \\ldots\\left(P P^{-1}\\right) A P &amp;=P^{-1} A^n P \\ \\implies f(B) &amp;= P^{-1} A^n P +a_{n-1} P^{-1} A^{n-1} P +\\ldots +a_1 P^{-1} A P+a_0 P^{-1} I P &amp;=P^{-1}\\left(A^n+a_{n-1} A^{n-1}+\\ldots+a_1 A+a_0 I\\right) P &amp;=P^{-1} \\cdot f(A) \\cdot P &amp;=0 \\end{aligned}] Consider \\(g(x)\\) is the minimal polynomial of matrix \\(B\\), since \\(f(B) = 0\\) then we imply \\(\\operatorname{deg}(f) \\geq \\operatorname{deg}(g)\\). Using similarly proof, we have \\(\\operatorname{deg}(g) \\geq \\operatorname{deg}(f)\\), then we imply that \\(\\operatorname{deg}(f) = \\operatorname{deg}(g)\\). Assume that \\(f(x) \\neq g(x)\\), then consider \\(h(x) = f(x) - g(x)\\). We have \\(h(x)\\) has smaller degree than \\(f , g\\), and \\(h(x)\\) is not a zero polynomial. \\(\\implies h(B) = f(B) - g(B) = 0\\) is a characteristic polynomial of matrix \\(B\\) with smaller degree than its minimal polynomial \\(g(x)\\) (CONTRADICTION). Therefore, we have \\(f(x) = g(x)\\), which means two matrices \\(A\\) and \\(B\\) have same minimal polynomial. 2.9: SIMILAR GEOMETRIC MULTIPLICITY, ALGEBRAIC MULTIPLICITY PROPERTY 2.9: \\(A\\) and \\(B\\) are two similar matrices. By Property 2.7 they also have same eigenvalues. Then for each eigenvalue, matrix \\(A\\) and \\(B\\) have same geometric multiplicity and algebraic multiplicity corresponding to this eigenvalue. From Property 2.7 we have \\(A\\) anh \\(B\\) have same chracteristic polynomial. By Definition 1.13 we imply that these two matrices have same algebraic multiplicity for each eigenvalue. Since \\(A\\) and \\(B\\) are similar, so there exist invertible matrix \\(P\\) such that \\(B=P^{-1}AP\\). Consider \\(\\lambda\\) is an eigenvalue of matrix \\(A\\) and \\(\\left( \\vec{v_1}, \\vec{v_2}, \\ldots, \\vec{v_k} \\right)\\) is a basis of eigenspace corresponding to eigenvalue \\(\\lambda\\) Using proof in Property 2.7 we imply that \\(P \\vec{v_1}, P \\vec{v_2}, \\ldots, P \\vec{v_k}\\) are eigenvectors corresponding to the eigenvalue \\(\\lambda\\) of the matrix \\(B\\). We prove that \\(\\left( P^{-1} \\vec{v_1}, P^{-1} \\vec{v_2}, \\ldots, P^{-1} \\vec{v_k} \\right)\\) is the basis of eigenspace corresponding to eigenvalue \\(\\lambda\\) of matrix \\(B\\). Assume there exists \\(\\alpha_1, \\alpha_2, \\ldots, \\alpha_n\\) such that [\\begin{aligned} &amp; \\alpha_1 P^{-1} \\vec{v_1}+\\alpha_2 P^{-1} \\vec{v_2}+\\ldots+\\alpha_k P^{-1} \\vec{v_k}=\\vec{0} \\ \\Rightarrow &amp; \\alpha_1 \\vec{v_1}+\\alpha_2 \\vec{v_2}+\\ldots+\\alpha_k \\vec{v_k}= P \\cdot \\vec{0}= \\vec{0} \\end{aligned}] Since \\(\\left( \\vec{v_1}, \\vec{v_2}, \\ldots, \\vec{v_k} \\right)\\) is a basis of vector space with dimension \\(k\\), then we have \\(\\vec{\\alpha_1} = \\vec{\\alpha_2} = \\ldots = \\vec{\\alpha_k} = 0\\) Hence, we have \\(\\left(P^{-1} \\vec{v_1}, P^{-1} \\vec{v_2}, \\ldots, P^{-1} \\vec{v_k} \\right)\\) is linear independent. Assume there exists vector \\(\\vec{v}\\) is a eigenvector of matrix \\(B\\) that correspond to eigenvalue \\(\\lambda\\) such that \\(\\vec{v} \\notin \\operatorname{Span}({P^{-1} \\vec{v_1}, P^{-1} \\vec{v_2}, \\ldots, P^{-1} \\vec{v_k}})\\) Hence, there exists eigenvector \\(\\vec{u}\\) corresponding to eigenvalue \\(\\lambda\\) of matrix \\(A\\) such that \\(\\vec{v} = P^{-1} \\vec{u}\\). Then we imply that \\(\\vec{u} \\notin \\operatorname{Span}({\\vec{v_1}, \\vec{v_2}, \\ldots, \\vec{v_k} })\\) (CONTRADICTION) Therefore, we can imply that \\(\\left( P^{-1} \\vec{v_1}, P^{-1} \\vec{v_2}, \\ldots, P^{-1} \\vec{v_k} \\right)\\) is the basis of eigenspace corresponding to eigenvalue \\(\\lambda\\) of matrix \\(B\\). Hence, we can conclude that eigenspace corresponding to eigenvalue \\(\\lambda\\) of \\(A\\) and \\(B\\) have same dimension. Therefore, by definition, similar matrices have same geometric multiplicity. (Q.E.D)","headline":"Similar Properties of Similarity Matrix","image":"/assets/img/similarity_matrix/similarity_matrix_resize.png","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/posts/similarity-matrix/"},"url":"http://localhost:4000/posts/similarity-matrix/"}</script>
<!-- End Jekyll SEO tag -->


  <title>Similar Properties of Similarity Matrix | NgocQuan's Research
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">
<link rel="manifest" href="/assets/img/favicons/site.webmanifest">
<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="NgocQuan's Research">
<meta name="application-name" content="NgocQuan's Research">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      <link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin>
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://cdn.jsdelivr.net" >
      <link rel="dns-prefetch" href="https://cdn.jsdelivr.net" >
    

    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap">
  

  <!-- GA -->
  

  <!-- Bootstrap -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css">

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css">

  <link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css">

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.25.0/dist/tocbot.min.css">
  

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css">
  

  
    <!-- Manific Popup -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css">
  

  <!-- JavaScript -->

  
    <!-- Switch the mode between dark and light. -->

<script type="text/javascript">
  class ModeToggle {
    static get MODE_KEY() {
      return 'mode';
    }
    static get MODE_ATTR() {
      return 'data-mode';
    }
    static get DARK_MODE() {
      return 'dark';
    }
    static get LIGHT_MODE() {
      return 'light';
    }
    static get ID() {
      return 'mode-toggle';
    }

    constructor() {
      if (this.hasMode) {
        if (this.isDarkMode) {
          if (!this.isSysDarkPrefer) {
            this.setDark();
          }
        } else {
          if (this.isSysDarkPrefer) {
            this.setLight();
          }
        }
      }

      let self = this;

      /* always follow the system prefers */
      this.sysDarkPrefers.addEventListener('change', () => {
        if (self.hasMode) {
          if (self.isDarkMode) {
            if (!self.isSysDarkPrefer) {
              self.setDark();
            }
          } else {
            if (self.isSysDarkPrefer) {
              self.setLight();
            }
          }

          self.clearMode();
        }

        self.notify();
      });
    } /* constructor() */

    get sysDarkPrefers() {
      return window.matchMedia('(prefers-color-scheme: dark)');
    }

    get isSysDarkPrefer() {
      return this.sysDarkPrefers.matches;
    }

    get isDarkMode() {
      return this.mode === ModeToggle.DARK_MODE;
    }

    get isLightMode() {
      return this.mode === ModeToggle.LIGHT_MODE;
    }

    get hasMode() {
      return this.mode != null;
    }

    get mode() {
      return sessionStorage.getItem(ModeToggle.MODE_KEY);
    }

    /* get the current mode on screen */
    get modeStatus() {
      if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) {
        return ModeToggle.DARK_MODE;
      } else {
        return ModeToggle.LIGHT_MODE;
      }
    }

    setDark() {
      document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
    }

    setLight() {
      document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
    }

    clearMode() {
      document.documentElement.removeAttribute(ModeToggle.MODE_ATTR);
      sessionStorage.removeItem(ModeToggle.MODE_KEY);
    }

    /* Notify another plugins that the theme mode has changed */
    notify() {
      window.postMessage(
        {
          direction: ModeToggle.ID,
          message: this.modeStatus
        },
        '*'
      );
    }

    flipMode() {
      if (this.hasMode) {
        if (this.isSysDarkPrefer) {
          if (this.isLightMode) {
            this.clearMode();
          } else {
            this.setLight();
          }
        } else {
          if (this.isDarkMode) {
            this.clearMode();
          } else {
            this.setDark();
          }
        }
      } else {
        if (this.isSysDarkPrefer) {
          this.setLight();
        } else {
          this.setDark();
        }
      }

      this.notify();
    } /* flipMode() */
  } /* ModeToggle */

  const modeToggle = new ModeToggle();
</script>

  

  <!-- A placeholder to allow defining custom metadata -->

</head>


  <body>
    <!-- The Side Bar -->

<aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end">
  <header class="profile-wrapper">
    <a href="/" id="avatar" class="rounded-circle"><img src="/assets/img/avatar.jpg" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a>

    <h1 class="site-title">
      <a href="/">NgocQuan's Research</a>
    </h1>
    <p class="site-subtitle fst-italic mb-0">A human researching about Machine Learning</p>
  </header>
  <!-- .profile-wrapper -->

  <nav class="flex-column flex-grow-1 w-100 ps-0">
    <ul class="nav">
      <!-- home -->
      <li class="nav-item">
        <a href="/" class="nav-link">
          <i class="fa-fw fas fa-home"></i>
          <span>HOME</span>
        </a>
      </li>
      <!-- the real tabs -->
      
        <li class="nav-item">
          <a href="/categories/" class="nav-link">
            <i class="fa-fw fas fa-stream"></i>
            

            <span>CATEGORIES</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/tags/" class="nav-link">
            <i class="fa-fw fas fa-tags"></i>
            

            <span>TAGS</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/archives/" class="nav-link">
            <i class="fa-fw fas fa-archive"></i>
            

            <span>ARCHIVES</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/about/" class="nav-link">
            <i class="fa-fw fas fa-info-circle"></i>
            

            <span>ABOUT</span>
          </a>
        </li>
        <!-- .nav-item -->
      
    </ul>
  </nav>

  <div class="sidebar-bottom d-flex flex-wrap  align-items-center w-100">
    
      <button type="button" class="mode-toggle btn" aria-label="Switch Mode">
        <i class="fas fa-adjust"></i>
      </button>

      
        <span class="icon-border"></span>
      
    

    
      

      
        <a
          href="https://github.com/ngocquanofficial"
          aria-label="github"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-github"></i>
        </a>
      
    
      

      
        <a
          href=""
          aria-label="linkedin"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-linkedin"></i>
        </a>
      
    
      

      
        <a
          href="javascript:location.href = 'mailto:' + ['ngocquanofficial','gmail.com'].join('@')"
          aria-label="email"
          

          

          

          
        >
          <i class="fas fa-envelope"></i>
        </a>
      
    
      

      
        <a
          href="https://www.facebook.com/ngocquanofficial/"
          aria-label="Facebook"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-facebook-square"></i>
        </a>
      
    
  </div>
  <!-- .sidebar-bottom -->
</aside>
<!-- #sidebar -->


    <div id="main-wrapper" class="d-flex justify-content-center">
      <div class="container d-flex flex-column px-xxl-5">
        <!-- The Top Bar -->

<header id="topbar-wrapper" aria-label="Top Bar">
  <div
    id="topbar"
    class="d-flex align-items-center justify-content-between px-lg-3 h-100"
  >
    <nav id="breadcrumb" aria-label="Breadcrumb">
      

      
        
          
            <span>
              <a href="/">
                Home
              </a>
            </span>

          
        
          
        
          
            
              <span>Similar Properties of Similarity Matrix</span>
            

          
        
      
    </nav>
    <!-- endof #breadcrumb -->

    <button type="button" id="sidebar-trigger" class="btn btn-link">
      <i class="fas fa-bars fa-fw"></i>
    </button>

    <div id="topbar-title">
      Post
    </div>

    <button type="button" id="search-trigger" class="btn btn-link">
      <i class="fas fa-search fa-fw"></i>
    </button>

    <search class="align-items-center ms-3 ms-lg-0">
      <i class="fas fa-search fa-fw"></i>
      <input
        class="form-control"
        id="search-input"
        type="search"
        aria-label="search"
        autocomplete="off"
        placeholder="Search..."
      >
    </search>
    <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button>
  </div>
</header>


        <div class="row flex-grow-1">
          <main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              <!-- Refactor the HTML structure -->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Change the icon of checkbox -->



<!-- Handle images -->




  
  

  
    
      
      
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    
      

      
      
      

      
    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  

  


<!-- Add header for code snippets -->



<!-- Create heading anchors -->





  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  

  
  

  




<!-- return -->




<article class="px-1">
  <header>
    <h1 data-toc-skip>Similar Properties of Similarity Matrix</h1>

    <div class="post-meta text-muted">
      <!-- published date -->
      <span>
        Posted
        <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1675962000"
  data-df="ll"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  Feb 10, 2023
</time>

      </span>

      <!-- lastmod date -->
      

      
        
        
        

        

        <div class="mt-3 mb-3">
          <a href="/assets/img/similarity_matrix/similarity_matrix_resize.png" class="popup img-link preview-img shimmer"><img src="/assets/img/similarity_matrix/similarity_matrix_resize.png"  alt="Preview Image" width="1200" height="630"  loading="lazy"></a></div>
      

      <div class="d-flex justify-content-between">
        <!-- author(s) -->
        <span>
          

          By

          <em>
            
              <a href="https://twitter.com/ngocquanofficial">Pham Ngoc Quan</a>
            
          </em>
        </span>

        <!-- read time -->
        <!-- Calculate the post's reading time, and display the word count in tooltip -->



<!-- words per minute -->










<!-- return element -->
<span
  class="readtime"
  data-bs-toggle="tooltip"
  data-bs-placement="bottom"
  title="2918 words"
>
  <em>16 min</em> read</span>

      </div>
      <!-- .d-flex -->
    </div>
    <!-- .post-meta -->
  </header>

  <div class="content">
    <p>Abstract: Firstly, I will introduce specific definitions relevant to matrix similarity topic. Subsequently, I will explore various common properties of similar matrices, such as trace, rank, determinant, among others. Finally, I will present criteria for determining whether any two given matrices are similar.</p>

<h1 id="definitions">DEFINITIONS</h1>
<h3 id="11-similarity-matrix"><span class="me-2">1.1: SIMILARITY MATRIX</span><a href="#11-similarity-matrix" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>Two square \(n \times n\) matrices A and B are called similar if there exists an invertible \(n*n\) matrix \(P\) such that
<!-- Block math, keep all blank lines --></p>

\[B = P^{-1}AP\]

<!-- Block math, keep all blank lines -->

<h3 id="12-representation-of-a-vector"><span class="me-2">1.2: REPRESENTATION OF A VECTOR</span><a href="#12-representation-of-a-vector" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>\(V\) is a vector space with the basis \(B=\left&lt;\overrightarrow{\beta_1}, \overrightarrow{\beta_2}, \ldots, \overrightarrow{\beta_n}\right&gt; , \vec{v} \in V,  \alpha_1, \ldots, \alpha_n \in \mathbb{R}\) such that \(\vec{v}=\alpha_1 \overrightarrow{\beta_1}+\alpha_2 \overrightarrow{\beta_2}+\ldots+\alpha_n \overrightarrow{\beta_n}.\) Then we have
<!-- Block math, keep all blank lines --></p>

\[{Rep}_B(\vec{v})=\left(\begin{array}{c} \alpha_1 \\ \alpha_2 \\ \vdots \\ \alpha_n \end{array}\right)\]

<!-- Block math, keep all blank lines -->

<h3 id="13-matrix-representation"><span class="me-2">1.3: MATRIX REPRESENTATION</span><a href="#13-matrix-representation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>\(V, W\) are vector spaces with dimension \(n, m\) and basis \(B, D\). \(f \colon V \to W\) is a linear mapping. If we have \(\operatorname{Rep}_D(h(\overrightarrow{\beta_1}))=\left(\begin{array}{c}h_{1,1} \\ h_{2,1} \\ \vdots \\ h_{n, 1}\end{array}\right), \ldots, \operatorname{Rep}_D(h(\overrightarrow{\beta_n}))=\left(\begin{array}{c}h_{1, n} \\ h_{2, n} \\ \vdots \\ h_{n, n}\end{array}\right)\). Then</p>

\[\left(\begin{array}{cccc}h_{1,1} &amp; h_{1,2} &amp; \cdots &amp; h_{1, n} \\ h_{2,1} &amp; h_{2,2} &amp; \cdots &amp; h_{2, n} \\ \vdots &amp; \vdots &amp;  &amp; \vdots \\ h_{n, 1} &amp; h_{n, 2} &amp; \cdots &amp; h_{n, n}\end{array}\right)\]

<p>is called matrix representation of mapping f with respect to the bases \(B, D\), notated by \({Rep}_{B,D}(f)\)</p>

<h3 id="14-determinant-of-matrix"><span class="me-2">1.4: DETERMINANT OF MATRIX</span><a href="#14-determinant-of-matrix" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>\(n \times n\) determinant is a function \(f \colon M_{n \times n} \rightarrow \mathbb{R}\) such that <br />
(1) \(\operatorname{det}\left(\overrightarrow{p_1}, \ldots, k \overrightarrow{p_i}+\overrightarrow{p_j}, \ldots, \overrightarrow{p_n}\right)=\operatorname{det}\left(\overrightarrow{p_1}, \ldots, \overrightarrow{p_j}, \ldots, \overrightarrow{p_n}\right)\), for all \(k \in \mathbb{R}\) <br />
(2) \(\operatorname{det}\left(\overrightarrow{p_1}, \ldots, \overrightarrow{p_j}, \ldots, \overrightarrow{p_i}, \ldots, \overrightarrow{p_n}\right)=-\operatorname{det}\left(\overrightarrow{p_1}, \ldots, \overrightarrow{p_i}, \ldots, \overrightarrow{p_j}, \ldots, \overrightarrow{p_n}\right)\) <br />
(3) \(\operatorname{det}\left(\overrightarrow{p_1}, \ldots, k \overrightarrow{p_i}, \ldots, \overrightarrow{p_n}\right) = k \operatorname{det}\left(\overrightarrow{p_1}, \ldots, \overrightarrow{p_i}, \ldots, \overrightarrow{p_n}\right)\), for all \(k \in \mathbb{R}\) <br />
(4) \(\operatorname{det}(I) = 1\), \(I\) is the identity matrix. <br />
Normally, we use the notation \(|T|\) instead of \(\operatorname{det}(T)\).</p>

<h3 id="15-trace-of-matrix"><span class="me-2">1.5: TRACE OF MATRIX</span><a href="#15-trace-of-matrix" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>The trace of a square matrix \(A\) is the sum of elements on the main diagonal of matrix \(A\), denoted by \(tr(A)\)</p>

<h3 id="16-polynomial-of-matrix"><span class="me-2">1.6: POLYNOMIAL OF MATRIX</span><a href="#16-polynomial-of-matrix" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>\(T\) is a square matrix, \(f(x)\) is a polynomial with degree n 
\(f(x) = c_n x^n + c_{n-1} x^{n-1} + \cdots + c_1 x + c_0\) \((c_i \in \mathbb{R})\). Then we have \(f(T) = c_n T^n + c_{n-1} T^{n-1} + \cdots + c_1 T + c_0 I\) (\(I\) is identity matrix) is a polynomial of matrix \(T\) and also a square matrix of the same size as \(T\).</p>

<h3 id="17-minimal-polynomial"><span class="me-2">1.7: MINIMAL POLYNOMIAL</span><a href="#17-minimal-polynomial" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>For a square matrix \(T\), its minimal polynomial is the monic polynomial \(f\) of the lowest degree such that \(f(T) = 0\).</p>

<h3 id="18-index-of-nipotent"><span class="me-2">1.8: INDEX OF NIPOTENT</span><a href="#18-index-of-nipotent" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>A square matrix $A$ is said to have a nilpotent index $k$ if \(k\) is the smallest natural number(if exists) such that $A^k = 0$.</p>

<h3 id="19-subdiagonal"><span class="me-2">1.9: SUBDIAGONAL</span><a href="#19-subdiagonal" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>The subdiagonal of a square matrix is the line immediately below the main diagonal of the matrix.</p>

<h3 id="110-jordan-block"><span class="me-2">1.10: JORDAN BLOCK</span><a href="#110-jordan-block" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>A Jordan block is a square matrix where the off-diagonal entries are zero, the entries on the main diagonal are all $\lambda$ (where $\lambda \in \mathbb{C}$), and every entry on the subdiagonal is 1.</p>

<h3 id="111-jordan-canonical-form"><span class="me-2">1.11: JORDAN CANONICAL FORM</span><a href="#111-jordan-canonical-form" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>The square matrix $A$ can be represented in the form of a Jordan normal form if 
<!-- Block math, keep all blank lines --></p>

\[A = \left(
\begin{array}{cccc}
J_{\lambda_1} &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; J_{\lambda_2} &amp; \ddots &amp; \vdots \\
\vdots &amp; \ddots &amp; \ddots &amp; 0 \\
0 &amp; \cdots &amp; 0 &amp; J_{\lambda_k} \\
\end{array}
\right)\]

<!-- Block math, keep all blank lines -->
<p>with \(J_{\lambda_i}\) are Jordan blocks.</p>

<h3 id="112-characteristic-polynomial"><span class="me-2">1.12: CHARACTERISTIC POLYNOMIAL</span><a href="#112-characteristic-polynomial" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>A is an \(n \times n\) matrix, the characteristic polynomial of A, denoted by \(p_{A}(x)\) is defined by
<!-- Block math, keep all blank lines --></p>

\[p_{A}(x) = \operatorname{det}(A - xI)\]

<!-- Block math, keep all blank lines -->
<p>where \(I\) denotes the \(n \times n\) identity matrix.</p>

<h3 id="113-geometric-and-algebraic-multiplicity"><span class="me-2">1.13: GEOMETRIC AND ALGEBRAIC MULTIPLICITY</span><a href="#113-geometric-and-algebraic-multiplicity" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>A is a square matrix with characteristic polynomial \(f(x) = (x- \lambda_1)^{m_1} (x - \lambda_2)^{m_2} \ldots (x- \lambda_r)^{m_r}\). Then \(m_i\) is called algebraic multiplicity corresponding to eigenvalue \(\lambda_i\), the dimension of eigenspace corresponding to eigenvalue \(\lambda_i\) is called geometric multiplicity.</p>

<h3 id="114-cofactor-of-matrix"><span class="me-2">1.14: COFACTOR OF MATRIX</span><a href="#114-cofactor-of-matrix" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>Consider the \(n \times n\) square matrix \(T\), we define the \(i,j\) minor \(M_{i,j}\) of matrix \(T\) is the \((n-1) \times (n-1)\) matrix obtained by eliminating the i-th row and j-th column in the matrix \(T\).
Then we have \(i,j\) cofactor \(T_{i,j} = (-1)^{i+j} \operatorname{det}(M_{i,j})\)</p>

<h3 id="115-matrix-adjoint"><span class="me-2">1.15: MATRIX ADJOINT</span><a href="#115-matrix-adjoint" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>Matrix adjoint of square matrix T is denoted by
<!-- Block math, keep all blank lines --></p>

\[\text{adj}(T) = \left(
\begin{array}{cccc}
T_{1,1} &amp; T_{2,1} &amp; \cdots &amp; T_{n,1} \\
T_{1,2} &amp; T_{2,2} &amp; \cdots &amp; T_{n,2} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
T_{1,n} &amp; T_{2,n} &amp; \cdots &amp; T_{n,n}
\end{array}
\right)\]

<!-- Block math, keep all blank lines -->
<p>where \(T_{i,j}\) is the \(i,j\) cofactor of matrix T.</p>
<h3 id="116-equivalence-relation"><span class="me-2">1.16: EQUIVALENCE RELATION</span><a href="#116-equivalence-relation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>A binary relation \(\sim\) on a set \(\mathbb{X}\) is said to be an equivalence relation if and only if it is relexive, symmetric and transitive. That is, for all \(a, b, c \in \mathbb{X}\) :</p>
<ol>
  <li>Reflexive: \(a \sim a\)</li>
  <li>Symmetric: \(a \sim b\) if and only of \(b \sim a\)</li>
  <li>Transitive: If \(a \sim b\) and \(b \sim c\), then \(a \sim c\)</li>
</ol>

<p><br />
<br /></p>

<h1 id="2-properties">2. PROPERTIES</h1>
<h2 id="21-equivalence-relation"><span class="me-2">2.1: EQUIVALENCE RELATION</span><a href="#21-equivalence-relation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p><strong>PROPERTY 2.1:</strong>  Matrix similarity is equivalence relation. <br />
<br />
We aim to prove that the similarity matrix relation satisfies the properties of reflexivity, symmetry, and transitivity.</p>
<ol>
  <li>
    <p>Reflexive:
For any square matrix \(A\), it is obvious that \(I^{-1}AI = A\), where \(I\) is the identity matrix. Therefore, \(A\) is similar to itself.</p>
  </li>
  <li>
    <p>Symmetric: 
Suppose that B is similar to A,which means there exists an invertible matrix \(P\) such that \(B = P^{-1}AP\) \(\implies B P^{-1} = P^{-1}AI = P^{-1}A\)  <br />
\(\iff PBP^{-1} = IA = A\) 
<br />
This implies A is also similar to B, so we have the similarity relation is symmetric.</p>
  </li>
  <li>
    <p>Transitive:
Consider three matrices \(A\), \(B\), and \(C\) such that \(A\) is similar to \(B\) and \(B\) is similar to \(C\). Hence, there exist invertible matrices \(P\), \(Q\) such that \(B = P^{-1}AP\) and \(C = Q^{-1}BQ\). <br />
\(\implies C = Q^{-1}P^{-1}APQ = (PQ)^{-1}APQ\)</p>
  </li>
</ol>

<p>Therefore, \(C\) is similar to \(A\), and matrix similarity relation is transitive.</p>

<h2 id="22-similar-linear-mapping"><span class="me-2">2.2: SIMILAR LINEAR MAPPING</span><a href="#22-similar-linear-mapping" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p><strong>PROPERTY 2.2:</strong> Similarity matrices are matrix representations of the same linear mapping but with respect to different bases. <br />
<br />
<strong>Lemma 1:</strong> Consider three vector spaces \(U,V,W\) and two linear mappings \(f \colon U \to V\) and \(g \colon V \to W\); \(B,D,E\) are the bases of 3 vector spaces, respectively. Then we have 
<!-- Block math, keep all blank lines -->
<br />
$$
\operatorname{Rep}<em>{B,E}(f \circ g) = \operatorname{Rep}</em>{B,D}(f) \cdot \operatorname{Rep}_{D,E}(g)</p>

<p>$$</p>

<!-- Block math, keep all blank lines -->
<p><a href="/assets/img/similarity_matrix/matrix_transform.jpg" class="popup img-link  shimmer"><img src="/assets/img/similarity_matrix/matrix_transform.jpg" alt="Desktop View" loading="lazy"></a>
<em>The notation id represent for identity matrix</em></p>

<p>Consider two similar matrices \(H,G\) such that \(H= \operatorname{Rep}_{B,B}(h)\) and \(G= P^{-1}HP\), where \(B\) is the basis of vector space \(V\) including \(\overrightarrow{\beta_1}, \overrightarrow{\beta_2}, \ldots, \overrightarrow{\beta_n}\). \(h\) is a linear mapping. <br />
Consider the basis \(D\) such that \(\operatorname{Rep}_{D,B}(id) = P\)
From the figure above, we have \(h = id \circ h \circ id\). <br />
 Using <strong>Lemma 1</strong> then we have
<!-- Block math, keep all blank lines --></p>

\[\operatorname{Rep}_{D,D}(h) = \operatorname{Rep}_{D,B}(id) \cdot \operatorname{Rep}_{B,B}(h) \cdot \operatorname{Rep}_{B,D}(id) = P^{-1}HP 
\iff G = \operatorname{Rep}_{D,D}(h) \: \: \: (Q.E.D)\]

<!-- Block math, keep all blank lines -->
<h2 id="23-similar-rank"><span class="me-2">2.3: SIMILAR RANK</span><a href="#23-similar-rank" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p><strong>PROPERTY 2.3:</strong> Similarity matrices have the same rank. <br />
<br />
<strong>Lemma 2:</strong> \(f \colon U \to V\) is a linear mapping with a matrix representation \(A\). Then we have rank \(A\) equals to the dimension of the image space of linear mapping \(f\)</p>

<p>So from <strong>Lemma 2</strong> and <strong>Properties 2.1</strong>, we infer that similar matrices have the same rank.</p>

<h2 id="24-similar-determinant"><span class="me-2">2.4: SIMILAR DETERMINANT</span><a href="#24-similar-determinant" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p><strong>PROPERTY 2.4:</strong> Similarity matrices have the same determinant. <br />
<br />
Consider two similar matrices \(A, B\). Since \(A, B\) are two similar matrices, there exists an invertible matric P such that \(B = PAP^{-1}\)
We have 
<!-- Block math, keep all blank lines --></p>

\[\begin{align*}
|B| &amp;= |PAP^{-1}| \\
&amp;= |P| |A| |P^{-1}| \\
&amp;= |A| |PP^{-1}| \\
&amp;= |A| \: \: \text{Determinant of identity matrix equals 1}
\end{align*}\]

<!-- Block math, keep all blank lines -->
<p>So we have \(|A| = |B|\)   (Q.E.D)</p>

<h2 id="25-similar-trace"><span class="me-2">2.5: SIMILAR TRACE</span><a href="#25-similar-trace" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p><strong>PROPERTY 2.5:</strong> Similarity matrices have the same trace. <br />
<br />
<strong>Lemma 3:</strong> \(M,N\) are two arbitrary square matrices, then we have \(\operatorname{tr}(MN) = \operatorname{tr}(NM)\). \</p>

<p><br /></p>

<p>Since \(A, B\) are two similar matrices, there exists an invertible matric P such that \(B = PAP^{-1}\)<br />
\(\implies \operatorname{tr}(B) = \operatorname{tr}(P^{-1}AP)\) <br />
Using <strong>Lemma 3</strong> we have :
<!-- Block math, keep all blank lines --></p>

\[\begin{align*}
\operatorname{tr}(B) &amp;= \operatorname{tr}(PAP^{-1}) \\
&amp;= \operatorname{tr}(APP^{-1}) \\
&amp;= \operatorname{tr}(AI) \\
&amp;= \operatorname{tr}(A)
\end{align*}\]

<!-- Block math, keep all blank lines -->
<p>So we have \(\operatorname{trace}(A) = \operatorname{trace}(B)\)<br />
<strong>Remark:</strong> If two square matrices of the same order \(A\) and \(B\) satisfy \(\operatorname{tr}(A) = \operatorname{tr}(B)\) and \(|A| = |B|\), we cannot conclude that \(A\) and \(B\) are similar matrices.</p>

<p>Indeed, we will demonstrate a counterexample.
Consider the two matrices 
\(A = \begin{pmatrix}
1 &amp; 0 \\
0 &amp; 1
\end{pmatrix},
B = \begin{pmatrix}
1 &amp; 1 \\
0 &amp; 1
\end{pmatrix}.\)</p>

<p>It is easy to see that 
\(|A| = \left| B \right| = 1, \quad \operatorname{tr}(A) = \operatorname{tr}(B) = 2.\)</p>

<p>However, there was not exist an invertible matrix \(P\) such that \(B = PAP^{-1}\) because if it were so, then \(B = PAP^{-1} = PIP^{-1} = PP^{-1} = I\) which is absurd.</p>

<p>Therefore, we conclude that \(A\) and \(B\) are not similar matrices.</p>

<h2 id="26-similarity-matrix-adjoint"><span class="me-2">2.6: SIMILARITY MATRIX ADJOINT</span><a href="#26-similarity-matrix-adjoint" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p><strong>PROPERTY 2.6:</strong> Matrix adjoint of two similar matrices are also similar.  <br />
<br />
<strong>Lemma 4(Laplace Expansion of Determinants):</strong> For an \(n \times n\) square matrix \(T\) with the \(i,j\) cofactor \(T_{i,j}\), the entry at row \(i\), column \(j\) of matrix \(T\) is denoted by \(t_{i,j}\). Then we have:
<!-- Block math, keep all blank lines --></p>

\[\begin{align*}
|T| &amp;= t_{i,1}T_{i,1} + t_{i,2}T_{i,2} + \ldots + t_{i,n}T_{i,n} \\
&amp;= t_{1,j}T_{1,j} + t_{2,j}T_{2,j} + \ldots + t_{n,j}T_{n,j} \\
\end{align*}\]

<!-- Block math, keep all blank lines -->

<p><strong>Lemma 5:</strong> Consider \(M_n\) is the set contains all square matrices of degree \(n\). Then the function \(f \colon M_n \to M_n\) that satisfy \(f(H) = adj(H), \forall H \in M_n\) is a continuous function. <br />
<br /></p>

<p>Consider the matrix 
\(T = \begin{pmatrix}
t_{1,1} &amp; t_{1,2} &amp; \ldots &amp; t_{1,n} \\
t_{2,1} &amp; t_{2,2} &amp; \ldots &amp; t_{2,n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
t_{n,1} &amp; t_{n,2} &amp; \ldots &amp; t_{n,n}
\end{pmatrix}\) 
with the adjugate 
\(\text{adj}(T) = \begin{pmatrix}
T_{1,1} &amp; T_{2,1} &amp; \ldots &amp; T_{n,1} \\
T_{1,2} &amp; T_{2,2} &amp; \ldots &amp; T_{n,2} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
T_{1,n} &amp; T_{2,n} &amp; \ldots &amp; T_{n,n}
\end{pmatrix}\)</p>

<p>First, we prove that
\(\sum_{j=1}^{n}t_{i,j} T_{k,j} = \sum_{j=1}^{n} t_{j,i}T_{j,k} = 0 \text{ for all } i \neq k\)
<br />
Consider square matrix \(T\), and the matrix \(A\) defined by preserving all values of the matrix \(T\) except for the values in row \(k\); the values in row \(k\) of matrix \(A\) equal to row \(i\) in matrix \(T\).(which means row \(i\) and \(k\) of matrix A have the same values.) Similarly, matrix A have entries \(a_{i,j}\) and \(i,j\) cofactor \(A_{i,j}\). <br />
Since the row \(i\) and the row \(k\) of matrix \(A\) is the same, it implies that \(|A| = 0\). Using <strong>Lemma 4</strong> we have</p>

<!-- Block math, keep all blank lines -->

\[\begin{align*}
\sum_{j=1}^{n}t_{i,j} T_{k,j} &amp;= \sum_{j=1}^{n} a_{k,j}T_{k,j} &amp; \text{(Row i of T and row k of A are the same)}  \\ 
&amp;= \sum_{j=1}^{n} a_{k,j}A_{k,j} &amp; \text{(when eliminating row k, the remaining of A and T are the same)} \\
&amp;= |A|  &amp; \text{Lemma 4} \\ 
&amp;= 0 &amp; \text{(A have 2 rows with the same values.)}
\end{align*}\]

<!-- Block math, keep all blank lines -->
<p>Similarly, we prove that 
\(\sum_{j=1}^{n} t_{j,i}T_{j,k} = 0, \forall i \neq k\)</p>

<p>Using the <strong>Lemma 4</strong> we imply that
<!-- Block math, keep all blank lines --></p>

\[T \operatorname{adj}(T) = \operatorname{adj}(T) \cdot T = |T| \cdot I \: \: \: \: (1)\]

<!-- Block math, keep all blank lines -->
<p><br />
Consider two similar matrices A and B. There exist an invertible matrix \(P\) such that \(B = P^{-1}AP\) <br />
We prove the <strong>Property 2.5</strong> in two cases: \(|A| = 0\) and \(|A| \neq 0\)</p>

<p><strong>Case 1:</strong> \(|A| \neq 0\) <br />
Using <strong>Property 2.4</strong> we imply \(|B| \neq 0\), then \(A\) and \(B\) are invertible matrices. 
Using <strong>(1)</strong> we have 
<!-- math block  --></p>

\[\begin{align*} 
\operatorname{adj}(A) &amp; =\operatorname{det}(A) \cdot A^{-1} \\ 
\operatorname{adj}(B) &amp; =\operatorname{det}(B) \cdot B^{-1} \\ \operatorname{adj}(AB) &amp;= |AB| \cdot (AB)^{-1} \\
&amp;= |A| \cdot |B| \cdot B^{-1} A^{-1} \\
&amp;= |B| \cdot B^{-1} \cdot |A| \cdot A^{-1} \\
&amp;= \operatorname{adj}(B) \operatorname{adj}(A)
\end{align*}\]

<!-- math block  -->

<p>From the above, we imply the following:
<!-- math block --></p>

\[\begin{align*}
\operatorname{adj}(B) &amp;= \operatorname{adj}(P^{-1}AP) \\
&amp;= \operatorname{adj}(P (AP^{-1}))
&amp;= \operatorname{adj}(AP^{-1}) \operatorname{adj}(P) \\
&amp;= \operatorname{adj}(P^{-1}) \operatorname{adj}(A) \operatorname{adj}(P) \\
\end{align*}\]

<!-- Block math, keep all blank lines -->
<p>Otherwise, from <strong>(1)</strong> we have \(T^{-1} \operatorname{adj}(T^{-1}) = |T^{-1}| \cdot I\) with an invertible matrix \(T\) <br />
\(\implies \operatorname{adj}(T^{-1}) = |T^{-1}| \cdot T\) <br />
But we have \(\operatorname{adj}(T) = |T| \cdot T^{-1} \implies \operatorname{adj}(T) \operatorname{adj}(T^{-1}) = I\) <br />
So we imply that \(\operatorname{adj}(B) = \operatorname{adj}(P)^{-1} \operatorname{adj}(A) \operatorname{adj}(P)\), which means \(\operatorname{adj}(A)\) and \(\operatorname{adj}(B)\) are two similar matrices
. <br />
<strong>Case 2:</strong> \(|A| = 0\)
From the <strong>Property 2.3</strong> we imply \(|B| = 0\) <br />
For an arbitrary square matrix \(M\), we have \(M\) is an invertible matrix \(\iff |M| \neq 0\) <br />
Using <strong>Definition 1.12</strong> we imply that matrix \(A + \lambda I\) is invertible for all \(\lambda\) such that \(\lambda\) is not an eigenvalue of matrix \(-A\). <br />
Since matrix \(-A\) has finite eigenvalues; therefore, for all \(\lambda\) small enough we have matrix \(A + \lambda I\) is invertible, then :</p>

<!-- block math -->

\[\operatorname{adj}((A + \lambda I)B) = \operatorname{adj}(B) \operatorname{adj}(A + \lambda I)\]

<!-- block math -->
<p>Using <strong>Lemma 5</strong> we imply that :</p>

<!-- block math -->

\[\begin{aligned}
\operatorname{adj}(A B) &amp; =\operatorname{adj}\left(\lim _{\lambda \rightarrow 0}(A+\lambda I) B\right) \\
&amp; =\lim _{\lambda \rightarrow 0} \operatorname{adj}((A+\lambda I) B) \\
&amp; =\lim _{\lambda \rightarrow 0} \operatorname{adj}(B) \cdot \operatorname{adj}(A+\lambda I) \\
&amp; =\operatorname{adj}(B) \cdot \lim _{\lambda \rightarrow 0} \operatorname{adj}(A+\lambda I) \\
&amp; =\operatorname{adj}(B) . \operatorname{adj}(A)
\end{aligned}\]

<!-- blocj math  -->

<p>Hence, by a similar proof as in <strong>Case 1</strong>, we conclude that A and B are two similar matrices.</p>

<h2 id="27-similar-eigenvalues"><span class="me-2">2.7: SIMILAR EIGENVALUES</span><a href="#27-similar-eigenvalues" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p><strong>PROPERTY 2.7:</strong> Similarity matrices have the same eigenvalues. <br />
<br />
Consider two similar matrices \(A\) and \(B\). Assume that B has an eigenvalue \(\lambda\) corresponding to eigenvector \(\vec{v}\). We have :
<!-- block math --></p>

\[\begin{aligned}
B \vec{v}=\lambda \vec{v} \\ 
\Leftrightarrow &amp; P^{-1} A P \vec{v}=\lambda \vec{v} \\ \Leftrightarrow &amp; A P \vec{v}=P \lambda \vec{v}=\lambda P \vec{v}
\end{aligned}\]

<!-- block math -->
<p>\(\implies\) Every eigenvalue \(\lambda\) of matrix \(B\) is also an eigenvalue of matrix \(A\) 
Using similarly proof, we have every eigenvalue of matrix \(A\) is also an eigenvalue of matrix \(B\); therefore, \(A\) and \(B\) have same eigenvalues. (Q.E.D)</p>

<h2 id="28-similar-characteristic-and-minimal-polynomial"><span class="me-2">2.8: SIMILAR CHARACTERISTIC AND MINIMAL POLYNOMIAL</span><a href="#28-similar-characteristic-and-minimal-polynomial" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p><strong>PROPERTY 2.8:</strong> \(A\) and \(B\) are two similar matrices, then \(A\) and \(B\) have the same characteristic polynomial and minimal polynomial. <br />
<br /></p>

<p>The characteristic polynomial of matrix \(A\) and \(B\) is \(\operatorname{det}(A - xI)\) and \(\operatorname{det}(B - xI)\), respectively, with \(x\) is a variable.
We have:</p>

<!-- math block  -->

\[\begin{aligned} 
\operatorname{det}(B-x I) &amp; =\operatorname{det}\left(P^{-1} A P-x I\right) \\ 
&amp; =\operatorname{det}\left(P^{-1} A P-P^{-1} x I P\right) \\ 
&amp; =\operatorname{det}\left(P^{-1}(A-x I) P\right) \\ 
&amp; =\operatorname{det}\left(P^{-1}\right) \operatorname{det}(A-x I) \operatorname{det} P \\ 
&amp; =\operatorname{det}(A-x I) \operatorname{det} P \cdot \operatorname{det}\left(P^{-1}\right) \\ 
&amp; =\operatorname{det}(A-x I)
\end{aligned}\]

<!-- block math -->

<p>So we conclude that \(A\) and \(B\) have same characteristic polynomial. \</p>

<p>Consider polynomial \(f(x)=x^n+a_{n-1} x^{n-1}+\ldots+a_1 x+a_0\) is a minimal polynomial of matrix \(A\). \(\implies f(A)=0\). Then we have :</p>

<!-- block math -->

\[\begin{aligned}

f\left( B \right) &amp;= f\left(P^{-1} A P\right) \\
&amp;= \left(P^{-1} A P\right)^n+a_{n-1}\left(P^{-1} A P\right)^{n-1}+\ldots+a_n\left(P^{-1} A P\right) +a_0 I \\

\left(P^{-1} A P\right)^n &amp;= P^{-1} A P \cdot P^{-1} A P \ldots P^{-1} A P \\
&amp;=P^{-1} A\left(P P^{-1}\right) \cdot A\left(P P^{-1}\right) \ldots\left(P P^{-1}\right) A P \\
&amp;=P^{-1} A^n P \\

\implies f(B) &amp;= P^{-1} A^n P +a_{n-1} P^{-1} A^{n-1} P +\ldots +a_1 P^{-1} A P+a_0 P^{-1} I P \\
&amp;=P^{-1}\left(A^n+a_{n-1} A^{n-1}+\ldots+a_1 A+a_0 I\right) P \\
&amp;=P^{-1} \cdot f(A) \cdot P \\
&amp;=0
\end{aligned}\]

<!-- block math -->

<p>Consider \(g(x)\) is the minimal polynomial of matrix \(B\), since \(f(B) = 0\) then we imply \(\operatorname{deg}(f) \geq \operatorname{deg}(g)\).
<br />
Using similarly proof, we have \(\operatorname{deg}(g) \geq \operatorname{deg}(f)\), then we imply that \(\operatorname{deg}(f) = \operatorname{deg}(g)\). 
<br />
Assume that \(f(x) \neq g(x)\), then consider \(h(x) = f(x) - g(x)\). We have \(h(x)\) has smaller degree than \(f , g\), and \(h(x)\) is not a zero polynomial. <br />
\(\implies h(B) = f(B) - g(B) = 0\) is a characteristic polynomial of matrix \(B\) with smaller degree than its minimal polynomial \(g(x)\)  (<strong>CONTRADICTION</strong>).
<br />
Therefore, we have \(f(x) = g(x)\), which means two matrices \(A\) and \(B\) have same minimal polynomial.</p>

<h2 id="29-similar-geometric-multiplicity-algebraic-multiplicity"><span class="me-2">2.9: SIMILAR GEOMETRIC MULTIPLICITY, ALGEBRAIC MULTIPLICITY</span><a href="#29-similar-geometric-multiplicity-algebraic-multiplicity" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p><strong>PROPERTY 2.9:</strong> \(A\) and \(B\) are two similar matrices. By <strong>Property 2.7</strong> they also have same eigenvalues. Then for each eigenvalue, matrix \(A\) and \(B\) have same geometric multiplicity and algebraic multiplicity corresponding to this eigenvalue. <br />
<br /></p>

<p>From <strong>Property 2.7</strong> we have \(A\) anh \(B\) have same chracteristic polynomial. By <strong>Definition 1.13</strong> we imply that these two matrices have same algebraic multiplicity for each eigenvalue. 
<br />
Since \(A\) and \(B\) are similar, so there exist invertible matrix \(P\) such that \(B=P^{-1}AP\).
<br />
Consider \(\lambda\) is an eigenvalue of matrix \(A\) and \(\left( \vec{v_1}, \vec{v_2}, \ldots, \vec{v_k} \right)\) is a basis of eigenspace corresponding to eigenvalue \(\lambda\)</p>

<p>Using proof in <strong>Property 2.7</strong> we imply that \(P \vec{v_1}, P \vec{v_2}, \ldots, P \vec{v_k}\) are eigenvectors corresponding to the eigenvalue \(\lambda\) of the matrix \(B\).
<br />
We prove that \(\left( P^{-1} \vec{v_1}, P^{-1} \vec{v_2}, \ldots, P^{-1} \vec{v_k} \right)\) is the basis of eigenspace corresponding to eigenvalue \(\lambda\) of matrix \(B\).
<br />
Assume there exists \(\alpha_1, \alpha_2, \ldots, \alpha_n\) such that</p>

<!-- block math -->

\[\begin{aligned} 
&amp; \alpha_1 P^{-1} \vec{v_1}+\alpha_2 P^{-1} \vec{v_2}+\ldots+\alpha_k P^{-1} \vec{v_k}=\vec{0} \\ \Rightarrow &amp; \alpha_1 \vec{v_1}+\alpha_2 \vec{v_2}+\ldots+\alpha_k \vec{v_k}= P \cdot \vec{0}= \vec{0}
\end{aligned}\]

<!-- block math -->
<p>Since \(\left( \vec{v_1}, \vec{v_2}, \ldots, \vec{v_k} \right)\) is a basis of vector space with dimension \(k\), then we have \(\vec{\alpha_1} = \vec{\alpha_2} = \ldots = \vec{\alpha_k} = 0\)
<br />
Hence, we have \(\left(P^{-1} \vec{v_1}, P^{-1} \vec{v_2}, \ldots, P^{-1} \vec{v_k} \right)\) is linear independent.</p>

<p>Assume there exists vector \(\vec{v}\) is a eigenvector of matrix \(B\) that correspond to eigenvalue \(\lambda\) such that \(\vec{v} \notin \operatorname{Span}({P^{-1} \vec{v_1}, P^{-1} \vec{v_2}, \ldots, P^{-1} \vec{v_k}})\)
<br />
Hence, there exists eigenvector \(\vec{u}\) corresponding to eigenvalue \(\lambda\) of matrix \(A\) such that \(\vec{v} = P^{-1} \vec{u}\). Then we imply that \(\vec{u} \notin \operatorname{Span}({\vec{v_1}, \vec{v_2}, \ldots, \vec{v_k} })\) (<strong>CONTRADICTION</strong>)
<br />
Therefore, we can imply that \(\left( P^{-1} \vec{v_1}, P^{-1} \vec{v_2}, \ldots, P^{-1} \vec{v_k} \right)\) is the basis of eigenspace corresponding to eigenvalue \(\lambda\) of matrix \(B\).
<br />
Hence, we can conclude that eigenspace corresponding to eigenvalue \(\lambda\) of \(A\) and \(B\) have same dimension. Therefore, by definition, similar matrices have same geometric multiplicity.  <strong>(Q.E.D)</strong></p>

<!-- ## 2.10: SIMILAR INDEX OF NIPOTENT
**PROPERTY 2.10:** Similar matrices have same index of nipotent (if exists). \\
<br>
**Lemma 6:** Consider $$A$$ and $$B$$ are two similar matrices, then $$A^k$$ and $$B^k$$ are also similar, for all positive integer $$k$$. \\
<br>  -->


  </div>

  <div class="post-tail-wrapper text-muted">
    <!-- categories -->
    
      <div class="post-meta mb-3">
        <i class="far fa-folder-open fa-fw me-1"></i>
        
          <a href="/categories/mathematics/">Mathematics</a>,
          <a href="/categories/linear-algebra/">Linear Algebra</a>
      </div>
    

    <!-- tags -->
    
      <div class="post-tags">
        <i class="fa fa-tags fa-fw me-1"></i>
        
          <a
            href="/tags/similar-matrices/"
            class="post-tag no-text-decoration"
          >similar matrices</a>
        
          <a
            href="/tags/similarity-matrix/"
            class="post-tag no-text-decoration"
          >similarity matrix</a>
        
          <a
            href="/tags/mathematics/"
            class="post-tag no-text-decoration"
          >mathematics</a>
        
          <a
            href="/tags/algebra/"
            class="post-tag no-text-decoration"
          >algebra</a>
        
      </div>
    

    <div
      class="
        post-tail-bottom
        d-flex justify-content-between align-items-center mt-5 pb-2
      "
    >
      <div class="license-wrapper">
        
          

          This post is licensed under 
        <a href="https://creativecommons.org/licenses/by/4.0/">
          CC BY 4.0
        </a>
         by the author.
        
      </div>

      <!-- Post sharing snippet -->

<div class="share-wrapper d-flex align-items-center">
  <span class="share-label text-muted">Share</span>
  <span class="share-icons">
    
    
    

    

      

      <a href="https://twitter.com/intent/tweet?text=Similar%20Properties%20of%20Similarity%20Matrix%20-%20NgocQuan's%20Research&url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Fsimilarity-matrix%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter">
        <i class="fa-fw fa-brands fa-square-x-twitter"></i>
      </a>
    

      

      <a href="https://www.facebook.com/sharer/sharer.php?title=Similar%20Properties%20of%20Similarity%20Matrix%20-%20NgocQuan's%20Research&u=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Fsimilarity-matrix%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook">
        <i class="fa-fw fab fa-facebook-square"></i>
      </a>
    

      

      <a href="https://t.me/share/url?url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Fsimilarity-matrix%2F&text=Similar%20Properties%20of%20Similarity%20Matrix%20-%20NgocQuan's%20Research" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram">
        <i class="fa-fw fab fa-telegram"></i>
      </a>
    

    <button
      id="copy-link"
      aria-label="Copy link"
      class="btn small"
      data-bs-toggle="tooltip"
      data-bs-placement="top"
      title="Copy link"
      data-title-succeed="Link copied successfully!"
    >
      <i class="fa-fw fas fa-link pe-none fs-6"></i>
    </button>
  </span>
</div>

    </div>
    <!-- .post-tail-bottom -->
  </div>
  <!-- div.post-tail-wrapper -->
</article>


            
          </main>

          <!-- panel -->
          <aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 mb-5 text-muted">
            <div class="access">
              <!-- Get 5 last posted/updated posts -->














  <section id="access-lastmod">
    <h2 class="panel-heading">Recently Updated</h2>
    <ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2">
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/NgocQuan/">Hello World, I'm NgocQuan</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/similarity-matrix/">Similar Properties of Similarity Matrix</a>
        </li>
      
    </ul>
  </section>
  <!-- #access-lastmod -->


              <!-- The trending tags list -->















  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Trending Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/algebra/">algebra</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/mathematics/">mathematics</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/ngocquan/">ngocquan</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/similar-matrices/">similar matrices</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/similarity-matrix/">similarity matrix</a>
      
    </div>
  </section>


            </div>

            
              
              



  <section id="toc-wrapper" class="ps-0 pe-4">
    <h2 class="panel-heading ps-3 pt-2 mb-2">Contents</h2>
    <nav id="toc"></nav>
  </section>


            
          </aside>
        </div>

        <div class="row">
          <!-- tail -->
          <div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              
              <!-- Recommend the other 3 posts according to the tags and categories of the current post. -->

<!-- The total size of related posts -->


<!-- An random integer that bigger than 0 -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy} -->














  

  
    











            
              
              <!-- Navigation buttons at the bottom of the post. -->

<nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation">
  
  

  
    <a
      href="/posts/NgocQuan/"
      class="btn btn-outline-primary"
      aria-label="Older"
    >
      <p>Hello World, I'm NgocQuan</p>
    </a>
  

  
    <div class="btn btn-outline-primary disabled" aria-label="Newer">
      <p>-</p>
    </div>
  
</nav>

            
              
              <!--  The comments switcher -->


            

            <!-- The Footer -->

<footer
  aria-label="Site Info"
  class="
    d-flex flex-column justify-content-center text-muted
    flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3
  "
>
  <p>
    ©
    <time>2024</time>
    <a href="https://twitter.com/ngocquanofficial">Pham Ngoc Quan</a>.
    
      <span
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author."
      >Some rights reserved.</span>
    
  </p>

  <p>Using the <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>.
  </p>
</footer>

          </div>
        </div>

        <!-- The Search results -->

<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
  <div class="col-11 content">
    <div id="search-hints">
      <!-- The trending tags list -->















  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Trending Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/algebra/">algebra</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/mathematics/">mathematics</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/ngocquan/">ngocquan</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/similar-matrices/">similar matrices</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/similarity-matrix/">similarity matrix</a>
      
    </div>
  </section>


    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>

      </div>

      <aside aria-label="Scroll to Top">
        <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow">
          <i class="fas fa-angle-up"></i>
        </button>
      </aside>
    </div>

    <div id="mask"></div>

    
      <aside
  id="notification"
  class="toast"
  role="alert"
  aria-live="assertive"
  aria-atomic="true"
  data-bs-animation="true"
  data-bs-autohide="false"
>
  <div class="toast-header">
    <button
      type="button"
      class="btn-close ms-auto"
      data-bs-dismiss="toast"
      aria-label="Close"
    ></button>
  </div>
  <div class="toast-body text-center pt-0">
    <p class="px-2 mb-3">A new version of content is available.</p>
    <button type="button" class="btn btn-primary" aria-label="Update">
      Update
    </button>
  </div>
</aside>

    

    <!-- JavaScripts -->

    <!-- JS selector for site. -->

<!-- commons -->



<!-- layout specified -->


  

  
    <!-- image lazy-loading & popup & clipboard -->
    
  















  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  



  <script src="https://cdn.jsdelivr.net/combine/npm/jquery@3.7.1/dist/jquery.min.js,npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js,npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/magnific-popup@1.1.0/dist/jquery.magnific-popup.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.10/dayjs.min.js,npm/dayjs@1.11.10/locale/en.min.js,npm/dayjs@1.11.10/plugin/relativeTime.min.js,npm/dayjs@1.11.10/plugin/localizedFormat.min.js,npm/tocbot@4.25.0/dist/tocbot.min.js"></script>






<script defer src="/assets/js/dist/post.min.js"></script>


  <!-- MathJax -->
  <script>
    /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */
    MathJax = {
      tex: {
        /* start/end delimiter pairs for in-line math */
        inlineMath: [
          ['$', '$'],
          ['\\(', '\\)']
        ],
        /* start/end delimiter pairs for display math */
        displayMath: [
          ['$$', '$$'],
          ['\\[', '\\]']
        ]
      }
    };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"></script>





    

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script>
  /* Note: dependent library will be loaded in `js-selector.html` */
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('search-results'),
    json: '/assets/js/data/search.json',
    searchResultTemplate: '  <article class="px-1 px-sm-2 px-lg-4 px-xl-0">    <header>      <h2><a href="{url}">{title}</a></h2>      <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">        {categories}        {tags}      </div>    </header>    <p>{snippet}</p>  </article>',
    noResultsText: '<p class="mt-5"></p>',
    templateMiddleware: function(prop, value, template) {
      if (prop === 'categories') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
        }
      }

      if (prop === 'tags') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
        }
      }
    }
  });
</script>

  </body>
</html>

