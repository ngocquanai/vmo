<!doctype html>














<!-- `site.alt_lang` can specify a language different from the UI -->
<html lang="en" >
  <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta
    name="viewport"
    content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover"
  ><!-- Setup Open Graph image -->

  

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Similar Properties of Similarity Matrix" />
<meta property="og:locale" content="en" />
<meta name="description" content="Abstract: Initially I will introduce specific definitions relevant to matrix similarity topic. Subsequently, it will explore various properties common to similar matrices, such as trace, rank, determinant, among others. Finally, the paper will present criteria for determining whether any two given matrices are similar. DEFINITIONS 1.1: SIMILARITY MATRIX Two square \(n \times n\) matrices A and B are called similar if there exists an invertible \(n*n\) matrix \(P\) such that [B = P^{-1}AP] 1.2: REPRESENTATION OF A VECTOR \(V\) is a vector space with the basis \(B=\left&lt;\overrightarrow{\beta_1}, \overrightarrow{\beta_2}, \ldots, \overrightarrow{\beta_n}\right&gt; , \vec{v} \in V, \alpha_1, \ldots, \alpha_n \in \mathbb{R}\) such that \(\vec{v}=\alpha_1 \overrightarrow{\beta_1}+\alpha_2 \overrightarrow{\beta_2}+\ldots+\alpha_n \overrightarrow{\beta_n}.\) Then we have [{Rep}_B(\vec{v})=\left(\begin{array}{c}\overrightarrow{\beta_1} \ \overrightarrow{\beta_2} \ \vdots \ \overrightarrow{\beta_n}\end{array}\right)] 1.3: MATRIX REPRESENTATION \(V, W\) are vector spaces with dimension \(n, m\) and basis \(B, D\). \(f \colon V \to W\) is a linear mapping. If we have \(\operatorname{Rep}_D(h(\overrightarrow{\beta_1}))=\left(\begin{array}{c}h_{1,1} \\ h_{2,1} \\ \vdots \\ h_{n, 1}\end{array}\right), \ldots, \operatorname{Rep}_D(h(\overrightarrow{\beta_n}))=\left(\begin{array}{c}h_{1, n} \\ h_{2, n} \\ \vdots \\ h_{n, n}\end{array}\right)\). Then [\left(\begin{array}{cccc}h_{1,1} &amp; h_{1,2} &amp; \cdots &amp; h_{1, n} \ h_{2,1} &amp; h_{2,2} &amp; \cdots &amp; h_{2, n} \ \vdots &amp; \vdots &amp; &amp; \vdots \ h_{n, 1} &amp; h_{n, 2} &amp; \cdots &amp; h_{n, n}\end{array}\right)] is called matrix representation of mapping f with respect to the bases \(B, D\), notated by \({Rep}_{B,D}(f)\) 1.4: DETERMINANT OF MATRIX \(n \times n\) determinant is a function \(f \colon M_{n \times n} \rightarrow \mathbb{R}\) such that (1) \(\operatorname{det}\left(\overrightarrow{p_1}, \ldots, k \overrightarrow{p_i}+\overrightarrow{p_j}, \ldots, \overrightarrow{p_n}\right)=\operatorname{det}\left(\overrightarrow{p_1}, \ldots, \overrightarrow{p_j}, \ldots, \overrightarrow{p_n}\right)\), for all \(k \in \mathbb{R}\) (2) \(\operatorname{det}\left(\overrightarrow{p_1}, \ldots, \overrightarrow{p_j}, \ldots, \overrightarrow{p_i}, \ldots, \overrightarrow{p_n}\right)=-\operatorname{det}\left(\overrightarrow{p_1}, \ldots, \overrightarrow{p_i}, \ldots, \overrightarrow{p_j}, \ldots, \overrightarrow{p_n}\right)\) (3) \(\operatorname{det}\left(\overrightarrow{p_1}, \ldots, k \overrightarrow{p_i}, \ldots, \overrightarrow{p_n}\right) = k \operatorname{det}\left(\overrightarrow{p_1}, \ldots, \overrightarrow{p_i}, \ldots, \overrightarrow{p_n}\right)\), for all \(k \in \mathbb{R}\) (4) \(\operatorname{det}(I) = 1\), \(I\) is the identity matrix. Normally, we use the notation \(|T|\) instead of \(\operatorname{det}(T)\). 1.5: TRACE OF MATRIX The trace of a square matrix \(A\) is the sum of elements on the main diagonal of matrix \(A\), denoted by \(tr(A)\) 1.6: POLYNOMIAL OF MATRIX \(T\) is a square matrix, \(f(x)\) is a polynomial with degree n \(f(x) = c_n x^n + c_{n-1} x^{n-1} + \cdots + c_1 x + c_0\) \((c_i \in \mathbb{R})\). Then we have \(f(T) = c_n T^n + c_{n-1} T^{n-1} + \cdots + c_1 T + c_0 I\) (\(I\) is identity matrix) is a polynomial of matrix \(T\) and also a square matrix of the same size as \(T\). 1.7: MINIMAL POLYNOMIAL For a square matrix \(T\), its minimal polynomial is the monic polynomial \(f\) of the lowest degree such that \(f(T) = 0\). 1.8: INDEX OF NIPOTENT A square matrix $A$ is said to have a nilpotent index $k$ if \(k\) is the smallest natural number(if exists) such that $A^k = 0$. 1.9: SUBDIAGONAL The subdiagonal of a square matrix is the line immediately below the main diagonal of the matrix. 1.10: JORDAN BLOCK A Jordan block is a square matrix where the off-diagonal entries are zero, the entries on the main diagonal are all $\lambda$ (where $\lambda \in \mathbb{C}$), and every entry on the subdiagonal is 1. 1.11: JORDAN CANONICAL FORM The square matrix $A$ can be represented in the form of a Jordan normal form if [A = \left( \begin{array}{cccc} J_{\lambda_1} &amp; 0 &amp; \cdots &amp; 0 0 &amp; J_{\lambda_2} &amp; \ddots &amp; \vdots \vdots &amp; \ddots &amp; \ddots &amp; 0 0 &amp; \cdots &amp; 0 &amp; J_{\lambda_k} \end{array} \right)] with \(J_{\lambda_i}\) are Jordan blocks. 1.12: CHARACTERISTIC POLYNOMIAL A is an \(n \times n\) matrix, the characteristic polynomial of A, denoted by \(p_{A}(x)\) is defined by [p_{A}(x) = \operatorname{det}(A - xI)] where \(I\) denotes the \(n \times n\) identity matrix. 1.13: GEOMETRIC AND ALGEBRAIC MULTIPLICITY A is a square matrix with characteristic polynomial \(f(x) = (x- \lambda_1)^{m_1} (x - \lambda_2)^{m_2} \ldots (x- \lambda_r)^{m_r}\). Then \(m_i\) is called algebraic multiplicity corresponding to eigenvalue \(\lambda_i\), the dimension of eigenspace corresponding to eigenvalue \(\lambda_i\) is called geometric multiplicity. 1.14: COFACTOR OF MATRIX Consider the \(n \times n\) square matrix \(T\), we define the \(i,j\) minor \(M_{i,j}\) of matrix \(T\) is the \((n-1) \times (n-1)\) matrix obtained by eliminating the i-th row and j-th column in the matrix \(T\). Then we have \(i,j\) cofactor \(T_{i,j} = (-1)^{i+j} \operatorname{det}(M_{i,j})\) 1.15: MATRIX ADJOINT Matrix adjoint of square matrix T is denoted by [\text{adj}(T) = \left( \begin{array}{cccc} T_{1,1} &amp; T_{2,1} &amp; \cdots &amp; T_{n,1} T_{1,2} &amp; T_{2,2} &amp; \cdots &amp; T_{n,2} \vdots &amp; \vdots &amp; \ddots &amp; \vdots T_{1,n} &amp; T_{2,n} &amp; \cdots &amp; T_{n,n} \end{array} \right)] where \(T_{i,j}\) is the \(i,j\) cofactor of matrix T. 1.16: EQUIVALENCE RELATION A binary relation \(\sim\) on a set \(\mathbb{X}\) is said to be an equivalence relation if and only if it is relexive, symmetric and transitive. That is, for all \(a, b, c \in \mathbb{X}\) : Reflexive: \(a \sim a\) Symmetric: \(a \sim b\) if and only of \(b \sim a\) Transitive: If \(a \sim b\) and \(b \sim c\), then \(a \sim c\) 2. PROPERTIES 2.1: Matrix similarity is equivalence relation We aim to prove that the similarity matrix relation satisfies the properties of reflexivity, symmetry, and transitivity. Reflexive: For any square matrix \(A\), it is obvious that \(I^{-1}AI = A\), where \(I\) is the identity matrix. Therefore, \(A\) is similar to itself. Symmetric: Suppose that B is similar to A,which means there exists an invertible matrix \(P\) such that \(B = P^{-1}AP\) \(\implies B P^{-1} = P^{-1}AI = P^{-1}A\) \(\iff PBP^{-1} = IA = A\) This implies A is also similar to B, so we have the similarity relation is symmetric. Transitive: Consider three matrices \(A\), \(B\), and \(C\) such that \(A\) is similar to \(B\) and \(B\) is similar to \(C\). Hence, there exist invertible matrices \(P\), \(Q\) such that \(B = P^{-1}AP\) and \(C = Q^{-1}BQ\). \(\implies C = Q^{-1}P^{-1}APQ = (PQ)^{-1}APQ\) Therefore, \(C\) is similar to \(A\), and matrix similarity relation is transitive. 2.2: Similarity matrix are matrix representations of the same linear mapping but with respect to different bases. Lemma 1: Consider three vector spaces \(U,V,W\) and two linear mappings \(f \colon U \to V\) and \(g \colon V \to W\); \(B,D,E\) are the bases of 3 vector spaces, respectively. Then we have [\operatorname{Rep}{B,E}(f \circ g) = \operatorname{Rep}{B,D}(f) \cdot \operatorname{Rep}_{D,E}(g)] The notation id represent for identity matrix Consider two similar matrices \(H,G\) such that \(H= \operatorname{Rep}_{B,B}(h)\) and \(G= P^{-1}HP\), where \(B\) is the basis of vector space \(V\) including \(\overrightarrow{\beta_1}, \overrightarrow{\beta_2}, \ldots, \overrightarrow{\beta_n}\). \(h\) is a linear mapping. Consider the basis \(D\) such that \(\operatorname{Rep}_{D,B}(id) = P\) From the figure above, we have \(h = id \circ h \circ id\). Using Lemma 1 then we have [\operatorname{Rep}{D,D}(h) = \operatorname{Rep}{D,B}(id) \cdot \operatorname{Rep}{B,B}(h) \cdot \operatorname{Rep}{B,D}(id) = P^{-1}HP \iff G = \operatorname{Rep}_{D,D}(h) : : : (Q.E.D)] 2.3: Similarity matrices have the same rank. Lemma 2: \(f \colon U \to V\) is a linear mapping with a matrix representation \(A\). Then we have rank \(A\) equals to the dimension of the image space of linear mapping \(f\) So from Lemma 2 and Properties 2.1, we infer that similar matrices have the same rank. 2.4: Similarity matrices have the same determinant. Consider two similar matrices \(A, B\). Since \(A, B\) are two similar matrices, there exists an invertible matric P such that \(B = PAP^{-1}\) We have [\begin{align} |B| &amp;= |PAP^{-1}| &amp;= |P| |A| |P^{-1}| &amp;= |A| |PP^{-1}| &amp;= |A| : : \text{Determinant of identity matrix equals 1} \end{align}] So we have \(|A| = |B|\) (Q.E.D) 2.5: Similarity matrices have the same trace. Lemma 3: \(M,N\) are two arbitrary square matrices, then we have \(\operatorname{tr}(MN) = \operatorname{tr}(NM)\) Since \(A, B\) are two similar matrices, there exists an invertible matric P such that \(B = PAP^{-1}\) \(\implies \operatorname{tr}(B) = \operatorname{tr}(P^{-1}AP)\) Using Lemma 3 we have : [\begin{align} \operatorname{tr}(B) &amp;= \operatorname{tr}(PAP^{-1}) &amp;= \operatorname{tr}(APP^{-1}) &amp;= \operatorname{tr}(AI) &amp;= \operatorname{tr}(A) \end{align}] So we have \(\operatorname{trace}(A) = \operatorname{trace}(B)\) Remark: If two square matrices of the same order \(A\) and \(B\) satisfy \(\operatorname{tr}(A) = \operatorname{tr}(B)\) and \(|A| = |B|\), we cannot conclude that \(A\) and \(B\) are similar matrices. Indeed, we will demonstrate a counterexample. Consider the two matrices \(A = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix}, B = \begin{pmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{pmatrix}.\) It is easy to see that \(|A| = \left| B \right| = 1, \quad \operatorname{tr}(A) = \operatorname{tr}(B) = 2.\) However, there was not exist an invertible matrix \(P\) such that \(B = PAP^{-1}\) because if it were so, then \(B = PAP^{-1} = PIP^{-1} = PP^{-1} = I\) which is absurd. Therefore, we conclude that \(A\) and \(B\) are not similar matrices. 2.6: Matrix adjoint of two similar matrices are also similar. Lemma 4(Laplace Expansion of Determinants): For an \(n \times n\) square matrix \(T\) with the \(i,j\) cofactor \(T_{i,j}\), the entry at row \(i\), column \(j\) of matrix \(T\) is denoted by \(t_{i,j}\). Then we have: [\begin{align} |T| &amp;= t_{i,1}T_{i,1} + t_{i,2}T_{i,2} + \ldots + t_{i,n}T_{i,n} &amp;= t_{1,j}T_{1,j} + t_{2,j}T_{2,j} + \ldots + t_{n,j}T_{n,j} \end{align}] Lemma 5: Consider \(M_n\) is the set contains all square matrix of degree \(n\). Then the function \(f \colon M_n \to M_n\) that satisfy \(f(H) = adj(H), \forall H \in M_n\) is a continuous function. Consider the matrix \(T = \begin{pmatrix} t_{1,1} &amp; t_{1,2} &amp; \ldots &amp; t_{1,n} \\ t_{2,1} &amp; t_{2,2} &amp; \ldots &amp; t_{2,n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ t_{n,1} &amp; t_{n,2} &amp; \ldots &amp; t_{n,n} \end{pmatrix}\) with the adjugate \(\text{adj}(T) = \begin{pmatrix} T_{1,1} &amp; T_{1,2} &amp; \ldots &amp; T_{1,n} \\ T_{2,1} &amp; T_{2,2} &amp; \ldots &amp; T_{2,n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ T_{n,1} &amp; T_{n,2} &amp; \ldots &amp; T_{n,n} \end{pmatrix}\) First, we prove that \(\sum_{j=1}^{n}t_{i,j} T_{k,j} = \sum_{j=1}^{n} t_{j,i}T_{j,k} = 0 \text{ for all } i \neq k\) \(T\) is a square matrix. Consider the matrix \(A\) defined by preserving all values of the matrix \(T\) except for the values in row \(k\); the values in row \(k\) of matrix \(T\) equal to row \(i\) in matrix \(A\). Similarly, matrix A have entries \(a_{i,j}\) and \(i,j\) cofactor \(A_{i,j}\). Since the row \(i\) and the row \(k\) of matrix \(A\) is the same, it implies that \(|A| = 0\). Using Lemma 4 we have [\begin{align} \sum_{j=1}^{n}t_{i,j} T_{k,j} &amp;= \sum_{j=1}^{n} a_{k,j}T_{k,j} &amp; \text{(Row i of T and row k of A are the same)} \ &amp;= \sum_{j=1}^{n} a_{k,j}A_{k,j} &amp; \text{(when eliminating row k, the remaining of A and T are the same)} &amp;= |A| &amp; text{Lemma 4} \ &amp;= 0 &amp; text{(A have 2 rows with the same values.)} \end{align}] Similarly, we prove that \(\sum_{j=1}^{n}t_{j,i} T_{k,j} = \sum_{j=1}^{n} t_{j,i}T_{j,k} = 0 \text{ for all } i \neq k\) Using the Lemma 4 we imply that [T \operatorname{adj}(T) = \operatorname{adj}(T) T = T \cdot I : : : : (1)] Consider two similar matrices A and B. There exist an invertible matrix \(P\) such that \(B = P^{-1}AP\) We prove the Property 2.5 in two cases: \(|A| = 0\) and \(|A| \neq 0\) Case 1: \(|A| \neq 0\) Using Property 2.4 we imply \(|B| \neq 0\), then \(A\) and \(B\) are invertible matrices. Using (1) we have [\operatorname{adj}(A) = A \cdot A^{-1} \ \operatorname{adj}(B) = B \cdot B^{-1}] [\begin{align} \operatorname{adj}(AB) &amp;= |AB| \cdot (AB)^{-1} &amp;= |A| \cdot |B| \cdot B^{-1} A^{-1} &amp;= |B| \cdot B^{-1} \cdot |A| \cdot A^{-1} &amp;= \operatorname{adj}(B) \operatorname{adj}(A) \end{align}] From the above, we imply the following: [\begin{align} \operatorname{adj}(B) &amp;= \operatorname{adj}(PAP^{-1}) &amp;= \operatorname{adj}(P) \operatorname{adj}(AP^{-1}) &amp;= \operatorname{adj}(P) \operatorname{adj}(P^{-1}) \operatorname{adj}(A) \end{align}] Otherwise, from (1) we have \(T^{-1} \operatorname{adj}(T^{-1}) = |T^{-1}| \cdot I\) with an invertible matrix \(T\) \(\implies \operatorname{adj}(T^{-1}) = |T^{-1}| \cdot T\) But we have \(\operatorname{adj}(T) = |T| \cdot T^{-1} \implies \operatorname{adj}(T) \operatorname{adj}(T^{-1}) = 1\) So we imply that \(\operatorname{adj}(B) = \operatorname{adj}(P)^{-1} \operatorname{adj}(A) \operatorname{adj}(P)\), which means \(\operatorname{adj}(A)\) and \(\operatorname{adj}(B)\) are two similar matrices . Case 2: \(|A| = 0\)" />
<meta property="og:description" content="Abstract: Initially I will introduce specific definitions relevant to matrix similarity topic. Subsequently, it will explore various properties common to similar matrices, such as trace, rank, determinant, among others. Finally, the paper will present criteria for determining whether any two given matrices are similar. DEFINITIONS 1.1: SIMILARITY MATRIX Two square \(n \times n\) matrices A and B are called similar if there exists an invertible \(n*n\) matrix \(P\) such that [B = P^{-1}AP] 1.2: REPRESENTATION OF A VECTOR \(V\) is a vector space with the basis \(B=\left&lt;\overrightarrow{\beta_1}, \overrightarrow{\beta_2}, \ldots, \overrightarrow{\beta_n}\right&gt; , \vec{v} \in V, \alpha_1, \ldots, \alpha_n \in \mathbb{R}\) such that \(\vec{v}=\alpha_1 \overrightarrow{\beta_1}+\alpha_2 \overrightarrow{\beta_2}+\ldots+\alpha_n \overrightarrow{\beta_n}.\) Then we have [{Rep}_B(\vec{v})=\left(\begin{array}{c}\overrightarrow{\beta_1} \ \overrightarrow{\beta_2} \ \vdots \ \overrightarrow{\beta_n}\end{array}\right)] 1.3: MATRIX REPRESENTATION \(V, W\) are vector spaces with dimension \(n, m\) and basis \(B, D\). \(f \colon V \to W\) is a linear mapping. If we have \(\operatorname{Rep}_D(h(\overrightarrow{\beta_1}))=\left(\begin{array}{c}h_{1,1} \\ h_{2,1} \\ \vdots \\ h_{n, 1}\end{array}\right), \ldots, \operatorname{Rep}_D(h(\overrightarrow{\beta_n}))=\left(\begin{array}{c}h_{1, n} \\ h_{2, n} \\ \vdots \\ h_{n, n}\end{array}\right)\). Then [\left(\begin{array}{cccc}h_{1,1} &amp; h_{1,2} &amp; \cdots &amp; h_{1, n} \ h_{2,1} &amp; h_{2,2} &amp; \cdots &amp; h_{2, n} \ \vdots &amp; \vdots &amp; &amp; \vdots \ h_{n, 1} &amp; h_{n, 2} &amp; \cdots &amp; h_{n, n}\end{array}\right)] is called matrix representation of mapping f with respect to the bases \(B, D\), notated by \({Rep}_{B,D}(f)\) 1.4: DETERMINANT OF MATRIX \(n \times n\) determinant is a function \(f \colon M_{n \times n} \rightarrow \mathbb{R}\) such that (1) \(\operatorname{det}\left(\overrightarrow{p_1}, \ldots, k \overrightarrow{p_i}+\overrightarrow{p_j}, \ldots, \overrightarrow{p_n}\right)=\operatorname{det}\left(\overrightarrow{p_1}, \ldots, \overrightarrow{p_j}, \ldots, \overrightarrow{p_n}\right)\), for all \(k \in \mathbb{R}\) (2) \(\operatorname{det}\left(\overrightarrow{p_1}, \ldots, \overrightarrow{p_j}, \ldots, \overrightarrow{p_i}, \ldots, \overrightarrow{p_n}\right)=-\operatorname{det}\left(\overrightarrow{p_1}, \ldots, \overrightarrow{p_i}, \ldots, \overrightarrow{p_j}, \ldots, \overrightarrow{p_n}\right)\) (3) \(\operatorname{det}\left(\overrightarrow{p_1}, \ldots, k \overrightarrow{p_i}, \ldots, \overrightarrow{p_n}\right) = k \operatorname{det}\left(\overrightarrow{p_1}, \ldots, \overrightarrow{p_i}, \ldots, \overrightarrow{p_n}\right)\), for all \(k \in \mathbb{R}\) (4) \(\operatorname{det}(I) = 1\), \(I\) is the identity matrix. Normally, we use the notation \(|T|\) instead of \(\operatorname{det}(T)\). 1.5: TRACE OF MATRIX The trace of a square matrix \(A\) is the sum of elements on the main diagonal of matrix \(A\), denoted by \(tr(A)\) 1.6: POLYNOMIAL OF MATRIX \(T\) is a square matrix, \(f(x)\) is a polynomial with degree n \(f(x) = c_n x^n + c_{n-1} x^{n-1} + \cdots + c_1 x + c_0\) \((c_i \in \mathbb{R})\). Then we have \(f(T) = c_n T^n + c_{n-1} T^{n-1} + \cdots + c_1 T + c_0 I\) (\(I\) is identity matrix) is a polynomial of matrix \(T\) and also a square matrix of the same size as \(T\). 1.7: MINIMAL POLYNOMIAL For a square matrix \(T\), its minimal polynomial is the monic polynomial \(f\) of the lowest degree such that \(f(T) = 0\). 1.8: INDEX OF NIPOTENT A square matrix $A$ is said to have a nilpotent index $k$ if \(k\) is the smallest natural number(if exists) such that $A^k = 0$. 1.9: SUBDIAGONAL The subdiagonal of a square matrix is the line immediately below the main diagonal of the matrix. 1.10: JORDAN BLOCK A Jordan block is a square matrix where the off-diagonal entries are zero, the entries on the main diagonal are all $\lambda$ (where $\lambda \in \mathbb{C}$), and every entry on the subdiagonal is 1. 1.11: JORDAN CANONICAL FORM The square matrix $A$ can be represented in the form of a Jordan normal form if [A = \left( \begin{array}{cccc} J_{\lambda_1} &amp; 0 &amp; \cdots &amp; 0 0 &amp; J_{\lambda_2} &amp; \ddots &amp; \vdots \vdots &amp; \ddots &amp; \ddots &amp; 0 0 &amp; \cdots &amp; 0 &amp; J_{\lambda_k} \end{array} \right)] with \(J_{\lambda_i}\) are Jordan blocks. 1.12: CHARACTERISTIC POLYNOMIAL A is an \(n \times n\) matrix, the characteristic polynomial of A, denoted by \(p_{A}(x)\) is defined by [p_{A}(x) = \operatorname{det}(A - xI)] where \(I\) denotes the \(n \times n\) identity matrix. 1.13: GEOMETRIC AND ALGEBRAIC MULTIPLICITY A is a square matrix with characteristic polynomial \(f(x) = (x- \lambda_1)^{m_1} (x - \lambda_2)^{m_2} \ldots (x- \lambda_r)^{m_r}\). Then \(m_i\) is called algebraic multiplicity corresponding to eigenvalue \(\lambda_i\), the dimension of eigenspace corresponding to eigenvalue \(\lambda_i\) is called geometric multiplicity. 1.14: COFACTOR OF MATRIX Consider the \(n \times n\) square matrix \(T\), we define the \(i,j\) minor \(M_{i,j}\) of matrix \(T\) is the \((n-1) \times (n-1)\) matrix obtained by eliminating the i-th row and j-th column in the matrix \(T\). Then we have \(i,j\) cofactor \(T_{i,j} = (-1)^{i+j} \operatorname{det}(M_{i,j})\) 1.15: MATRIX ADJOINT Matrix adjoint of square matrix T is denoted by [\text{adj}(T) = \left( \begin{array}{cccc} T_{1,1} &amp; T_{2,1} &amp; \cdots &amp; T_{n,1} T_{1,2} &amp; T_{2,2} &amp; \cdots &amp; T_{n,2} \vdots &amp; \vdots &amp; \ddots &amp; \vdots T_{1,n} &amp; T_{2,n} &amp; \cdots &amp; T_{n,n} \end{array} \right)] where \(T_{i,j}\) is the \(i,j\) cofactor of matrix T. 1.16: EQUIVALENCE RELATION A binary relation \(\sim\) on a set \(\mathbb{X}\) is said to be an equivalence relation if and only if it is relexive, symmetric and transitive. That is, for all \(a, b, c \in \mathbb{X}\) : Reflexive: \(a \sim a\) Symmetric: \(a \sim b\) if and only of \(b \sim a\) Transitive: If \(a \sim b\) and \(b \sim c\), then \(a \sim c\) 2. PROPERTIES 2.1: Matrix similarity is equivalence relation We aim to prove that the similarity matrix relation satisfies the properties of reflexivity, symmetry, and transitivity. Reflexive: For any square matrix \(A\), it is obvious that \(I^{-1}AI = A\), where \(I\) is the identity matrix. Therefore, \(A\) is similar to itself. Symmetric: Suppose that B is similar to A,which means there exists an invertible matrix \(P\) such that \(B = P^{-1}AP\) \(\implies B P^{-1} = P^{-1}AI = P^{-1}A\) \(\iff PBP^{-1} = IA = A\) This implies A is also similar to B, so we have the similarity relation is symmetric. Transitive: Consider three matrices \(A\), \(B\), and \(C\) such that \(A\) is similar to \(B\) and \(B\) is similar to \(C\). Hence, there exist invertible matrices \(P\), \(Q\) such that \(B = P^{-1}AP\) and \(C = Q^{-1}BQ\). \(\implies C = Q^{-1}P^{-1}APQ = (PQ)^{-1}APQ\) Therefore, \(C\) is similar to \(A\), and matrix similarity relation is transitive. 2.2: Similarity matrix are matrix representations of the same linear mapping but with respect to different bases. Lemma 1: Consider three vector spaces \(U,V,W\) and two linear mappings \(f \colon U \to V\) and \(g \colon V \to W\); \(B,D,E\) are the bases of 3 vector spaces, respectively. Then we have [\operatorname{Rep}{B,E}(f \circ g) = \operatorname{Rep}{B,D}(f) \cdot \operatorname{Rep}_{D,E}(g)] The notation id represent for identity matrix Consider two similar matrices \(H,G\) such that \(H= \operatorname{Rep}_{B,B}(h)\) and \(G= P^{-1}HP\), where \(B\) is the basis of vector space \(V\) including \(\overrightarrow{\beta_1}, \overrightarrow{\beta_2}, \ldots, \overrightarrow{\beta_n}\). \(h\) is a linear mapping. Consider the basis \(D\) such that \(\operatorname{Rep}_{D,B}(id) = P\) From the figure above, we have \(h = id \circ h \circ id\). Using Lemma 1 then we have [\operatorname{Rep}{D,D}(h) = \operatorname{Rep}{D,B}(id) \cdot \operatorname{Rep}{B,B}(h) \cdot \operatorname{Rep}{B,D}(id) = P^{-1}HP \iff G = \operatorname{Rep}_{D,D}(h) : : : (Q.E.D)] 2.3: Similarity matrices have the same rank. Lemma 2: \(f \colon U \to V\) is a linear mapping with a matrix representation \(A\). Then we have rank \(A\) equals to the dimension of the image space of linear mapping \(f\) So from Lemma 2 and Properties 2.1, we infer that similar matrices have the same rank. 2.4: Similarity matrices have the same determinant. Consider two similar matrices \(A, B\). Since \(A, B\) are two similar matrices, there exists an invertible matric P such that \(B = PAP^{-1}\) We have [\begin{align} |B| &amp;= |PAP^{-1}| &amp;= |P| |A| |P^{-1}| &amp;= |A| |PP^{-1}| &amp;= |A| : : \text{Determinant of identity matrix equals 1} \end{align}] So we have \(|A| = |B|\) (Q.E.D) 2.5: Similarity matrices have the same trace. Lemma 3: \(M,N\) are two arbitrary square matrices, then we have \(\operatorname{tr}(MN) = \operatorname{tr}(NM)\) Since \(A, B\) are two similar matrices, there exists an invertible matric P such that \(B = PAP^{-1}\) \(\implies \operatorname{tr}(B) = \operatorname{tr}(P^{-1}AP)\) Using Lemma 3 we have : [\begin{align} \operatorname{tr}(B) &amp;= \operatorname{tr}(PAP^{-1}) &amp;= \operatorname{tr}(APP^{-1}) &amp;= \operatorname{tr}(AI) &amp;= \operatorname{tr}(A) \end{align}] So we have \(\operatorname{trace}(A) = \operatorname{trace}(B)\) Remark: If two square matrices of the same order \(A\) and \(B\) satisfy \(\operatorname{tr}(A) = \operatorname{tr}(B)\) and \(|A| = |B|\), we cannot conclude that \(A\) and \(B\) are similar matrices. Indeed, we will demonstrate a counterexample. Consider the two matrices \(A = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix}, B = \begin{pmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{pmatrix}.\) It is easy to see that \(|A| = \left| B \right| = 1, \quad \operatorname{tr}(A) = \operatorname{tr}(B) = 2.\) However, there was not exist an invertible matrix \(P\) such that \(B = PAP^{-1}\) because if it were so, then \(B = PAP^{-1} = PIP^{-1} = PP^{-1} = I\) which is absurd. Therefore, we conclude that \(A\) and \(B\) are not similar matrices. 2.6: Matrix adjoint of two similar matrices are also similar. Lemma 4(Laplace Expansion of Determinants): For an \(n \times n\) square matrix \(T\) with the \(i,j\) cofactor \(T_{i,j}\), the entry at row \(i\), column \(j\) of matrix \(T\) is denoted by \(t_{i,j}\). Then we have: [\begin{align} |T| &amp;= t_{i,1}T_{i,1} + t_{i,2}T_{i,2} + \ldots + t_{i,n}T_{i,n} &amp;= t_{1,j}T_{1,j} + t_{2,j}T_{2,j} + \ldots + t_{n,j}T_{n,j} \end{align}] Lemma 5: Consider \(M_n\) is the set contains all square matrix of degree \(n\). Then the function \(f \colon M_n \to M_n\) that satisfy \(f(H) = adj(H), \forall H \in M_n\) is a continuous function. Consider the matrix \(T = \begin{pmatrix} t_{1,1} &amp; t_{1,2} &amp; \ldots &amp; t_{1,n} \\ t_{2,1} &amp; t_{2,2} &amp; \ldots &amp; t_{2,n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ t_{n,1} &amp; t_{n,2} &amp; \ldots &amp; t_{n,n} \end{pmatrix}\) with the adjugate \(\text{adj}(T) = \begin{pmatrix} T_{1,1} &amp; T_{1,2} &amp; \ldots &amp; T_{1,n} \\ T_{2,1} &amp; T_{2,2} &amp; \ldots &amp; T_{2,n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ T_{n,1} &amp; T_{n,2} &amp; \ldots &amp; T_{n,n} \end{pmatrix}\) First, we prove that \(\sum_{j=1}^{n}t_{i,j} T_{k,j} = \sum_{j=1}^{n} t_{j,i}T_{j,k} = 0 \text{ for all } i \neq k\) \(T\) is a square matrix. Consider the matrix \(A\) defined by preserving all values of the matrix \(T\) except for the values in row \(k\); the values in row \(k\) of matrix \(T\) equal to row \(i\) in matrix \(A\). Similarly, matrix A have entries \(a_{i,j}\) and \(i,j\) cofactor \(A_{i,j}\). Since the row \(i\) and the row \(k\) of matrix \(A\) is the same, it implies that \(|A| = 0\). Using Lemma 4 we have [\begin{align} \sum_{j=1}^{n}t_{i,j} T_{k,j} &amp;= \sum_{j=1}^{n} a_{k,j}T_{k,j} &amp; \text{(Row i of T and row k of A are the same)} \ &amp;= \sum_{j=1}^{n} a_{k,j}A_{k,j} &amp; \text{(when eliminating row k, the remaining of A and T are the same)} &amp;= |A| &amp; text{Lemma 4} \ &amp;= 0 &amp; text{(A have 2 rows with the same values.)} \end{align}] Similarly, we prove that \(\sum_{j=1}^{n}t_{j,i} T_{k,j} = \sum_{j=1}^{n} t_{j,i}T_{j,k} = 0 \text{ for all } i \neq k\) Using the Lemma 4 we imply that [T \operatorname{adj}(T) = \operatorname{adj}(T) T = T \cdot I : : : : (1)] Consider two similar matrices A and B. There exist an invertible matrix \(P\) such that \(B = P^{-1}AP\) We prove the Property 2.5 in two cases: \(|A| = 0\) and \(|A| \neq 0\) Case 1: \(|A| \neq 0\) Using Property 2.4 we imply \(|B| \neq 0\), then \(A\) and \(B\) are invertible matrices. Using (1) we have [\operatorname{adj}(A) = A \cdot A^{-1} \ \operatorname{adj}(B) = B \cdot B^{-1}] [\begin{align} \operatorname{adj}(AB) &amp;= |AB| \cdot (AB)^{-1} &amp;= |A| \cdot |B| \cdot B^{-1} A^{-1} &amp;= |B| \cdot B^{-1} \cdot |A| \cdot A^{-1} &amp;= \operatorname{adj}(B) \operatorname{adj}(A) \end{align}] From the above, we imply the following: [\begin{align} \operatorname{adj}(B) &amp;= \operatorname{adj}(PAP^{-1}) &amp;= \operatorname{adj}(P) \operatorname{adj}(AP^{-1}) &amp;= \operatorname{adj}(P) \operatorname{adj}(P^{-1}) \operatorname{adj}(A) \end{align}] Otherwise, from (1) we have \(T^{-1} \operatorname{adj}(T^{-1}) = |T^{-1}| \cdot I\) with an invertible matrix \(T\) \(\implies \operatorname{adj}(T^{-1}) = |T^{-1}| \cdot T\) But we have \(\operatorname{adj}(T) = |T| \cdot T^{-1} \implies \operatorname{adj}(T) \operatorname{adj}(T^{-1}) = 1\) So we imply that \(\operatorname{adj}(B) = \operatorname{adj}(P)^{-1} \operatorname{adj}(A) \operatorname{adj}(P)\), which means \(\operatorname{adj}(A)\) and \(\operatorname{adj}(B)\) are two similar matrices . Case 2: \(|A| = 0\)" />
<link rel="canonical" href="http://localhost:4000/posts/similarity-matrix/" />
<meta property="og:url" content="http://localhost:4000/posts/similarity-matrix/" />
<meta property="og:site_name" content="NgocQuan’s Research" />
<meta property="og:image" content="/assets/img/similarity_matrix/similarity_matrix.jpeg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-08-20T20:00:00+07:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="/assets/img/similarity_matrix/similarity_matrix.jpeg" />
<meta property="twitter:title" content="Similar Properties of Similarity Matrix" />
<meta name="twitter:site" content="@twitter_username" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-08-20T20:00:00+07:00","datePublished":"2023-08-20T20:00:00+07:00","description":"Abstract: Initially I will introduce specific definitions relevant to matrix similarity topic. Subsequently, it will explore various properties common to similar matrices, such as trace, rank, determinant, among others. Finally, the paper will present criteria for determining whether any two given matrices are similar. DEFINITIONS 1.1: SIMILARITY MATRIX Two square \\(n \\times n\\) matrices A and B are called similar if there exists an invertible \\(n*n\\) matrix \\(P\\) such that [B = P^{-1}AP] 1.2: REPRESENTATION OF A VECTOR \\(V\\) is a vector space with the basis \\(B=\\left&lt;\\overrightarrow{\\beta_1}, \\overrightarrow{\\beta_2}, \\ldots, \\overrightarrow{\\beta_n}\\right&gt; , \\vec{v} \\in V, \\alpha_1, \\ldots, \\alpha_n \\in \\mathbb{R}\\) such that \\(\\vec{v}=\\alpha_1 \\overrightarrow{\\beta_1}+\\alpha_2 \\overrightarrow{\\beta_2}+\\ldots+\\alpha_n \\overrightarrow{\\beta_n}.\\) Then we have [{Rep}_B(\\vec{v})=\\left(\\begin{array}{c}\\overrightarrow{\\beta_1} \\ \\overrightarrow{\\beta_2} \\ \\vdots \\ \\overrightarrow{\\beta_n}\\end{array}\\right)] 1.3: MATRIX REPRESENTATION \\(V, W\\) are vector spaces with dimension \\(n, m\\) and basis \\(B, D\\). \\(f \\colon V \\to W\\) is a linear mapping. If we have \\(\\operatorname{Rep}_D(h(\\overrightarrow{\\beta_1}))=\\left(\\begin{array}{c}h_{1,1} \\\\ h_{2,1} \\\\ \\vdots \\\\ h_{n, 1}\\end{array}\\right), \\ldots, \\operatorname{Rep}_D(h(\\overrightarrow{\\beta_n}))=\\left(\\begin{array}{c}h_{1, n} \\\\ h_{2, n} \\\\ \\vdots \\\\ h_{n, n}\\end{array}\\right)\\). Then [\\left(\\begin{array}{cccc}h_{1,1} &amp; h_{1,2} &amp; \\cdots &amp; h_{1, n} \\ h_{2,1} &amp; h_{2,2} &amp; \\cdots &amp; h_{2, n} \\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\ h_{n, 1} &amp; h_{n, 2} &amp; \\cdots &amp; h_{n, n}\\end{array}\\right)] is called matrix representation of mapping f with respect to the bases \\(B, D\\), notated by \\({Rep}_{B,D}(f)\\) 1.4: DETERMINANT OF MATRIX \\(n \\times n\\) determinant is a function \\(f \\colon M_{n \\times n} \\rightarrow \\mathbb{R}\\) such that (1) \\(\\operatorname{det}\\left(\\overrightarrow{p_1}, \\ldots, k \\overrightarrow{p_i}+\\overrightarrow{p_j}, \\ldots, \\overrightarrow{p_n}\\right)=\\operatorname{det}\\left(\\overrightarrow{p_1}, \\ldots, \\overrightarrow{p_j}, \\ldots, \\overrightarrow{p_n}\\right)\\), for all \\(k \\in \\mathbb{R}\\) (2) \\(\\operatorname{det}\\left(\\overrightarrow{p_1}, \\ldots, \\overrightarrow{p_j}, \\ldots, \\overrightarrow{p_i}, \\ldots, \\overrightarrow{p_n}\\right)=-\\operatorname{det}\\left(\\overrightarrow{p_1}, \\ldots, \\overrightarrow{p_i}, \\ldots, \\overrightarrow{p_j}, \\ldots, \\overrightarrow{p_n}\\right)\\) (3) \\(\\operatorname{det}\\left(\\overrightarrow{p_1}, \\ldots, k \\overrightarrow{p_i}, \\ldots, \\overrightarrow{p_n}\\right) = k \\operatorname{det}\\left(\\overrightarrow{p_1}, \\ldots, \\overrightarrow{p_i}, \\ldots, \\overrightarrow{p_n}\\right)\\), for all \\(k \\in \\mathbb{R}\\) (4) \\(\\operatorname{det}(I) = 1\\), \\(I\\) is the identity matrix. Normally, we use the notation \\(|T|\\) instead of \\(\\operatorname{det}(T)\\). 1.5: TRACE OF MATRIX The trace of a square matrix \\(A\\) is the sum of elements on the main diagonal of matrix \\(A\\), denoted by \\(tr(A)\\) 1.6: POLYNOMIAL OF MATRIX \\(T\\) is a square matrix, \\(f(x)\\) is a polynomial with degree n \\(f(x) = c_n x^n + c_{n-1} x^{n-1} + \\cdots + c_1 x + c_0\\) \\((c_i \\in \\mathbb{R})\\). Then we have \\(f(T) = c_n T^n + c_{n-1} T^{n-1} + \\cdots + c_1 T + c_0 I\\) (\\(I\\) is identity matrix) is a polynomial of matrix \\(T\\) and also a square matrix of the same size as \\(T\\). 1.7: MINIMAL POLYNOMIAL For a square matrix \\(T\\), its minimal polynomial is the monic polynomial \\(f\\) of the lowest degree such that \\(f(T) = 0\\). 1.8: INDEX OF NIPOTENT A square matrix $A$ is said to have a nilpotent index $k$ if \\(k\\) is the smallest natural number(if exists) such that $A^k = 0$. 1.9: SUBDIAGONAL The subdiagonal of a square matrix is the line immediately below the main diagonal of the matrix. 1.10: JORDAN BLOCK A Jordan block is a square matrix where the off-diagonal entries are zero, the entries on the main diagonal are all $\\lambda$ (where $\\lambda \\in \\mathbb{C}$), and every entry on the subdiagonal is 1. 1.11: JORDAN CANONICAL FORM The square matrix $A$ can be represented in the form of a Jordan normal form if [A = \\left( \\begin{array}{cccc} J_{\\lambda_1} &amp; 0 &amp; \\cdots &amp; 0 0 &amp; J_{\\lambda_2} &amp; \\ddots &amp; \\vdots \\vdots &amp; \\ddots &amp; \\ddots &amp; 0 0 &amp; \\cdots &amp; 0 &amp; J_{\\lambda_k} \\end{array} \\right)] with \\(J_{\\lambda_i}\\) are Jordan blocks. 1.12: CHARACTERISTIC POLYNOMIAL A is an \\(n \\times n\\) matrix, the characteristic polynomial of A, denoted by \\(p_{A}(x)\\) is defined by [p_{A}(x) = \\operatorname{det}(A - xI)] where \\(I\\) denotes the \\(n \\times n\\) identity matrix. 1.13: GEOMETRIC AND ALGEBRAIC MULTIPLICITY A is a square matrix with characteristic polynomial \\(f(x) = (x- \\lambda_1)^{m_1} (x - \\lambda_2)^{m_2} \\ldots (x- \\lambda_r)^{m_r}\\). Then \\(m_i\\) is called algebraic multiplicity corresponding to eigenvalue \\(\\lambda_i\\), the dimension of eigenspace corresponding to eigenvalue \\(\\lambda_i\\) is called geometric multiplicity. 1.14: COFACTOR OF MATRIX Consider the \\(n \\times n\\) square matrix \\(T\\), we define the \\(i,j\\) minor \\(M_{i,j}\\) of matrix \\(T\\) is the \\((n-1) \\times (n-1)\\) matrix obtained by eliminating the i-th row and j-th column in the matrix \\(T\\). Then we have \\(i,j\\) cofactor \\(T_{i,j} = (-1)^{i+j} \\operatorname{det}(M_{i,j})\\) 1.15: MATRIX ADJOINT Matrix adjoint of square matrix T is denoted by [\\text{adj}(T) = \\left( \\begin{array}{cccc} T_{1,1} &amp; T_{2,1} &amp; \\cdots &amp; T_{n,1} T_{1,2} &amp; T_{2,2} &amp; \\cdots &amp; T_{n,2} \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots T_{1,n} &amp; T_{2,n} &amp; \\cdots &amp; T_{n,n} \\end{array} \\right)] where \\(T_{i,j}\\) is the \\(i,j\\) cofactor of matrix T. 1.16: EQUIVALENCE RELATION A binary relation \\(\\sim\\) on a set \\(\\mathbb{X}\\) is said to be an equivalence relation if and only if it is relexive, symmetric and transitive. That is, for all \\(a, b, c \\in \\mathbb{X}\\) : Reflexive: \\(a \\sim a\\) Symmetric: \\(a \\sim b\\) if and only of \\(b \\sim a\\) Transitive: If \\(a \\sim b\\) and \\(b \\sim c\\), then \\(a \\sim c\\) 2. PROPERTIES 2.1: Matrix similarity is equivalence relation We aim to prove that the similarity matrix relation satisfies the properties of reflexivity, symmetry, and transitivity. Reflexive: For any square matrix \\(A\\), it is obvious that \\(I^{-1}AI = A\\), where \\(I\\) is the identity matrix. Therefore, \\(A\\) is similar to itself. Symmetric: Suppose that B is similar to A,which means there exists an invertible matrix \\(P\\) such that \\(B = P^{-1}AP\\) \\(\\implies B P^{-1} = P^{-1}AI = P^{-1}A\\) \\(\\iff PBP^{-1} = IA = A\\) This implies A is also similar to B, so we have the similarity relation is symmetric. Transitive: Consider three matrices \\(A\\), \\(B\\), and \\(C\\) such that \\(A\\) is similar to \\(B\\) and \\(B\\) is similar to \\(C\\). Hence, there exist invertible matrices \\(P\\), \\(Q\\) such that \\(B = P^{-1}AP\\) and \\(C = Q^{-1}BQ\\). \\(\\implies C = Q^{-1}P^{-1}APQ = (PQ)^{-1}APQ\\) Therefore, \\(C\\) is similar to \\(A\\), and matrix similarity relation is transitive. 2.2: Similarity matrix are matrix representations of the same linear mapping but with respect to different bases. Lemma 1: Consider three vector spaces \\(U,V,W\\) and two linear mappings \\(f \\colon U \\to V\\) and \\(g \\colon V \\to W\\); \\(B,D,E\\) are the bases of 3 vector spaces, respectively. Then we have [\\operatorname{Rep}{B,E}(f \\circ g) = \\operatorname{Rep}{B,D}(f) \\cdot \\operatorname{Rep}_{D,E}(g)] The notation id represent for identity matrix Consider two similar matrices \\(H,G\\) such that \\(H= \\operatorname{Rep}_{B,B}(h)\\) and \\(G= P^{-1}HP\\), where \\(B\\) is the basis of vector space \\(V\\) including \\(\\overrightarrow{\\beta_1}, \\overrightarrow{\\beta_2}, \\ldots, \\overrightarrow{\\beta_n}\\). \\(h\\) is a linear mapping. Consider the basis \\(D\\) such that \\(\\operatorname{Rep}_{D,B}(id) = P\\) From the figure above, we have \\(h = id \\circ h \\circ id\\). Using Lemma 1 then we have [\\operatorname{Rep}{D,D}(h) = \\operatorname{Rep}{D,B}(id) \\cdot \\operatorname{Rep}{B,B}(h) \\cdot \\operatorname{Rep}{B,D}(id) = P^{-1}HP \\iff G = \\operatorname{Rep}_{D,D}(h) : : : (Q.E.D)] 2.3: Similarity matrices have the same rank. Lemma 2: \\(f \\colon U \\to V\\) is a linear mapping with a matrix representation \\(A\\). Then we have rank \\(A\\) equals to the dimension of the image space of linear mapping \\(f\\) So from Lemma 2 and Properties 2.1, we infer that similar matrices have the same rank. 2.4: Similarity matrices have the same determinant. Consider two similar matrices \\(A, B\\). Since \\(A, B\\) are two similar matrices, there exists an invertible matric P such that \\(B = PAP^{-1}\\) We have [\\begin{align} |B| &amp;= |PAP^{-1}| &amp;= |P| |A| |P^{-1}| &amp;= |A| |PP^{-1}| &amp;= |A| : : \\text{Determinant of identity matrix equals 1} \\end{align}] So we have \\(|A| = |B|\\) (Q.E.D) 2.5: Similarity matrices have the same trace. Lemma 3: \\(M,N\\) are two arbitrary square matrices, then we have \\(\\operatorname{tr}(MN) = \\operatorname{tr}(NM)\\) Since \\(A, B\\) are two similar matrices, there exists an invertible matric P such that \\(B = PAP^{-1}\\) \\(\\implies \\operatorname{tr}(B) = \\operatorname{tr}(P^{-1}AP)\\) Using Lemma 3 we have : [\\begin{align} \\operatorname{tr}(B) &amp;= \\operatorname{tr}(PAP^{-1}) &amp;= \\operatorname{tr}(APP^{-1}) &amp;= \\operatorname{tr}(AI) &amp;= \\operatorname{tr}(A) \\end{align}] So we have \\(\\operatorname{trace}(A) = \\operatorname{trace}(B)\\) Remark: If two square matrices of the same order \\(A\\) and \\(B\\) satisfy \\(\\operatorname{tr}(A) = \\operatorname{tr}(B)\\) and \\(|A| = |B|\\), we cannot conclude that \\(A\\) and \\(B\\) are similar matrices. Indeed, we will demonstrate a counterexample. Consider the two matrices \\(A = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix}, B = \\begin{pmatrix} 1 &amp; 1 \\\\ 0 &amp; 1 \\end{pmatrix}.\\) It is easy to see that \\(|A| = \\left| B \\right| = 1, \\quad \\operatorname{tr}(A) = \\operatorname{tr}(B) = 2.\\) However, there was not exist an invertible matrix \\(P\\) such that \\(B = PAP^{-1}\\) because if it were so, then \\(B = PAP^{-1} = PIP^{-1} = PP^{-1} = I\\) which is absurd. Therefore, we conclude that \\(A\\) and \\(B\\) are not similar matrices. 2.6: Matrix adjoint of two similar matrices are also similar. Lemma 4(Laplace Expansion of Determinants): For an \\(n \\times n\\) square matrix \\(T\\) with the \\(i,j\\) cofactor \\(T_{i,j}\\), the entry at row \\(i\\), column \\(j\\) of matrix \\(T\\) is denoted by \\(t_{i,j}\\). Then we have: [\\begin{align} |T| &amp;= t_{i,1}T_{i,1} + t_{i,2}T_{i,2} + \\ldots + t_{i,n}T_{i,n} &amp;= t_{1,j}T_{1,j} + t_{2,j}T_{2,j} + \\ldots + t_{n,j}T_{n,j} \\end{align}] Lemma 5: Consider \\(M_n\\) is the set contains all square matrix of degree \\(n\\). Then the function \\(f \\colon M_n \\to M_n\\) that satisfy \\(f(H) = adj(H), \\forall H \\in M_n\\) is a continuous function. Consider the matrix \\(T = \\begin{pmatrix} t_{1,1} &amp; t_{1,2} &amp; \\ldots &amp; t_{1,n} \\\\ t_{2,1} &amp; t_{2,2} &amp; \\ldots &amp; t_{2,n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ t_{n,1} &amp; t_{n,2} &amp; \\ldots &amp; t_{n,n} \\end{pmatrix}\\) with the adjugate \\(\\text{adj}(T) = \\begin{pmatrix} T_{1,1} &amp; T_{1,2} &amp; \\ldots &amp; T_{1,n} \\\\ T_{2,1} &amp; T_{2,2} &amp; \\ldots &amp; T_{2,n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ T_{n,1} &amp; T_{n,2} &amp; \\ldots &amp; T_{n,n} \\end{pmatrix}\\) First, we prove that \\(\\sum_{j=1}^{n}t_{i,j} T_{k,j} = \\sum_{j=1}^{n} t_{j,i}T_{j,k} = 0 \\text{ for all } i \\neq k\\) \\(T\\) is a square matrix. Consider the matrix \\(A\\) defined by preserving all values of the matrix \\(T\\) except for the values in row \\(k\\); the values in row \\(k\\) of matrix \\(T\\) equal to row \\(i\\) in matrix \\(A\\). Similarly, matrix A have entries \\(a_{i,j}\\) and \\(i,j\\) cofactor \\(A_{i,j}\\). Since the row \\(i\\) and the row \\(k\\) of matrix \\(A\\) is the same, it implies that \\(|A| = 0\\). Using Lemma 4 we have [\\begin{align} \\sum_{j=1}^{n}t_{i,j} T_{k,j} &amp;= \\sum_{j=1}^{n} a_{k,j}T_{k,j} &amp; \\text{(Row i of T and row k of A are the same)} \\ &amp;= \\sum_{j=1}^{n} a_{k,j}A_{k,j} &amp; \\text{(when eliminating row k, the remaining of A and T are the same)} &amp;= |A| &amp; text{Lemma 4} \\ &amp;= 0 &amp; text{(A have 2 rows with the same values.)} \\end{align}] Similarly, we prove that \\(\\sum_{j=1}^{n}t_{j,i} T_{k,j} = \\sum_{j=1}^{n} t_{j,i}T_{j,k} = 0 \\text{ for all } i \\neq k\\) Using the Lemma 4 we imply that [T \\operatorname{adj}(T) = \\operatorname{adj}(T) T = T \\cdot I : : : : (1)] Consider two similar matrices A and B. There exist an invertible matrix \\(P\\) such that \\(B = P^{-1}AP\\) We prove the Property 2.5 in two cases: \\(|A| = 0\\) and \\(|A| \\neq 0\\) Case 1: \\(|A| \\neq 0\\) Using Property 2.4 we imply \\(|B| \\neq 0\\), then \\(A\\) and \\(B\\) are invertible matrices. Using (1) we have [\\operatorname{adj}(A) = A \\cdot A^{-1} \\ \\operatorname{adj}(B) = B \\cdot B^{-1}] [\\begin{align} \\operatorname{adj}(AB) &amp;= |AB| \\cdot (AB)^{-1} &amp;= |A| \\cdot |B| \\cdot B^{-1} A^{-1} &amp;= |B| \\cdot B^{-1} \\cdot |A| \\cdot A^{-1} &amp;= \\operatorname{adj}(B) \\operatorname{adj}(A) \\end{align}] From the above, we imply the following: [\\begin{align} \\operatorname{adj}(B) &amp;= \\operatorname{adj}(PAP^{-1}) &amp;= \\operatorname{adj}(P) \\operatorname{adj}(AP^{-1}) &amp;= \\operatorname{adj}(P) \\operatorname{adj}(P^{-1}) \\operatorname{adj}(A) \\end{align}] Otherwise, from (1) we have \\(T^{-1} \\operatorname{adj}(T^{-1}) = |T^{-1}| \\cdot I\\) with an invertible matrix \\(T\\) \\(\\implies \\operatorname{adj}(T^{-1}) = |T^{-1}| \\cdot T\\) But we have \\(\\operatorname{adj}(T) = |T| \\cdot T^{-1} \\implies \\operatorname{adj}(T) \\operatorname{adj}(T^{-1}) = 1\\) So we imply that \\(\\operatorname{adj}(B) = \\operatorname{adj}(P)^{-1} \\operatorname{adj}(A) \\operatorname{adj}(P)\\), which means \\(\\operatorname{adj}(A)\\) and \\(\\operatorname{adj}(B)\\) are two similar matrices . Case 2: \\(|A| = 0\\)","headline":"Similar Properties of Similarity Matrix","image":"/assets/img/similarity_matrix/similarity_matrix.jpeg","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/posts/similarity-matrix/"},"url":"http://localhost:4000/posts/similarity-matrix/"}</script>
<!-- End Jekyll SEO tag -->


  <title>Similar Properties of Similarity Matrix | NgocQuan's Research
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">
<link rel="manifest" href="/assets/img/favicons/site.webmanifest">
<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="NgocQuan's Research">
<meta name="application-name" content="NgocQuan's Research">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      <link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin>
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://cdn.jsdelivr.net" >
      <link rel="dns-prefetch" href="https://cdn.jsdelivr.net" >
    

    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap">
  

  <!-- GA -->
  

  <!-- Bootstrap -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css">

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css">

  <link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css">

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.25.0/dist/tocbot.min.css">
  

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css">
  

  
    <!-- Manific Popup -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css">
  

  <!-- JavaScript -->

  
    <!-- Switch the mode between dark and light. -->

<script type="text/javascript">
  class ModeToggle {
    static get MODE_KEY() {
      return 'mode';
    }
    static get MODE_ATTR() {
      return 'data-mode';
    }
    static get DARK_MODE() {
      return 'dark';
    }
    static get LIGHT_MODE() {
      return 'light';
    }
    static get ID() {
      return 'mode-toggle';
    }

    constructor() {
      if (this.hasMode) {
        if (this.isDarkMode) {
          if (!this.isSysDarkPrefer) {
            this.setDark();
          }
        } else {
          if (this.isSysDarkPrefer) {
            this.setLight();
          }
        }
      }

      let self = this;

      /* always follow the system prefers */
      this.sysDarkPrefers.addEventListener('change', () => {
        if (self.hasMode) {
          if (self.isDarkMode) {
            if (!self.isSysDarkPrefer) {
              self.setDark();
            }
          } else {
            if (self.isSysDarkPrefer) {
              self.setLight();
            }
          }

          self.clearMode();
        }

        self.notify();
      });
    } /* constructor() */

    get sysDarkPrefers() {
      return window.matchMedia('(prefers-color-scheme: dark)');
    }

    get isSysDarkPrefer() {
      return this.sysDarkPrefers.matches;
    }

    get isDarkMode() {
      return this.mode === ModeToggle.DARK_MODE;
    }

    get isLightMode() {
      return this.mode === ModeToggle.LIGHT_MODE;
    }

    get hasMode() {
      return this.mode != null;
    }

    get mode() {
      return sessionStorage.getItem(ModeToggle.MODE_KEY);
    }

    /* get the current mode on screen */
    get modeStatus() {
      if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) {
        return ModeToggle.DARK_MODE;
      } else {
        return ModeToggle.LIGHT_MODE;
      }
    }

    setDark() {
      document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
    }

    setLight() {
      document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
    }

    clearMode() {
      document.documentElement.removeAttribute(ModeToggle.MODE_ATTR);
      sessionStorage.removeItem(ModeToggle.MODE_KEY);
    }

    /* Notify another plugins that the theme mode has changed */
    notify() {
      window.postMessage(
        {
          direction: ModeToggle.ID,
          message: this.modeStatus
        },
        '*'
      );
    }

    flipMode() {
      if (this.hasMode) {
        if (this.isSysDarkPrefer) {
          if (this.isLightMode) {
            this.clearMode();
          } else {
            this.setLight();
          }
        } else {
          if (this.isDarkMode) {
            this.clearMode();
          } else {
            this.setDark();
          }
        }
      } else {
        if (this.isSysDarkPrefer) {
          this.setLight();
        } else {
          this.setDark();
        }
      }

      this.notify();
    } /* flipMode() */
  } /* ModeToggle */

  const modeToggle = new ModeToggle();
</script>

  

  <!-- A placeholder to allow defining custom metadata -->

</head>


  <body>
    <!-- The Side Bar -->

<aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end">
  <header class="profile-wrapper">
    <a href="/" id="avatar" class="rounded-circle"><img src="/assets/img/avatar2.jpg" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a>

    <h1 class="site-title">
      <a href="/">NgocQuan's Research</a>
    </h1>
    <p class="site-subtitle fst-italic mb-0">A human researching about Machine Learning</p>
  </header>
  <!-- .profile-wrapper -->

  <nav class="flex-column flex-grow-1 w-100 ps-0">
    <ul class="nav">
      <!-- home -->
      <li class="nav-item">
        <a href="/" class="nav-link">
          <i class="fa-fw fas fa-home"></i>
          <span>HOME</span>
        </a>
      </li>
      <!-- the real tabs -->
      
        <li class="nav-item">
          <a href="/categories/" class="nav-link">
            <i class="fa-fw fas fa-stream"></i>
            

            <span>CATEGORIES</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/tags/" class="nav-link">
            <i class="fa-fw fas fa-tags"></i>
            

            <span>TAGS</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/archives/" class="nav-link">
            <i class="fa-fw fas fa-archive"></i>
            

            <span>ARCHIVES</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/about/" class="nav-link">
            <i class="fa-fw fas fa-info-circle"></i>
            

            <span>ABOUT</span>
          </a>
        </li>
        <!-- .nav-item -->
      
    </ul>
  </nav>

  <div class="sidebar-bottom d-flex flex-wrap  align-items-center w-100">
    
      <button type="button" class="mode-toggle btn" aria-label="Switch Mode">
        <i class="fas fa-adjust"></i>
      </button>

      
        <span class="icon-border"></span>
      
    

    
      

      
        <a
          href="https://github.com/ngocquanofficial"
          aria-label="github"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-github"></i>
        </a>
      
    
      

      
        <a
          href=""
          aria-label="linkedin"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-linkedin"></i>
        </a>
      
    
      

      
        <a
          href="javascript:location.href = 'mailto:' + ['ngocquanofficial','gmail.com'].join('@')"
          aria-label="email"
          

          

          

          
        >
          <i class="fas fa-envelope"></i>
        </a>
      
    
      

      
        <a
          href="https://www.facebook.com/ngocquanofficial/"
          aria-label="Facebook"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-facebook-square"></i>
        </a>
      
    
  </div>
  <!-- .sidebar-bottom -->
</aside>
<!-- #sidebar -->


    <div id="main-wrapper" class="d-flex justify-content-center">
      <div class="container d-flex flex-column px-xxl-5">
        <!-- The Top Bar -->

<header id="topbar-wrapper" aria-label="Top Bar">
  <div
    id="topbar"
    class="d-flex align-items-center justify-content-between px-lg-3 h-100"
  >
    <nav id="breadcrumb" aria-label="Breadcrumb">
      

      
        
          
            <span>
              <a href="/">
                Home
              </a>
            </span>

          
        
          
        
          
            
              <span>Similar Properties of Similarity Matrix</span>
            

          
        
      
    </nav>
    <!-- endof #breadcrumb -->

    <button type="button" id="sidebar-trigger" class="btn btn-link">
      <i class="fas fa-bars fa-fw"></i>
    </button>

    <div id="topbar-title">
      Post
    </div>

    <button type="button" id="search-trigger" class="btn btn-link">
      <i class="fas fa-search fa-fw"></i>
    </button>

    <search class="align-items-center ms-3 ms-lg-0">
      <i class="fas fa-search fa-fw"></i>
      <input
        class="form-control"
        id="search-input"
        type="search"
        aria-label="search"
        autocomplete="off"
        placeholder="Search..."
      >
    </search>
    <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button>
  </div>
</header>


        <div class="row flex-grow-1">
          <main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              <!-- Refactor the HTML structure -->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Change the icon of checkbox -->



<!-- Handle images -->




  
  

  
    
      
      
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    
      

      
      
      

      
    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  

  


<!-- Add header for code snippets -->



<!-- Create heading anchors -->





  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  

  
  

  




<!-- return -->




<article class="px-1">
  <header>
    <h1 data-toc-skip>Similar Properties of Similarity Matrix</h1>

    <div class="post-meta text-muted">
      <!-- published date -->
      <span>
        Posted
        <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1692536400"
  data-df="ll"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  Aug 20, 2023
</time>

      </span>

      <!-- lastmod date -->
      

      
        
        
        

        

        <div class="mt-3 mb-3">
          <a href="/assets/img/similarity_matrix/similarity_matrix.jpeg" class="popup img-link preview-img shimmer"><img src="/assets/img/similarity_matrix/similarity_matrix.jpeg"  alt="Preview Image" width="1200" height="630"  loading="lazy"></a></div>
      

      <div class="d-flex justify-content-between">
        <!-- author(s) -->
        <span>
          

          By

          <em>
            
              <a href="https://twitter.com/ngocquanofficial">Pham Ngoc Quan</a>
            
          </em>
        </span>

        <!-- read time -->
        <!-- Calculate the post's reading time, and display the word count in tooltip -->



<!-- words per minute -->










<!-- return element -->
<span
  class="readtime"
  data-bs-toggle="tooltip"
  data-bs-placement="bottom"
  title="1977 words"
>
  <em>10 min</em> read</span>

      </div>
      <!-- .d-flex -->
    </div>
    <!-- .post-meta -->
  </header>

  <div class="content">
    <p>Abstract: Initially I will introduce specific definitions relevant to matrix similarity topic. Subsequently, it will explore various properties common to similar matrices, such as trace, rank, determinant, among others. Finally, the paper will present criteria for determining whether any two given matrices are similar.</p>

<h1 id="definitions">DEFINITIONS</h1>
<h3 id="11-similarity-matrix"><span class="me-2">1.1: SIMILARITY MATRIX</span><a href="#11-similarity-matrix" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>Two square \(n \times n\) matrices A and B are called similar if there exists an invertible \(n*n\) matrix \(P\) such that
<!-- Block math, keep all blank lines --></p>

\[B = P^{-1}AP\]

<!-- Block math, keep all blank lines -->

<h3 id="12-representation-of-a-vector"><span class="me-2">1.2: REPRESENTATION OF A VECTOR</span><a href="#12-representation-of-a-vector" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>\(V\) is a vector space with the basis \(B=\left&lt;\overrightarrow{\beta_1}, \overrightarrow{\beta_2}, \ldots, \overrightarrow{\beta_n}\right&gt; , \vec{v} \in V,  \alpha_1, \ldots, \alpha_n \in \mathbb{R}\) such that \(\vec{v}=\alpha_1 \overrightarrow{\beta_1}+\alpha_2 \overrightarrow{\beta_2}+\ldots+\alpha_n \overrightarrow{\beta_n}.\) Then we have
<!-- Block math, keep all blank lines --></p>

\[{Rep}_B(\vec{v})=\left(\begin{array}{c}\overrightarrow{\beta_1} \\ \overrightarrow{\beta_2} \\ \vdots \\ \overrightarrow{\beta_n}\end{array}\right)\]

<!-- Block math, keep all blank lines -->

<h3 id="13-matrix-representation"><span class="me-2">1.3: MATRIX REPRESENTATION</span><a href="#13-matrix-representation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>\(V, W\) are vector spaces with dimension \(n, m\) and basis \(B, D\). \(f \colon V \to W\) is a linear mapping. If we have \(\operatorname{Rep}_D(h(\overrightarrow{\beta_1}))=\left(\begin{array}{c}h_{1,1} \\ h_{2,1} \\ \vdots \\ h_{n, 1}\end{array}\right), \ldots, \operatorname{Rep}_D(h(\overrightarrow{\beta_n}))=\left(\begin{array}{c}h_{1, n} \\ h_{2, n} \\ \vdots \\ h_{n, n}\end{array}\right)\). Then</p>

\[\left(\begin{array}{cccc}h_{1,1} &amp; h_{1,2} &amp; \cdots &amp; h_{1, n} \\ h_{2,1} &amp; h_{2,2} &amp; \cdots &amp; h_{2, n} \\ \vdots &amp; \vdots &amp;  &amp; \vdots \\ h_{n, 1} &amp; h_{n, 2} &amp; \cdots &amp; h_{n, n}\end{array}\right)\]

<p>is called matrix representation of mapping f with respect to the bases \(B, D\), notated by \({Rep}_{B,D}(f)\)</p>

<h3 id="14-determinant-of-matrix"><span class="me-2">1.4: DETERMINANT OF MATRIX</span><a href="#14-determinant-of-matrix" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>\(n \times n\) determinant is a function \(f \colon M_{n \times n} \rightarrow \mathbb{R}\) such that <br />
(1) \(\operatorname{det}\left(\overrightarrow{p_1}, \ldots, k \overrightarrow{p_i}+\overrightarrow{p_j}, \ldots, \overrightarrow{p_n}\right)=\operatorname{det}\left(\overrightarrow{p_1}, \ldots, \overrightarrow{p_j}, \ldots, \overrightarrow{p_n}\right)\), for all \(k \in \mathbb{R}\) <br />
(2) \(\operatorname{det}\left(\overrightarrow{p_1}, \ldots, \overrightarrow{p_j}, \ldots, \overrightarrow{p_i}, \ldots, \overrightarrow{p_n}\right)=-\operatorname{det}\left(\overrightarrow{p_1}, \ldots, \overrightarrow{p_i}, \ldots, \overrightarrow{p_j}, \ldots, \overrightarrow{p_n}\right)\) <br />
(3) \(\operatorname{det}\left(\overrightarrow{p_1}, \ldots, k \overrightarrow{p_i}, \ldots, \overrightarrow{p_n}\right) = k \operatorname{det}\left(\overrightarrow{p_1}, \ldots, \overrightarrow{p_i}, \ldots, \overrightarrow{p_n}\right)\), for all \(k \in \mathbb{R}\) <br />
(4) \(\operatorname{det}(I) = 1\), \(I\) is the identity matrix. <br />
Normally, we use the notation \(|T|\) instead of \(\operatorname{det}(T)\).</p>

<h3 id="15-trace-of-matrix"><span class="me-2">1.5: TRACE OF MATRIX</span><a href="#15-trace-of-matrix" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>The trace of a square matrix \(A\) is the sum of elements on the main diagonal of matrix \(A\), denoted by \(tr(A)\)</p>

<h3 id="16-polynomial-of-matrix"><span class="me-2">1.6: POLYNOMIAL OF MATRIX</span><a href="#16-polynomial-of-matrix" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>\(T\) is a square matrix, \(f(x)\) is a polynomial with degree n 
\(f(x) = c_n x^n + c_{n-1} x^{n-1} + \cdots + c_1 x + c_0\) \((c_i \in \mathbb{R})\). Then we have \(f(T) = c_n T^n + c_{n-1} T^{n-1} + \cdots + c_1 T + c_0 I\) (\(I\) is identity matrix) is a polynomial of matrix \(T\) and also a square matrix of the same size as \(T\).</p>

<h3 id="17-minimal-polynomial"><span class="me-2">1.7: MINIMAL POLYNOMIAL</span><a href="#17-minimal-polynomial" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>For a square matrix \(T\), its minimal polynomial is the monic polynomial \(f\) of the lowest degree such that \(f(T) = 0\).</p>

<h3 id="18-index-of-nipotent"><span class="me-2">1.8: INDEX OF NIPOTENT</span><a href="#18-index-of-nipotent" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>A square matrix $A$ is said to have a nilpotent index $k$ if \(k\) is the smallest natural number(if exists) such that $A^k = 0$.</p>

<h3 id="19-subdiagonal"><span class="me-2">1.9: SUBDIAGONAL</span><a href="#19-subdiagonal" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>The subdiagonal of a square matrix is the line immediately below the main diagonal of the matrix.</p>

<h3 id="110-jordan-block"><span class="me-2">1.10: JORDAN BLOCK</span><a href="#110-jordan-block" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>A Jordan block is a square matrix where the off-diagonal entries are zero, the entries on the main diagonal are all $\lambda$ (where $\lambda \in \mathbb{C}$), and every entry on the subdiagonal is 1.</p>

<h3 id="111-jordan-canonical-form"><span class="me-2">1.11: JORDAN CANONICAL FORM</span><a href="#111-jordan-canonical-form" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>The square matrix $A$ can be represented in the form of a Jordan normal form if 
<!-- Block math, keep all blank lines --></p>

\[A = \left(
\begin{array}{cccc}
J_{\lambda_1} &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; J_{\lambda_2} &amp; \ddots &amp; \vdots \\
\vdots &amp; \ddots &amp; \ddots &amp; 0 \\
0 &amp; \cdots &amp; 0 &amp; J_{\lambda_k} \\
\end{array}
\right)\]

<!-- Block math, keep all blank lines -->
<p>with \(J_{\lambda_i}\) are Jordan blocks.</p>

<h3 id="112-characteristic-polynomial"><span class="me-2">1.12: CHARACTERISTIC POLYNOMIAL</span><a href="#112-characteristic-polynomial" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>A is an \(n \times n\) matrix, the characteristic polynomial of A, denoted by \(p_{A}(x)\) is defined by
<!-- Block math, keep all blank lines --></p>

\[p_{A}(x) = \operatorname{det}(A - xI)\]

<!-- Block math, keep all blank lines -->
<p>where \(I\) denotes the \(n \times n\) identity matrix.</p>

<h3 id="113-geometric-and-algebraic-multiplicity"><span class="me-2">1.13: GEOMETRIC AND ALGEBRAIC MULTIPLICITY</span><a href="#113-geometric-and-algebraic-multiplicity" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>A is a square matrix with characteristic polynomial \(f(x) = (x- \lambda_1)^{m_1} (x - \lambda_2)^{m_2} \ldots (x- \lambda_r)^{m_r}\). Then \(m_i\) is called algebraic multiplicity corresponding to eigenvalue \(\lambda_i\), the dimension of eigenspace corresponding to eigenvalue \(\lambda_i\) is called geometric multiplicity.</p>

<h3 id="114-cofactor-of-matrix"><span class="me-2">1.14: COFACTOR OF MATRIX</span><a href="#114-cofactor-of-matrix" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>Consider the \(n \times n\) square matrix \(T\), we define the \(i,j\) minor \(M_{i,j}\) of matrix \(T\) is the \((n-1) \times (n-1)\) matrix obtained by eliminating the i-th row and j-th column in the matrix \(T\).
Then we have \(i,j\) cofactor \(T_{i,j} = (-1)^{i+j} \operatorname{det}(M_{i,j})\)</p>

<h3 id="115-matrix-adjoint"><span class="me-2">1.15: MATRIX ADJOINT</span><a href="#115-matrix-adjoint" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>Matrix adjoint of square matrix T is denoted by
<!-- Block math, keep all blank lines --></p>

\[\text{adj}(T) = \left(
\begin{array}{cccc}
T_{1,1} &amp; T_{2,1} &amp; \cdots &amp; T_{n,1} \\
T_{1,2} &amp; T_{2,2} &amp; \cdots &amp; T_{n,2} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
T_{1,n} &amp; T_{2,n} &amp; \cdots &amp; T_{n,n}
\end{array}
\right)\]

<!-- Block math, keep all blank lines -->
<p>where \(T_{i,j}\) is the \(i,j\) cofactor of matrix T.</p>
<h3 id="116-equivalence-relation"><span class="me-2">1.16: EQUIVALENCE RELATION</span><a href="#116-equivalence-relation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>A binary relation \(\sim\) on a set \(\mathbb{X}\) is said to be an equivalence relation if and only if it is relexive, symmetric and transitive. That is, for all \(a, b, c \in \mathbb{X}\) :</p>
<ol>
  <li>Reflexive: \(a \sim a\)</li>
  <li>Symmetric: \(a \sim b\) if and only of \(b \sim a\)</li>
  <li>Transitive: If \(a \sim b\) and \(b \sim c\), then \(a \sim c\)</li>
</ol>

<h1 id="2-properties">2. PROPERTIES</h1>
<h2 id="21-matrix-similarity-is-equivalence-relation"><span class="me-2">2.1: Matrix similarity is equivalence relation</span><a href="#21-matrix-similarity-is-equivalence-relation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p>We aim to prove that the similarity matrix relation satisfies the properties of reflexivity, symmetry, and transitivity.</p>
<ol>
  <li>
    <p>Reflexive:
For any square matrix \(A\), it is obvious that \(I^{-1}AI = A\), where \(I\) is the identity matrix. Therefore, \(A\) is similar to itself.</p>
  </li>
  <li>
    <p>Symmetric: 
Suppose that B is similar to A,which means there exists an invertible matrix \(P\) such that \(B = P^{-1}AP\) \(\implies B P^{-1} = P^{-1}AI = P^{-1}A\)  <br />
\(\iff PBP^{-1} = IA = A\) 
<br />
This implies A is also similar to B, so we have the similarity relation is symmetric.</p>
  </li>
  <li>
    <p>Transitive:
Consider three matrices \(A\), \(B\), and \(C\) such that \(A\) is similar to \(B\) and \(B\) is similar to \(C\). Hence, there exist invertible matrices \(P\), \(Q\) such that \(B = P^{-1}AP\) and \(C = Q^{-1}BQ\). <br />
\(\implies C = Q^{-1}P^{-1}APQ = (PQ)^{-1}APQ\)</p>
  </li>
</ol>

<p>Therefore, \(C\) is similar to \(A\), and matrix similarity relation is transitive.</p>

<h2 id="22-similarity-matrix-are-matrix-representations-of-the-same-linear-mapping-but-with-respect-to-different-bases"><span class="me-2">2.2: Similarity matrix are matrix representations of the same linear mapping but with respect to different bases.</span><a href="#22-similarity-matrix-are-matrix-representations-of-the-same-linear-mapping-but-with-respect-to-different-bases" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p><strong>Lemma 1:</strong> Consider three vector spaces \(U,V,W\) and two linear mappings \(f \colon U \to V\) and \(g \colon V \to W\); \(B,D,E\) are the bases of 3 vector spaces, respectively. Then we have 
<!-- Block math, keep all blank lines --></p>

\[\operatorname{Rep}_{B,E}(f \circ g) = \operatorname{Rep}_{B,D}(f) \cdot \operatorname{Rep}_{D,E}(g)\]

<!-- Block math, keep all blank lines -->
<p><a href="/assets/img/similarity_matrix/matrix_transform.jpg" class="popup img-link  shimmer"><img src="/assets/img/similarity_matrix/matrix_transform.jpg" alt="Desktop View" loading="lazy"></a>
<em>The notation id represent for identity matrix</em></p>

<p>Consider two similar matrices \(H,G\) such that \(H= \operatorname{Rep}_{B,B}(h)\) and \(G= P^{-1}HP\), where \(B\) is the basis of vector space \(V\) including \(\overrightarrow{\beta_1}, \overrightarrow{\beta_2}, \ldots, \overrightarrow{\beta_n}\). \(h\) is a linear mapping. <br />
Consider the basis \(D\) such that \(\operatorname{Rep}_{D,B}(id) = P\)
From the figure above, we have \(h = id \circ h \circ id\). <br />
 Using <strong>Lemma 1</strong> then we have
<!-- Block math, keep all blank lines --></p>

\[\operatorname{Rep}_{D,D}(h) = \operatorname{Rep}_{D,B}(id) \cdot \operatorname{Rep}_{B,B}(h) \cdot \operatorname{Rep}_{B,D}(id) = P^{-1}HP 
\iff G = \operatorname{Rep}_{D,D}(h) \: \: \: (Q.E.D)\]

<!-- Block math, keep all blank lines -->
<h2 id="23-similarity-matrices-have-the-same-rank"><span class="me-2">2.3: Similarity matrices have the same rank.</span><a href="#23-similarity-matrices-have-the-same-rank" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p><strong>Lemma 2:</strong> \(f \colon U \to V\) is a linear mapping with a matrix representation \(A\). Then we have rank \(A\) equals to the dimension of the image space of linear mapping \(f\)</p>

<p>So from <strong>Lemma 2</strong> and <strong>Properties 2.1</strong>, we infer that similar matrices have the same rank.</p>

<h2 id="24-similarity-matrices-have-the-same-determinant"><span class="me-2">2.4: Similarity matrices have the same determinant.</span><a href="#24-similarity-matrices-have-the-same-determinant" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p>Consider two similar matrices \(A, B\). Since \(A, B\) are two similar matrices, there exists an invertible matric P such that \(B = PAP^{-1}\)
We have 
<!-- Block math, keep all blank lines --></p>

\[\begin{align*}
|B| &amp;= |PAP^{-1}| \\
&amp;= |P| |A| |P^{-1}| \\
&amp;= |A| |PP^{-1}| \\
&amp;= |A| \: \: \text{Determinant of identity matrix equals 1}
\end{align*}\]

<!-- Block math, keep all blank lines -->
<p>So we have \(|A| = |B|\)   (Q.E.D)</p>

<h2 id="25-similarity-matrices-have-the-same-trace"><span class="me-2">2.5: Similarity matrices have the same trace.</span><a href="#25-similarity-matrices-have-the-same-trace" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p><strong>Lemma 3:</strong> \(M,N\) are two arbitrary square matrices, then we have \(\operatorname{tr}(MN) = \operatorname{tr}(NM)\)</p>

<p>Since \(A, B\) are two similar matrices, there exists an invertible matric P such that \(B = PAP^{-1}\)<br />
\(\implies \operatorname{tr}(B) = \operatorname{tr}(P^{-1}AP)\) <br />
Using <strong>Lemma 3</strong> we have :
<!-- Block math, keep all blank lines --></p>

\[\begin{align*}
\operatorname{tr}(B) &amp;= \operatorname{tr}(PAP^{-1}) \\
&amp;= \operatorname{tr}(APP^{-1}) \\
&amp;= \operatorname{tr}(AI) \\
&amp;= \operatorname{tr}(A)
\end{align*}\]

<!-- Block math, keep all blank lines -->
<p>So we have \(\operatorname{trace}(A) = \operatorname{trace}(B)\)<br />
<strong>Remark:</strong> If two square matrices of the same order \(A\) and \(B\) satisfy \(\operatorname{tr}(A) = \operatorname{tr}(B)\) and \(|A| = |B|\), we cannot conclude that \(A\) and \(B\) are similar matrices.</p>

<p>Indeed, we will demonstrate a counterexample.
Consider the two matrices 
\(A = \begin{pmatrix}
1 &amp; 0 \\
0 &amp; 1
\end{pmatrix},
B = \begin{pmatrix}
1 &amp; 1 \\
0 &amp; 1
\end{pmatrix}.\)</p>

<p>It is easy to see that 
\(|A| = \left| B \right| = 1, \quad \operatorname{tr}(A) = \operatorname{tr}(B) = 2.\)</p>

<p>However, there was not exist an invertible matrix \(P\) such that \(B = PAP^{-1}\) because if it were so, then \(B = PAP^{-1} = PIP^{-1} = PP^{-1} = I\) which is absurd.</p>

<p>Therefore, we conclude that \(A\) and \(B\) are not similar matrices.</p>

<h2 id="26-matrix-adjoint-of-two-similar-matrices-are-also-similar"><span class="me-2">2.6: Matrix adjoint of two similar matrices are also similar.</span><a href="#26-matrix-adjoint-of-two-similar-matrices-are-also-similar" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p><strong>Lemma 4(Laplace Expansion of Determinants):</strong> For an \(n \times n\) square matrix \(T\) with the \(i,j\) cofactor \(T_{i,j}\), the entry at row \(i\), column \(j\) of matrix \(T\) is denoted by \(t_{i,j}\). Then we have:
<!-- Block math, keep all blank lines --></p>

\[\begin{align*}
|T| &amp;= t_{i,1}T_{i,1} + t_{i,2}T_{i,2} + \ldots + t_{i,n}T_{i,n} \\
&amp;= t_{1,j}T_{1,j} + t_{2,j}T_{2,j} + \ldots + t_{n,j}T_{n,j} \\
\end{align*}\]

<!-- Block math, keep all blank lines -->

<p><strong>Lemma 5:</strong> Consider \(M_n\) is the set contains all square matrix of degree \(n\). Then the function \(f \colon M_n \to M_n\) that satisfy \(f(H) = adj(H), \forall H \in M_n\) is a continuous function.</p>

<p>Consider the matrix 
\(T = \begin{pmatrix}
t_{1,1} &amp; t_{1,2} &amp; \ldots &amp; t_{1,n} \\
t_{2,1} &amp; t_{2,2} &amp; \ldots &amp; t_{2,n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
t_{n,1} &amp; t_{n,2} &amp; \ldots &amp; t_{n,n}
\end{pmatrix}\) 
with the adjugate 
\(\text{adj}(T) = \begin{pmatrix}
T_{1,1} &amp; T_{1,2} &amp; \ldots &amp; T_{1,n} \\
T_{2,1} &amp; T_{2,2} &amp; \ldots &amp; T_{2,n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
T_{n,1} &amp; T_{n,2} &amp; \ldots &amp; T_{n,n}
\end{pmatrix}\)</p>

<p>First, we prove that
\(\sum_{j=1}^{n}t_{i,j} T_{k,j} = \sum_{j=1}^{n} t_{j,i}T_{j,k} = 0 \text{ for all } i \neq k\)
<br />
\(T\) is a square matrix. Consider the matrix \(A\) defined by preserving all values of the matrix \(T\) except for the values in row \(k\); the values in row \(k\) of matrix \(T\) equal to row \(i\) in matrix \(A\). Similarly, matrix A have entries \(a_{i,j}\) and \(i,j\) cofactor \(A_{i,j}\). <br />
Since the row \(i\) and the row \(k\) of matrix \(A\) is the same, it implies that \(|A| = 0\). Using <strong>Lemma 4</strong> we have</p>

<!-- Block math, keep all blank lines -->

\[\begin{align*}
\sum_{j=1}^{n}t_{i,j} T_{k,j} &amp;= \sum_{j=1}^{n} a_{k,j}T_{k,j} &amp; \text{(Row i of T and row k of A are the same)}  \\ 
&amp;= \sum_{j=1}^{n} a_{k,j}A_{k,j} &amp; \text{(when eliminating row k, the remaining of A and T are the same)} \\
&amp;= |A|  &amp; text{Lemma 4} \\ 
&amp;= 0 &amp; text{(A have 2 rows with the same values.)}
\end{align*}\]

<!-- Block math, keep all blank lines -->
<p>Similarly, we prove that 
\(\sum_{j=1}^{n}t_{j,i} T_{k,j} = \sum_{j=1}^{n} t_{j,i}T_{j,k} = 0 \text{ for all } i \neq k\)</p>

<p>Using the <strong>Lemma 4</strong> we imply that
<!-- Block math, keep all blank lines --></p>

\[T \operatorname{adj}(T) = \operatorname{adj}(T) T = |T| \cdot I \: \: \: \: (1)\]

<!-- Block math, keep all blank lines -->
<p><br />
Consider two similar matrices A and B. There exist an invertible matrix \(P\) such that \(B = P^{-1}AP\) <br />
We prove the <strong>Property 2.5</strong> in two cases: \(|A| = 0\) and \(|A| \neq 0\)</p>

<p><strong>Case 1:</strong> \(|A| \neq 0\) <br />
Using <strong>Property 2.4</strong> we imply \(|B| \neq 0\), then \(A\) and \(B\) are invertible matrices. 
Using <strong>(1)</strong> we have 
<!-- math block  --></p>

\[\operatorname{adj}(A) = |A| \cdot A^{-1} \\
\operatorname{adj}(B) = |B| \cdot B^{-1}\]

<!-- math block -->

<!-- Block math, keep all blank lines -->

\[\begin{align*}
\operatorname{adj}(AB) &amp;= |AB| \cdot (AB)^{-1} \\
&amp;= |A| \cdot |B| \cdot B^{-1} A^{-1} \\
&amp;= |B| \cdot B^{-1} \cdot |A| \cdot A^{-1} \\
&amp;= \operatorname{adj}(B) \operatorname{adj}(A)
\end{align*}\]

<!-- math block  -->

<p>From the above, we imply the following:
<!-- math block --></p>

\[\begin{align*}
\operatorname{adj}(B) &amp;= \operatorname{adj}(PAP^{-1}) \\
&amp;= \operatorname{adj}(P) \operatorname{adj}(AP^{-1}) \\
&amp;= \operatorname{adj}(P) \operatorname{adj}(P^{-1}) \operatorname{adj}(A) \\
\end{align*}\]

<!-- Block math, keep all blank lines -->
<p>Otherwise, from <strong>(1)</strong> we have \(T^{-1} \operatorname{adj}(T^{-1}) = |T^{-1}| \cdot I\) with an invertible matrix \(T\) <br />
\(\implies \operatorname{adj}(T^{-1}) = |T^{-1}| \cdot T\) <br />
But we have \(\operatorname{adj}(T) = |T| \cdot T^{-1} \implies \operatorname{adj}(T) \operatorname{adj}(T^{-1}) = 1\) <br />
So we imply that \(\operatorname{adj}(B) = \operatorname{adj}(P)^{-1} \operatorname{adj}(A) \operatorname{adj}(P)\), which means \(\operatorname{adj}(A)\) and \(\operatorname{adj}(B)\) are two similar matrices
. <br />
<strong>Case 2:</strong> \(|A| = 0\)</p>


  </div>

  <div class="post-tail-wrapper text-muted">
    <!-- categories -->
    
      <div class="post-meta mb-3">
        <i class="far fa-folder-open fa-fw me-1"></i>
        
          <a href="/categories/mathematics/">Mathematics</a>,
          <a href="/categories/linear-algebra/">Linear Algebra</a>
      </div>
    

    <!-- tags -->
    
      <div class="post-tags">
        <i class="fa fa-tags fa-fw me-1"></i>
        
          <a
            href="/tags/similar-matrices/"
            class="post-tag no-text-decoration"
          >similar matrices</a>
        
          <a
            href="/tags/similarity-matrix/"
            class="post-tag no-text-decoration"
          >similarity matrix</a>
        
          <a
            href="/tags/mathematics/"
            class="post-tag no-text-decoration"
          >mathematics</a>
        
          <a
            href="/tags/algebra/"
            class="post-tag no-text-decoration"
          >algebra</a>
        
      </div>
    

    <div
      class="
        post-tail-bottom
        d-flex justify-content-between align-items-center mt-5 pb-2
      "
    >
      <div class="license-wrapper">
        
          

          This post is licensed under 
        <a href="https://creativecommons.org/licenses/by/4.0/">
          CC BY 4.0
        </a>
         by the author.
        
      </div>

      <!-- Post sharing snippet -->

<div class="share-wrapper d-flex align-items-center">
  <span class="share-label text-muted">Share</span>
  <span class="share-icons">
    
    
    

    

      

      <a href="https://twitter.com/intent/tweet?text=Similar%20Properties%20of%20Similarity%20Matrix%20-%20NgocQuan's%20Research&url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Fsimilarity-matrix%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter">
        <i class="fa-fw fa-brands fa-square-x-twitter"></i>
      </a>
    

      

      <a href="https://www.facebook.com/sharer/sharer.php?title=Similar%20Properties%20of%20Similarity%20Matrix%20-%20NgocQuan's%20Research&u=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Fsimilarity-matrix%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook">
        <i class="fa-fw fab fa-facebook-square"></i>
      </a>
    

      

      <a href="https://t.me/share/url?url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Fsimilarity-matrix%2F&text=Similar%20Properties%20of%20Similarity%20Matrix%20-%20NgocQuan's%20Research" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram">
        <i class="fa-fw fab fa-telegram"></i>
      </a>
    

    <button
      id="copy-link"
      aria-label="Copy link"
      class="btn small"
      data-bs-toggle="tooltip"
      data-bs-placement="top"
      title="Copy link"
      data-title-succeed="Link copied successfully!"
    >
      <i class="fa-fw fas fa-link pe-none fs-6"></i>
    </button>
  </span>
</div>

    </div>
    <!-- .post-tail-bottom -->
  </div>
  <!-- div.post-tail-wrapper -->
</article>


            
          </main>

          <!-- panel -->
          <aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 mb-5 text-muted">
            <div class="access">
              <!-- Get 5 last posted/updated posts -->














  <section id="access-lastmod">
    <h2 class="panel-heading">Recently Updated</h2>
    <ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2">
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/NgocQuan/">Hello World, I'm NgocQuan</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/similarity-matrix/">Similar Properties of Similarity Matrix</a>
        </li>
      
    </ul>
  </section>
  <!-- #access-lastmod -->


              <!-- The trending tags list -->















  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Trending Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/algebra/">algebra</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/mathematics/">mathematics</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/ngocquan/">ngocquan</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/similar-matrices/">similar matrices</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/similarity-matrix/">similarity matrix</a>
      
    </div>
  </section>


            </div>

            
              
              



  <section id="toc-wrapper" class="ps-0 pe-4">
    <h2 class="panel-heading ps-3 pt-2 mb-2">Contents</h2>
    <nav id="toc"></nav>
  </section>


            
          </aside>
        </div>

        <div class="row">
          <!-- tail -->
          <div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              
              <!-- Recommend the other 3 posts according to the tags and categories of the current post. -->

<!-- The total size of related posts -->


<!-- An random integer that bigger than 0 -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy} -->














  

  
    











            
              
              <!-- Navigation buttons at the bottom of the post. -->

<nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation">
  
  

  
    <a
      href="/posts/NgocQuan/"
      class="btn btn-outline-primary"
      aria-label="Older"
    >
      <p>Hello World, I'm NgocQuan</p>
    </a>
  

  
    <div class="btn btn-outline-primary disabled" aria-label="Newer">
      <p>-</p>
    </div>
  
</nav>

            
              
              <!--  The comments switcher -->


            

            <!-- The Footer -->

<footer
  aria-label="Site Info"
  class="
    d-flex flex-column justify-content-center text-muted
    flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3
  "
>
  <p>
    ©
    <time>2024</time>
    <a href="https://twitter.com/ngocquanofficial">Pham Ngoc Quan</a>.
    
      <span
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author."
      >Some rights reserved.</span>
    
  </p>

  <p>Using the <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>.
  </p>
</footer>

          </div>
        </div>

        <!-- The Search results -->

<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
  <div class="col-11 content">
    <div id="search-hints">
      <!-- The trending tags list -->















  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Trending Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/algebra/">algebra</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/mathematics/">mathematics</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/ngocquan/">ngocquan</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/similar-matrices/">similar matrices</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/similarity-matrix/">similarity matrix</a>
      
    </div>
  </section>


    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>

      </div>

      <aside aria-label="Scroll to Top">
        <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow">
          <i class="fas fa-angle-up"></i>
        </button>
      </aside>
    </div>

    <div id="mask"></div>

    
      <aside
  id="notification"
  class="toast"
  role="alert"
  aria-live="assertive"
  aria-atomic="true"
  data-bs-animation="true"
  data-bs-autohide="false"
>
  <div class="toast-header">
    <button
      type="button"
      class="btn-close ms-auto"
      data-bs-dismiss="toast"
      aria-label="Close"
    ></button>
  </div>
  <div class="toast-body text-center pt-0">
    <p class="px-2 mb-3">A new version of content is available.</p>
    <button type="button" class="btn btn-primary" aria-label="Update">
      Update
    </button>
  </div>
</aside>

    

    <!-- JavaScripts -->

    <!-- JS selector for site. -->

<!-- commons -->



<!-- layout specified -->


  

  
    <!-- image lazy-loading & popup & clipboard -->
    
  















  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  



  <script src="https://cdn.jsdelivr.net/combine/npm/jquery@3.7.1/dist/jquery.min.js,npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js,npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/magnific-popup@1.1.0/dist/jquery.magnific-popup.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.10/dayjs.min.js,npm/dayjs@1.11.10/locale/en.min.js,npm/dayjs@1.11.10/plugin/relativeTime.min.js,npm/dayjs@1.11.10/plugin/localizedFormat.min.js,npm/tocbot@4.25.0/dist/tocbot.min.js"></script>






<script defer src="/assets/js/dist/post.min.js"></script>


  <!-- MathJax -->
  <script>
    /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */
    MathJax = {
      tex: {
        /* start/end delimiter pairs for in-line math */
        inlineMath: [
          ['$', '$'],
          ['\\(', '\\)']
        ],
        /* start/end delimiter pairs for display math */
        displayMath: [
          ['$$', '$$'],
          ['\\[', '\\]']
        ]
      }
    };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"></script>





    

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script>
  /* Note: dependent library will be loaded in `js-selector.html` */
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('search-results'),
    json: '/assets/js/data/search.json',
    searchResultTemplate: '  <article class="px-1 px-sm-2 px-lg-4 px-xl-0">    <header>      <h2><a href="{url}">{title}</a></h2>      <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">        {categories}        {tags}      </div>    </header>    <p>{snippet}</p>  </article>',
    noResultsText: '<p class="mt-5"></p>',
    templateMiddleware: function(prop, value, template) {
      if (prop === 'categories') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
        }
      }

      if (prop === 'tags') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
        }
      }
    }
  });
</script>

  </body>
</html>

