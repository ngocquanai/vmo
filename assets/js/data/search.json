[
  
  {
    "title": "Similar Properties of Similarity Matrix",
    "url": "/posts/similarity-matrix/",
    "categories": "Mathematics, Linear Algebra",
    "tags": "similar matrices, similarity matrix, mathematics, algebra",
    "date": "2024-02-10 00:00:00 +0700",
    





    
    "snippet": "Abstract: Firstly, I will introduce specific definitions relevant to matrix similarity topic. Subsequently, I will explore various common properties of similar matrices, such as trace, rank, determ...",
    "content": "Abstract: Firstly, I will introduce specific definitions relevant to matrix similarity topic. Subsequently, I will explore various common properties of similar matrices, such as trace, rank, determinant, among others. Finally, I will present criteria for determining whether any two given matrices are similar.DEFINITIONS1.1: SIMILARITY MATRIXTwo square \\(n \\times n\\) matrices A and B are called similar if there exists an invertible \\(n*n\\) matrix \\(P\\) such that\\[B = P^{-1}AP\\]1.2: REPRESENTATION OF A VECTOR\\(V\\) is a vector space with the basis \\(B=\\left&lt;\\overrightarrow{\\beta_1}, \\overrightarrow{\\beta_2}, \\ldots, \\overrightarrow{\\beta_n}\\right&gt; , \\vec{v} \\in V,  \\alpha_1, \\ldots, \\alpha_n \\in \\mathbb{R}\\) such that \\(\\vec{v}=\\alpha_1 \\overrightarrow{\\beta_1}+\\alpha_2 \\overrightarrow{\\beta_2}+\\ldots+\\alpha_n \\overrightarrow{\\beta_n}.\\) Then we have\\[{Rep}_B(\\vec{v})=\\left(\\begin{array}{c} \\alpha_1 \\\\ \\alpha_2 \\\\ \\vdots \\\\ \\alpha_n \\end{array}\\right)\\]1.3: MATRIX REPRESENTATION\\(V, W\\) are vector spaces with dimension \\(n, m\\) and basis \\(B, D\\). \\(f \\colon V \\to W\\) is a linear mapping. If we have \\(\\operatorname{Rep}_D(h(\\overrightarrow{\\beta_1}))=\\left(\\begin{array}{c}h_{1,1} \\\\ h_{2,1} \\\\ \\vdots \\\\ h_{n, 1}\\end{array}\\right), \\ldots, \\operatorname{Rep}_D(h(\\overrightarrow{\\beta_n}))=\\left(\\begin{array}{c}h_{1, n} \\\\ h_{2, n} \\\\ \\vdots \\\\ h_{n, n}\\end{array}\\right)\\). Then\\[\\left(\\begin{array}{cccc}h_{1,1} &amp; h_{1,2} &amp; \\cdots &amp; h_{1, n} \\\\ h_{2,1} &amp; h_{2,2} &amp; \\cdots &amp; h_{2, n} \\\\ \\vdots &amp; \\vdots &amp;  &amp; \\vdots \\\\ h_{n, 1} &amp; h_{n, 2} &amp; \\cdots &amp; h_{n, n}\\end{array}\\right)\\]is called matrix representation of mapping f with respect to the bases \\(B, D\\), notated by \\({Rep}_{B,D}(f)\\)1.4: DETERMINANT OF MATRIX\\(n \\times n\\) determinant is a function \\(f \\colon M_{n \\times n} \\rightarrow \\mathbb{R}\\) such that (1) \\(\\operatorname{det}\\left(\\overrightarrow{p_1}, \\ldots, k \\overrightarrow{p_i}+\\overrightarrow{p_j}, \\ldots, \\overrightarrow{p_n}\\right)=\\operatorname{det}\\left(\\overrightarrow{p_1}, \\ldots, \\overrightarrow{p_j}, \\ldots, \\overrightarrow{p_n}\\right)\\), for all \\(k \\in \\mathbb{R}\\) (2) \\(\\operatorname{det}\\left(\\overrightarrow{p_1}, \\ldots, \\overrightarrow{p_j}, \\ldots, \\overrightarrow{p_i}, \\ldots, \\overrightarrow{p_n}\\right)=-\\operatorname{det}\\left(\\overrightarrow{p_1}, \\ldots, \\overrightarrow{p_i}, \\ldots, \\overrightarrow{p_j}, \\ldots, \\overrightarrow{p_n}\\right)\\) (3) \\(\\operatorname{det}\\left(\\overrightarrow{p_1}, \\ldots, k \\overrightarrow{p_i}, \\ldots, \\overrightarrow{p_n}\\right) = k \\operatorname{det}\\left(\\overrightarrow{p_1}, \\ldots, \\overrightarrow{p_i}, \\ldots, \\overrightarrow{p_n}\\right)\\), for all \\(k \\in \\mathbb{R}\\) (4) \\(\\operatorname{det}(I) = 1\\), \\(I\\) is the identity matrix. Normally, we use the notation \\(|T|\\) instead of \\(\\operatorname{det}(T)\\).1.5: TRACE OF MATRIXThe trace of a square matrix \\(A\\) is the sum of elements on the main diagonal of matrix \\(A\\), denoted by \\(tr(A)\\)1.6: POLYNOMIAL OF MATRIX\\(T\\) is a square matrix, \\(f(x)\\) is a polynomial with degree n \\(f(x) = c_n x^n + c_{n-1} x^{n-1} + \\cdots + c_1 x + c_0\\) \\((c_i \\in \\mathbb{R})\\). Then we have \\(f(T) = c_n T^n + c_{n-1} T^{n-1} + \\cdots + c_1 T + c_0 I\\) (\\(I\\) is identity matrix) is a polynomial of matrix \\(T\\) and also a square matrix of the same size as \\(T\\).1.7: MINIMAL POLYNOMIALFor a square matrix \\(T\\), its minimal polynomial is the monic polynomial \\(f\\) of the lowest degree such that \\(f(T) = 0\\).1.8: INDEX OF NIPOTENTA square matrix $A$ is said to have a nilpotent index $k$ if \\(k\\) is the smallest natural number(if exists) such that $A^k = 0$.1.9: SUBDIAGONALThe subdiagonal of a square matrix is the line immediately below the main diagonal of the matrix.1.10: JORDAN BLOCKA Jordan block is a square matrix where the off-diagonal entries are zero, the entries on the main diagonal are all $\\lambda$ (where $\\lambda \\in \\mathbb{C}$), and every entry on the subdiagonal is 1.1.11: JORDAN CANONICAL FORMThe square matrix $A$ can be represented in the form of a Jordan normal form if \\[A = \\left(\\begin{array}{cccc}J_{\\lambda_1} &amp; 0 &amp; \\cdots &amp; 0 \\\\0 &amp; J_{\\lambda_2} &amp; \\ddots &amp; \\vdots \\\\\\vdots &amp; \\ddots &amp; \\ddots &amp; 0 \\\\0 &amp; \\cdots &amp; 0 &amp; J_{\\lambda_k} \\\\\\end{array}\\right)\\]with \\(J_{\\lambda_i}\\) are Jordan blocks.1.12: CHARACTERISTIC POLYNOMIALA is an \\(n \\times n\\) matrix, the characteristic polynomial of A, denoted by \\(p_{A}(x)\\) is defined by\\[p_{A}(x) = \\operatorname{det}(A - xI)\\]where \\(I\\) denotes the \\(n \\times n\\) identity matrix.1.13: GEOMETRIC AND ALGEBRAIC MULTIPLICITYA is a square matrix with characteristic polynomial \\(f(x) = (x- \\lambda_1)^{m_1} (x - \\lambda_2)^{m_2} \\ldots (x- \\lambda_r)^{m_r}\\). Then \\(m_i\\) is called algebraic multiplicity corresponding to eigenvalue \\(\\lambda_i\\), the dimension of eigenspace corresponding to eigenvalue \\(\\lambda_i\\) is called geometric multiplicity.1.14: COFACTOR OF MATRIXConsider the \\(n \\times n\\) square matrix \\(T\\), we define the \\(i,j\\) minor \\(M_{i,j}\\) of matrix \\(T\\) is the \\((n-1) \\times (n-1)\\) matrix obtained by eliminating the i-th row and j-th column in the matrix \\(T\\).Then we have \\(i,j\\) cofactor \\(T_{i,j} = (-1)^{i+j} \\operatorname{det}(M_{i,j})\\)1.15: MATRIX ADJOINTMatrix adjoint of square matrix T is denoted by\\[\\text{adj}(T) = \\left(\\begin{array}{cccc}T_{1,1} &amp; T_{2,1} &amp; \\cdots &amp; T_{n,1} \\\\T_{1,2} &amp; T_{2,2} &amp; \\cdots &amp; T_{n,2} \\\\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\T_{1,n} &amp; T_{2,n} &amp; \\cdots &amp; T_{n,n}\\end{array}\\right)\\]where \\(T_{i,j}\\) is the \\(i,j\\) cofactor of matrix T.1.16: EQUIVALENCE RELATIONA binary relation \\(\\sim\\) on a set \\(\\mathbb{X}\\) is said to be an equivalence relation if and only if it is relexive, symmetric and transitive. That is, for all \\(a, b, c \\in \\mathbb{X}\\) :  Reflexive: \\(a \\sim a\\)  Symmetric: \\(a \\sim b\\) if and only of \\(b \\sim a\\)  Transitive: If \\(a \\sim b\\) and \\(b \\sim c\\), then \\(a \\sim c\\)2. PROPERTIES2.1: EQUIVALENCE RELATIONPROPERTY 2.1:  Matrix similarity is equivalence relation. We aim to prove that the similarity matrix relation satisfies the properties of reflexivity, symmetry, and transitivity.      Reflexive:For any square matrix \\(A\\), it is obvious that \\(I^{-1}AI = A\\), where \\(I\\) is the identity matrix. Therefore, \\(A\\) is similar to itself.        Symmetric: Suppose that B is similar to A,which means there exists an invertible matrix \\(P\\) such that \\(B = P^{-1}AP\\) \\(\\implies B P^{-1} = P^{-1}AI = P^{-1}A\\)  \\(\\iff PBP^{-1} = IA = A\\) This implies A is also similar to B, so we have the similarity relation is symmetric.        Transitive:Consider three matrices \\(A\\), \\(B\\), and \\(C\\) such that \\(A\\) is similar to \\(B\\) and \\(B\\) is similar to \\(C\\). Hence, there exist invertible matrices \\(P\\), \\(Q\\) such that \\(B = P^{-1}AP\\) and \\(C = Q^{-1}BQ\\). \\(\\implies C = Q^{-1}P^{-1}APQ = (PQ)^{-1}APQ\\)  Therefore, \\(C\\) is similar to \\(A\\), and matrix similarity relation is transitive.2.2: SIMILAR LINEAR MAPPINGPROPERTY 2.2: Similarity matrices are matrix representations of the same linear mapping but with respect to different bases. Lemma 1: Consider three vector spaces \\(U,V,W\\) and two linear mappings \\(f \\colon U \\to V\\) and \\(g \\colon V \\to W\\); \\(B,D,E\\) are the bases of 3 vector spaces, respectively. Then we have $$\\operatorname{Rep}{B,E}(f \\circ g) = \\operatorname{Rep}{B,D}(f) \\cdot \\operatorname{Rep}_{D,E}(g)$$The notation id represent for identity matrixConsider two similar matrices \\(H,G\\) such that \\(H= \\operatorname{Rep}_{B,B}(h)\\) and \\(G= P^{-1}HP\\), where \\(B\\) is the basis of vector space \\(V\\) including \\(\\overrightarrow{\\beta_1}, \\overrightarrow{\\beta_2}, \\ldots, \\overrightarrow{\\beta_n}\\). \\(h\\) is a linear mapping. Consider the basis \\(D\\) such that \\(\\operatorname{Rep}_{D,B}(id) = P\\)From the figure above, we have \\(h = id \\circ h \\circ id\\).  Using Lemma 1 then we have\\[\\operatorname{Rep}_{D,D}(h) = \\operatorname{Rep}_{D,B}(id) \\cdot \\operatorname{Rep}_{B,B}(h) \\cdot \\operatorname{Rep}_{B,D}(id) = P^{-1}HP \\iff G = \\operatorname{Rep}_{D,D}(h) \\: \\: \\: (Q.E.D)\\]2.3: SIMILAR RANKPROPERTY 2.3: Similarity matrices have the same rank. Lemma 2: \\(f \\colon U \\to V\\) is a linear mapping with a matrix representation \\(A\\). Then we have rank \\(A\\) equals to the dimension of the image space of linear mapping \\(f\\)So from Lemma 2 and Properties 2.1, we infer that similar matrices have the same rank.2.4: SIMILAR DETERMINANTPROPERTY 2.4: Similarity matrices have the same determinant. Consider two similar matrices \\(A, B\\). Since \\(A, B\\) are two similar matrices, there exists an invertible matric P such that \\(B = PAP^{-1}\\)We have \\[\\begin{align*}|B| &amp;= |PAP^{-1}| \\\\&amp;= |P| |A| |P^{-1}| \\\\&amp;= |A| |PP^{-1}| \\\\&amp;= |A| \\: \\: \\text{Determinant of identity matrix equals 1}\\end{align*}\\]So we have \\(|A| = |B|\\)   (Q.E.D)2.5: SIMILAR TRACEPROPERTY 2.5: Similarity matrices have the same trace. Lemma 3: \\(M,N\\) are two arbitrary square matrices, then we have \\(\\operatorname{tr}(MN) = \\operatorname{tr}(NM)\\). \\Since \\(A, B\\) are two similar matrices, there exists an invertible matric P such that \\(B = PAP^{-1}\\)\\(\\implies \\operatorname{tr}(B) = \\operatorname{tr}(P^{-1}AP)\\) Using Lemma 3 we have :\\[\\begin{align*}\\operatorname{tr}(B) &amp;= \\operatorname{tr}(PAP^{-1}) \\\\&amp;= \\operatorname{tr}(APP^{-1}) \\\\&amp;= \\operatorname{tr}(AI) \\\\&amp;= \\operatorname{tr}(A)\\end{align*}\\]So we have \\(\\operatorname{trace}(A) = \\operatorname{trace}(B)\\)Remark: If two square matrices of the same order \\(A\\) and \\(B\\) satisfy \\(\\operatorname{tr}(A) = \\operatorname{tr}(B)\\) and \\(|A| = |B|\\), we cannot conclude that \\(A\\) and \\(B\\) are similar matrices.Indeed, we will demonstrate a counterexample.Consider the two matrices \\(A = \\begin{pmatrix}1 &amp; 0 \\\\0 &amp; 1\\end{pmatrix},B = \\begin{pmatrix}1 &amp; 1 \\\\0 &amp; 1\\end{pmatrix}.\\)It is easy to see that \\(|A| = \\left| B \\right| = 1, \\quad \\operatorname{tr}(A) = \\operatorname{tr}(B) = 2.\\)However, there was not exist an invertible matrix \\(P\\) such that \\(B = PAP^{-1}\\) because if it were so, then \\(B = PAP^{-1} = PIP^{-1} = PP^{-1} = I\\) which is absurd.Therefore, we conclude that \\(A\\) and \\(B\\) are not similar matrices.2.6: SIMILARITY MATRIX ADJOINTPROPERTY 2.6: Matrix adjoint of two similar matrices are also similar.  Lemma 4(Laplace Expansion of Determinants): For an \\(n \\times n\\) square matrix \\(T\\) with the \\(i,j\\) cofactor \\(T_{i,j}\\), the entry at row \\(i\\), column \\(j\\) of matrix \\(T\\) is denoted by \\(t_{i,j}\\). Then we have:\\[\\begin{align*}|T| &amp;= t_{i,1}T_{i,1} + t_{i,2}T_{i,2} + \\ldots + t_{i,n}T_{i,n} \\\\&amp;= t_{1,j}T_{1,j} + t_{2,j}T_{2,j} + \\ldots + t_{n,j}T_{n,j} \\\\\\end{align*}\\]Lemma 5: Consider \\(M_n\\) is the set contains all square matrices of degree \\(n\\). Then the function \\(f \\colon M_n \\to M_n\\) that satisfy \\(f(H) = adj(H), \\forall H \\in M_n\\) is a continuous function. Consider the matrix \\(T = \\begin{pmatrix}t_{1,1} &amp; t_{1,2} &amp; \\ldots &amp; t_{1,n} \\\\t_{2,1} &amp; t_{2,2} &amp; \\ldots &amp; t_{2,n} \\\\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\t_{n,1} &amp; t_{n,2} &amp; \\ldots &amp; t_{n,n}\\end{pmatrix}\\) with the adjugate \\(\\text{adj}(T) = \\begin{pmatrix}T_{1,1} &amp; T_{2,1} &amp; \\ldots &amp; T_{n,1} \\\\T_{1,2} &amp; T_{2,2} &amp; \\ldots &amp; T_{n,2} \\\\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\T_{1,n} &amp; T_{2,n} &amp; \\ldots &amp; T_{n,n}\\end{pmatrix}\\)First, we prove that\\(\\sum_{j=1}^{n}t_{i,j} T_{k,j} = \\sum_{j=1}^{n} t_{j,i}T_{j,k} = 0 \\text{ for all } i \\neq k\\)Consider square matrix \\(T\\), and the matrix \\(A\\) defined by preserving all values of the matrix \\(T\\) except for the values in row \\(k\\); the values in row \\(k\\) of matrix \\(A\\) equal to row \\(i\\) in matrix \\(T\\).(which means row \\(i\\) and \\(k\\) of matrix A have the same values.) Similarly, matrix A have entries \\(a_{i,j}\\) and \\(i,j\\) cofactor \\(A_{i,j}\\). Since the row \\(i\\) and the row \\(k\\) of matrix \\(A\\) is the same, it implies that \\(|A| = 0\\). Using Lemma 4 we have\\[\\begin{align*}\\sum_{j=1}^{n}t_{i,j} T_{k,j} &amp;= \\sum_{j=1}^{n} a_{k,j}T_{k,j} &amp; \\text{(Row i of T and row k of A are the same)}  \\\\ &amp;= \\sum_{j=1}^{n} a_{k,j}A_{k,j} &amp; \\text{(when eliminating row k, the remaining of A and T are the same)} \\\\&amp;= |A|  &amp; \\text{Lemma 4} \\\\ &amp;= 0 &amp; \\text{(A have 2 rows with the same values.)}\\end{align*}\\]Similarly, we prove that \\(\\sum_{j=1}^{n} t_{j,i}T_{j,k} = 0, \\forall i \\neq k\\)Using the Lemma 4 we imply that\\[T \\operatorname{adj}(T) = \\operatorname{adj}(T) \\cdot T = |T| \\cdot I \\: \\: \\: \\: (1)\\]Consider two similar matrices A and B. There exist an invertible matrix \\(P\\) such that \\(B = P^{-1}AP\\) We prove the Property 2.5 in two cases: \\(|A| = 0\\) and \\(|A| \\neq 0\\)Case 1: \\(|A| \\neq 0\\) Using Property 2.4 we imply \\(|B| \\neq 0\\), then \\(A\\) and \\(B\\) are invertible matrices. Using (1) we have \\[\\begin{align*} \\operatorname{adj}(A) &amp; =\\operatorname{det}(A) \\cdot A^{-1} \\\\ \\operatorname{adj}(B) &amp; =\\operatorname{det}(B) \\cdot B^{-1} \\\\ \\operatorname{adj}(AB) &amp;= |AB| \\cdot (AB)^{-1} \\\\&amp;= |A| \\cdot |B| \\cdot B^{-1} A^{-1} \\\\&amp;= |B| \\cdot B^{-1} \\cdot |A| \\cdot A^{-1} \\\\&amp;= \\operatorname{adj}(B) \\operatorname{adj}(A)\\end{align*}\\]From the above, we imply the following:\\[\\begin{align*}\\operatorname{adj}(B) &amp;= \\operatorname{adj}(P^{-1}AP) \\\\&amp;= \\operatorname{adj}(P (AP^{-1}))&amp;= \\operatorname{adj}(AP^{-1}) \\operatorname{adj}(P) \\\\&amp;= \\operatorname{adj}(P^{-1}) \\operatorname{adj}(A) \\operatorname{adj}(P) \\\\\\end{align*}\\]Otherwise, from (1) we have \\(T^{-1} \\operatorname{adj}(T^{-1}) = |T^{-1}| \\cdot I\\) with an invertible matrix \\(T\\) \\(\\implies \\operatorname{adj}(T^{-1}) = |T^{-1}| \\cdot T\\) But we have \\(\\operatorname{adj}(T) = |T| \\cdot T^{-1} \\implies \\operatorname{adj}(T) \\operatorname{adj}(T^{-1}) = I\\) So we imply that \\(\\operatorname{adj}(B) = \\operatorname{adj}(P)^{-1} \\operatorname{adj}(A) \\operatorname{adj}(P)\\), which means \\(\\operatorname{adj}(A)\\) and \\(\\operatorname{adj}(B)\\) are two similar matrices. Case 2: \\(|A| = 0\\)From the Property 2.3 we imply \\(|B| = 0\\) For an arbitrary square matrix \\(M\\), we have \\(M\\) is an invertible matrix \\(\\iff |M| \\neq 0\\) Using Definition 1.12 we imply that matrix \\(A + \\lambda I\\) is invertible for all \\(\\lambda\\) such that \\(\\lambda\\) is not an eigenvalue of matrix \\(-A\\). Since matrix \\(-A\\) has finite eigenvalues; therefore, for all \\(\\lambda\\) small enough we have matrix \\(A + \\lambda I\\) is invertible, then :\\[\\operatorname{adj}((A + \\lambda I)B) = \\operatorname{adj}(B) \\operatorname{adj}(A + \\lambda I)\\]Using Lemma 5 we imply that :\\[\\begin{aligned}\\operatorname{adj}(A B) &amp; =\\operatorname{adj}\\left(\\lim _{\\lambda \\rightarrow 0}(A+\\lambda I) B\\right) \\\\&amp; =\\lim _{\\lambda \\rightarrow 0} \\operatorname{adj}((A+\\lambda I) B) \\\\&amp; =\\lim _{\\lambda \\rightarrow 0} \\operatorname{adj}(B) \\cdot \\operatorname{adj}(A+\\lambda I) \\\\&amp; =\\operatorname{adj}(B) \\cdot \\lim _{\\lambda \\rightarrow 0} \\operatorname{adj}(A+\\lambda I) \\\\&amp; =\\operatorname{adj}(B) . \\operatorname{adj}(A)\\end{aligned}\\]Hence, by a similar proof as in Case 1, we conclude that A and B are two similar matrices.2.7: SIMILAR EIGENVALUESPROPERTY 2.7: Similarity matrices have the same eigenvalues. Consider two similar matrices \\(A\\) and \\(B\\). Assume that B has an eigenvalue \\(\\lambda\\) corresponding to eigenvector \\(\\vec{v}\\). We have :\\[\\begin{aligned}B \\vec{v}=\\lambda \\vec{v} \\\\ \\Leftrightarrow &amp; P^{-1} A P \\vec{v}=\\lambda \\vec{v} \\\\ \\Leftrightarrow &amp; A P \\vec{v}=P \\lambda \\vec{v}=\\lambda P \\vec{v}\\end{aligned}\\]\\(\\implies\\) Every eigenvalue \\(\\lambda\\) of matrix \\(B\\) is also an eigenvalue of matrix \\(A\\) Using similarly proof, we have every eigenvalue of matrix \\(A\\) is also an eigenvalue of matrix \\(B\\); therefore, \\(A\\) and \\(B\\) have same eigenvalues. (Q.E.D)2.8: SIMILAR CHARACTERISTIC AND MINIMAL POLYNOMIALPROPERTY 2.8: \\(A\\) and \\(B\\) are two similar matrices, then \\(A\\) and \\(B\\) have the same characteristic polynomial and minimal polynomial. The characteristic polynomial of matrix \\(A\\) and \\(B\\) is \\(\\operatorname{det}(A - xI)\\) and \\(\\operatorname{det}(B - xI)\\), respectively, with \\(x\\) is a variable.We have:\\[\\begin{aligned} \\operatorname{det}(B-x I) &amp; =\\operatorname{det}\\left(P^{-1} A P-x I\\right) \\\\ &amp; =\\operatorname{det}\\left(P^{-1} A P-P^{-1} x I P\\right) \\\\ &amp; =\\operatorname{det}\\left(P^{-1}(A-x I) P\\right) \\\\ &amp; =\\operatorname{det}\\left(P^{-1}\\right) \\operatorname{det}(A-x I) \\operatorname{det} P \\\\ &amp; =\\operatorname{det}(A-x I) \\operatorname{det} P \\cdot \\operatorname{det}\\left(P^{-1}\\right) \\\\ &amp; =\\operatorname{det}(A-x I)\\end{aligned}\\]So we conclude that \\(A\\) and \\(B\\) have same characteristic polynomial. \\Consider polynomial \\(f(x)=x^n+a_{n-1} x^{n-1}+\\ldots+a_1 x+a_0\\) is a minimal polynomial of matrix \\(A\\). \\(\\implies f(A)=0\\). Then we have :\\[\\begin{aligned}f\\left( B \\right) &amp;= f\\left(P^{-1} A P\\right) \\\\&amp;= \\left(P^{-1} A P\\right)^n+a_{n-1}\\left(P^{-1} A P\\right)^{n-1}+\\ldots+a_n\\left(P^{-1} A P\\right) +a_0 I \\\\\\left(P^{-1} A P\\right)^n &amp;= P^{-1} A P \\cdot P^{-1} A P \\ldots P^{-1} A P \\\\&amp;=P^{-1} A\\left(P P^{-1}\\right) \\cdot A\\left(P P^{-1}\\right) \\ldots\\left(P P^{-1}\\right) A P \\\\&amp;=P^{-1} A^n P \\\\\\implies f(B) &amp;= P^{-1} A^n P +a_{n-1} P^{-1} A^{n-1} P +\\ldots +a_1 P^{-1} A P+a_0 P^{-1} I P \\\\&amp;=P^{-1}\\left(A^n+a_{n-1} A^{n-1}+\\ldots+a_1 A+a_0 I\\right) P \\\\&amp;=P^{-1} \\cdot f(A) \\cdot P \\\\&amp;=0\\end{aligned}\\]Consider \\(g(x)\\) is the minimal polynomial of matrix \\(B\\), since \\(f(B) = 0\\) then we imply \\(\\operatorname{deg}(f) \\geq \\operatorname{deg}(g)\\).Using similarly proof, we have \\(\\operatorname{deg}(g) \\geq \\operatorname{deg}(f)\\), then we imply that \\(\\operatorname{deg}(f) = \\operatorname{deg}(g)\\). Assume that \\(f(x) \\neq g(x)\\), then consider \\(h(x) = f(x) - g(x)\\). We have \\(h(x)\\) has smaller degree than \\(f , g\\), and \\(h(x)\\) is not a zero polynomial. \\(\\implies h(B) = f(B) - g(B) = 0\\) is a characteristic polynomial of matrix \\(B\\) with smaller degree than its minimal polynomial \\(g(x)\\)  (CONTRADICTION).Therefore, we have \\(f(x) = g(x)\\), which means two matrices \\(A\\) and \\(B\\) have same minimal polynomial.2.9: SIMILAR GEOMETRIC MULTIPLICITY, ALGEBRAIC MULTIPLICITYPROPERTY 2.9: \\(A\\) and \\(B\\) are two similar matrices. By Property 2.7 they also have same eigenvalues. Then for each eigenvalue, matrix \\(A\\) and \\(B\\) have same geometric multiplicity and algebraic multiplicity corresponding to this eigenvalue. From Property 2.7 we have \\(A\\) anh \\(B\\) have same chracteristic polynomial. By Definition 1.13 we imply that these two matrices have same algebraic multiplicity for each eigenvalue. Since \\(A\\) and \\(B\\) are similar, so there exist invertible matrix \\(P\\) such that \\(B=P^{-1}AP\\).Consider \\(\\lambda\\) is an eigenvalue of matrix \\(A\\) and \\(\\left( \\vec{v_1}, \\vec{v_2}, \\ldots, \\vec{v_k} \\right)\\) is a basis of eigenspace corresponding to eigenvalue \\(\\lambda\\)Using proof in Property 2.7 we imply that \\(P \\vec{v_1}, P \\vec{v_2}, \\ldots, P \\vec{v_k}\\) are eigenvectors corresponding to the eigenvalue \\(\\lambda\\) of the matrix \\(B\\).We prove that \\(\\left( P^{-1} \\vec{v_1}, P^{-1} \\vec{v_2}, \\ldots, P^{-1} \\vec{v_k} \\right)\\) is the basis of eigenspace corresponding to eigenvalue \\(\\lambda\\) of matrix \\(B\\).Assume there exists \\(\\alpha_1, \\alpha_2, \\ldots, \\alpha_n\\) such that\\[\\begin{aligned} &amp; \\alpha_1 P^{-1} \\vec{v_1}+\\alpha_2 P^{-1} \\vec{v_2}+\\ldots+\\alpha_k P^{-1} \\vec{v_k}=\\vec{0} \\\\ \\Rightarrow &amp; \\alpha_1 \\vec{v_1}+\\alpha_2 \\vec{v_2}+\\ldots+\\alpha_k \\vec{v_k}= P \\cdot \\vec{0}= \\vec{0}\\end{aligned}\\]Since \\(\\left( \\vec{v_1}, \\vec{v_2}, \\ldots, \\vec{v_k} \\right)\\) is a basis of vector space with dimension \\(k\\), then we have \\(\\vec{\\alpha_1} = \\vec{\\alpha_2} = \\ldots = \\vec{\\alpha_k} = 0\\)Hence, we have \\(\\left(P^{-1} \\vec{v_1}, P^{-1} \\vec{v_2}, \\ldots, P^{-1} \\vec{v_k} \\right)\\) is linear independent.Assume there exists vector \\(\\vec{v}\\) is a eigenvector of matrix \\(B\\) that correspond to eigenvalue \\(\\lambda\\) such that \\(\\vec{v} \\notin \\operatorname{Span}({P^{-1} \\vec{v_1}, P^{-1} \\vec{v_2}, \\ldots, P^{-1} \\vec{v_k}})\\)Hence, there exists eigenvector \\(\\vec{u}\\) corresponding to eigenvalue \\(\\lambda\\) of matrix \\(A\\) such that \\(\\vec{v} = P^{-1} \\vec{u}\\). Then we imply that \\(\\vec{u} \\notin \\operatorname{Span}({\\vec{v_1}, \\vec{v_2}, \\ldots, \\vec{v_k} })\\) (CONTRADICTION)Therefore, we can imply that \\(\\left( P^{-1} \\vec{v_1}, P^{-1} \\vec{v_2}, \\ldots, P^{-1} \\vec{v_k} \\right)\\) is the basis of eigenspace corresponding to eigenvalue \\(\\lambda\\) of matrix \\(B\\).Hence, we can conclude that eigenspace corresponding to eigenvalue \\(\\lambda\\) of \\(A\\) and \\(B\\) have same dimension. Therefore, by definition, similar matrices have same geometric multiplicity.  (Q.E.D)"
  },
  
  {
    "title": "Hello World, I'm NgocQuan",
    "url": "/posts/NgocQuan/",
    "categories": "blog post",
    "tags": "ngocquan",
    "date": "2023-01-31 23:24:25 +0700",
    





    
    "snippet": "This is my page where I will talk about my research about Natural Language Processing and some other interesting mathematics like Linear Algebra, Probability and Statistics, …\\[e^{i\\pi} = -1\\]“Lore...",
    "content": "This is my page where I will talk about my research about Natural Language Processing and some other interesting mathematics like Linear Algebra, Probability and Statistics, …\\[e^{i\\pi} = -1\\]“Lorem ipsum dolor sit amet, \\(e^{i\\pi} = -1\\) consectetur adipiscing elit.”  \\(e^{i\\pi} = -1\\)  \\(e^{i\\pi} = -1\\)  \\(e^{i\\pi} = -1\\) Image Caption Image Caption hereIn a quaint village, nestled among verdant hills, there lived x= 3 here.for i in range(n) :    print(i)    print(\"Hello World\")"
  }
  
]

