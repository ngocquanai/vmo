[
  
  {
    "title": "Similar Properties of Similarity Matrix",
    "url": "/posts/similarity-matrix/",
    "categories": "Mathematics, Linear Algebra",
    "tags": "similar matrices, similarity matrix, mathematics, algebra",
    "date": "2023-08-20 20:00:00 +0700",
    





    
    "snippet": "Abstract: Initially I will introduce specific definitions relevant to matrix similarity topic. Subsequently, it will explore various properties common to similar matrices, such as trace, rank, dete...",
    "content": "Abstract: Initially I will introduce specific definitions relevant to matrix similarity topic. Subsequently, it will explore various properties common to similar matrices, such as trace, rank, determinant, among others. Finally, the paper will present criteria for determining whether any two given matrices are similar.DEFINITIONS1.1: SIMILARITY MATRIXTwo square \\(n \\times n\\) matrices A and B are called similar if there exists an invertible \\(n*n\\) matrix \\(P\\) such that\\[B = P^{-1}AP\\]1.2: REPRESENTATION OF A VECTOR\\(V\\) is a vector space with the basis \\(B=\\left&lt;\\overrightarrow{\\beta_1}, \\overrightarrow{\\beta_2}, \\ldots, \\overrightarrow{\\beta_n}\\right&gt; , \\vec{v} \\in V,  \\alpha_1, \\ldots, \\alpha_n \\in \\mathbb{R}\\) such that \\(\\vec{v}=\\alpha_1 \\overrightarrow{\\beta_1}+\\alpha_2 \\overrightarrow{\\beta_2}+\\ldots+\\alpha_n \\overrightarrow{\\beta_n}.\\) Then we have\\[{Rep}_B(\\vec{v})=\\left(\\begin{array}{c}\\overrightarrow{\\beta_1} \\\\ \\overrightarrow{\\beta_2} \\\\ \\vdots \\\\ \\overrightarrow{\\beta_n}\\end{array}\\right)\\]1.3: MATRIX REPRESENTATION\\(V, W\\) are vector spaces with dimension \\(n, m\\) and basis \\(B, D\\). \\(f \\colon V \\to W\\) is a linear mapping. If we have \\(\\operatorname{Rep}_D(h(\\overrightarrow{\\beta_1}))=\\left(\\begin{array}{c}h_{1,1} \\\\ h_{2,1} \\\\ \\vdots \\\\ h_{n, 1}\\end{array}\\right), \\ldots, \\operatorname{Rep}_D(h(\\overrightarrow{\\beta_n}))=\\left(\\begin{array}{c}h_{1, n} \\\\ h_{2, n} \\\\ \\vdots \\\\ h_{n, n}\\end{array}\\right)\\). Then\\[\\left(\\begin{array}{cccc}h_{1,1} &amp; h_{1,2} &amp; \\cdots &amp; h_{1, n} \\\\ h_{2,1} &amp; h_{2,2} &amp; \\cdots &amp; h_{2, n} \\\\ \\vdots &amp; \\vdots &amp;  &amp; \\vdots \\\\ h_{n, 1} &amp; h_{n, 2} &amp; \\cdots &amp; h_{n, n}\\end{array}\\right)\\]is called matrix representation of mapping f with respect to the bases \\(B, D\\), notated by \\({Rep}_{B,D}(f)\\)1.4: DETERMINANT OF MATRIX\\(n \\times n\\) determinant is a function \\(f \\colon M_{n \\times n} \\rightarrow \\mathbb{R}\\) such that (1) \\(\\operatorname{det}\\left(\\overrightarrow{p_1}, \\ldots, k \\overrightarrow{p_i}+\\overrightarrow{p_j}, \\ldots, \\overrightarrow{p_n}\\right)=\\operatorname{det}\\left(\\overrightarrow{p_1}, \\ldots, \\overrightarrow{p_j}, \\ldots, \\overrightarrow{p_n}\\right)\\), for all \\(k \\in \\mathbb{R}\\) (2) \\(\\operatorname{det}\\left(\\overrightarrow{p_1}, \\ldots, \\overrightarrow{p_j}, \\ldots, \\overrightarrow{p_i}, \\ldots, \\overrightarrow{p_n}\\right)=-\\operatorname{det}\\left(\\overrightarrow{p_1}, \\ldots, \\overrightarrow{p_i}, \\ldots, \\overrightarrow{p_j}, \\ldots, \\overrightarrow{p_n}\\right)\\) (3) \\(\\operatorname{det}\\left(\\overrightarrow{p_1}, \\ldots, k \\overrightarrow{p_i}, \\ldots, \\overrightarrow{p_n}\\right) = k \\operatorname{det}\\left(\\overrightarrow{p_1}, \\ldots, \\overrightarrow{p_i}, \\ldots, \\overrightarrow{p_n}\\right)\\), for all \\(k \\in \\mathbb{R}\\) (4) \\(\\operatorname{det}(I) = 1\\), \\(I\\) is the identity matrix. Normally, we use the notation \\(|T|\\) instead of \\(\\operatorname{det}(T)\\).1.5: TRACE OF MATRIXThe trace of a square matrix \\(A\\) is the sum of elements on the main diagonal of matrix \\(A\\), denoted by \\(tr(A)\\)1.6: POLYNOMIAL OF MATRIX\\(T\\) is a square matrix, \\(f(x)\\) is a polynomial with degree n \\(f(x) = c_n x^n + c_{n-1} x^{n-1} + \\cdots + c_1 x + c_0\\) \\((c_i \\in \\mathbb{R})\\). Then we have \\(f(T) = c_n T^n + c_{n-1} T^{n-1} + \\cdots + c_1 T + c_0 I\\) (\\(I\\) is identity matrix) is a polynomial of matrix \\(T\\) and also a square matrix of the same size as \\(T\\).1.7: MINIMAL POLYNOMIALFor a square matrix \\(T\\), its minimal polynomial is the monic polynomial \\(f\\) of the lowest degree such that \\(f(T) = 0\\).1.8: INDEX OF NIPOTENTA square matrix $A$ is said to have a nilpotent index $k$ if \\(k\\) is the smallest natural number(if exists) such that $A^k = 0$.1.9: SUBDIAGONALThe subdiagonal of a square matrix is the line immediately below the main diagonal of the matrix.1.10: JORDAN BLOCKA Jordan block is a square matrix where the off-diagonal entries are zero, the entries on the main diagonal are all $\\lambda$ (where $\\lambda \\in \\mathbb{C}$), and every entry on the subdiagonal is 1.1.11: JORDAN CANONICAL FORMThe square matrix $A$ can be represented in the form of a Jordan normal form if \\[A = \\left(\\begin{array}{cccc}J_{\\lambda_1} &amp; 0 &amp; \\cdots &amp; 0 \\\\0 &amp; J_{\\lambda_2} &amp; \\ddots &amp; \\vdots \\\\\\vdots &amp; \\ddots &amp; \\ddots &amp; 0 \\\\0 &amp; \\cdots &amp; 0 &amp; J_{\\lambda_k} \\\\\\end{array}\\right)\\]with \\(J_{\\lambda_i}\\) are Jordan blocks.1.12: CHARACTERISTIC POLYNOMIALA is an \\(n \\times n\\) matrix, the characteristic polynomial of A, denoted by \\(p_{A}(x)\\) is defined by\\[p_{A}(x) = \\operatorname{det}(A - xI)\\]where \\(I\\) denotes the \\(n \\times n\\) identity matrix.1.13: GEOMETRIC AND ALGEBRAIC MULTIPLICITYA is a square matrix with characteristic polynomial \\(f(x) = (x- \\lambda_1)^{m_1} (x - \\lambda_2)^{m_2} \\ldots (x- \\lambda_r)^{m_r}\\). Then \\(m_i\\) is called algebraic multiplicity corresponding to eigenvalue \\(\\lambda_i\\), the dimension of eigenspace corresponding to eigenvalue \\(\\lambda_i\\) is called geometric multiplicity.1.14: COFACTOR OF MATRIXConsider the \\(n \\times n\\) square matrix \\(T\\), we define the \\(i,j\\) minor \\(M_{i,j}\\) of matrix \\(T\\) is the \\((n-1) \\times (n-1)\\) matrix obtained by eliminating the i-th row and j-th column in the matrix \\(T\\).Then we have \\(i,j\\) cofactor \\(T_{i,j} = (-1)^{i+j} \\operatorname{det}(M_{i,j})\\)1.15: MATRIX ADJOINTMatrix adjoint of square matrix T is denoted by\\[\\text{adj}(T) = \\left(\\begin{array}{cccc}T_{1,1} &amp; T_{1,2} &amp; \\cdots &amp; T_{1,n} \\\\T_{2,1} &amp; T_{2,2} &amp; \\cdots &amp; T_{2,n} \\\\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\T_{n,1} &amp; T_{n,2} &amp; \\cdots &amp; T_{n,n}\\end{array}\\right)\\]where \\(T_{i,j}\\) is the \\(i,j\\) cofactor of matrix T.1.16: EQUIVALENCE RELATIONA binary relation \\(\\sim\\) on a set \\(\\mathbb{X}\\) is said to be an equivalence relation if and only if it is relexive, symmetric and transitive. That is, for all \\(a, b, c \\in \\mathbb{X}\\) :  Reflexive: \\(a \\sim a\\)  Symmetric: \\(a \\sim b\\) if and only of \\(b \\sim a\\)  Transitive: If \\(a \\sim b\\) and \\(b \\sim c\\), then \\(a \\sim c\\)2. PROPERTIES2.1: Matrix similarity is equivalence relationWe aim to prove that the similarity matrix relation satisfies the properties of reflexivity, symmetry, and transitivity.      Reflexive:For any square matrix \\(A\\), it is obvious that \\(I^{-1}AI = A\\), where \\(I\\) is the identity matrix. Therefore, \\(A\\) is similar to itself.        Symmetric: Suppose that B is similar to A,which means there exists an invertible matrix \\(P\\) such that \\(B = P^{-1}AP\\) \\(\\implies B P^{-1} = P^{-1}AI = P^{-1}A\\)  \\(\\iff PBP^{-1} = IA = A\\) This implies A is also similar to B, so we have the similarity relation is symmetric.        Transitive:Consider three matrices \\(A\\), \\(B\\), and \\(C\\) such that \\(A\\) is similar to \\(B\\) and \\(B\\) is similar to \\(C\\). Hence, there exist invertible matrices \\(P\\), \\(Q\\) such that \\(B = P^{-1}AP\\) and \\(C = Q^{-1}BQ\\). \\(\\implies C = Q^{-1}P^{-1}APQ = (PQ)^{-1}APQ\\)  Therefore, \\(C\\) is similar to \\(A\\), and matrix similarity relation is transitive.2.2: Similarity matrix are matrix representations of the same linear mapping but with respect to different bases.Lemma 1: Consider three vector spaces \\(U,V,W\\) and two linear mappings \\(f \\colon U \\to V\\) and \\(g \\colon V \\to W\\); \\(B,D,E\\) are the bases of 3 vector spaces, respectively. Then we have \\[\\operatorname{Rep}_{B,E}(f \\circ g) = \\operatorname{Rep}_{B,D}(f) \\cdot \\operatorname{Rep}_{D,E}(g)\\]The notation id represent for identity matrixConsider two similar matrices \\(H,G\\) such that \\(H= \\operatorname{Rep}_{B,B}(h)\\) and \\(G= P^{-1}HP\\), where \\(B\\) is the basis of vector space \\(V\\) including \\(\\overrightarrow{\\beta_1}, \\overrightarrow{\\beta_2}, \\ldots, \\overrightarrow{\\beta_n}\\). \\(h\\) is a linear mapping. Consider the basis \\(D\\) such that \\(\\operatorname{Rep}_{D,B}(id) = P\\)From the figure above, we have \\(h = id \\circ h \\circ id\\).  Using Lemma 1 then we have\\[\\operatorname{Rep}_{D,D}(h) = \\operatorname{Rep}_{D,B}(id) \\cdot \\operatorname{Rep}_{B,B}(h) \\cdot \\operatorname{Rep}_{B,D}(id) = P^{-1}HP\\iff G = \\operatorname{Rep}_{D,D}(h) \\, \\, (Q.E.D)\\]"
  },
  
  {
    "title": "Hello World, I'm NgocQuan",
    "url": "/posts/NgocQuan/",
    "categories": "blog post",
    "tags": "ngocquan",
    "date": "2023-01-31 23:24:25 +0700",
    





    
    "snippet": "This is my page where I will talk about my research about Natural Language Processing and some other interesting mathematics like Linear Algebra, Probability and Statistics, …\\[e^{i\\pi} = -1\\]“Lore...",
    "content": "This is my page where I will talk about my research about Natural Language Processing and some other interesting mathematics like Linear Algebra, Probability and Statistics, …\\[e^{i\\pi} = -1\\]“Lorem ipsum dolor sit amet, \\(e^{i\\pi} = -1\\) consectetur adipiscing elit.”  \\(e^{i\\pi} = -1\\)  \\(e^{i\\pi} = -1\\)  \\(e^{i\\pi} = -1\\) Image Caption Image Caption hereIn a quaint village, nestled among verdant hills, there lived x= 3 here.for i in range(n) :    print(i)    print(\"Hello World\")"
  }
  
]

